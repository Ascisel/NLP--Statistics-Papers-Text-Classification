terms,titles,abstracts
"['stat.ML', 'stat.TH']",Universal consistency of Wasserstein $k$-NN classifier,"The Wasserstein distance provides a notion of dissimilarities between
probability measures, which has recent applications in learning of structured
data with varying size such as images and text documents. In this work, we
analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein
distance and establish the universal consistency on families of distributions.
Using previous known results on the consistency of the $k$-NN classifier on
infinite dimensional metric spaces, it suffices to show that the families is a
countable union of finite dimension sets. As a result, we show that the $k$-NN
classifier is universally consistent on spaces of finitely supported measures,
the space of Gaussian measures, and the space of measures with finite wavelet
densities. In addition, we give a counterexample to show that the universal
consistency does not hold on $\mathcal{W}_p((0,1))$."
"['stat.ML', 'stat.CO']",Space Partitioning and Regression Mode Seeking via a Mean-Shift-Inspired Algorithm,"The mean shift (MS) algorithm is a nonparametric method used to cluster
sample points and find the local modes of kernel density estimates, using an
idea based on iterative gradient ascent. In this paper we develop a
mean-shift-inspired algorithm to estimate the modes of regression functions and
partition the sample points in the input space. We prove convergence of the
sequences generated by the algorithm and derive the non-asymptotic rates of
convergence of the estimated local modes for the underlying regression model.
We also demonstrate the utility of the algorithm for data-enabled discovery
through an application on biomolecular structure data. An extension to subspace
constrained mean shift (SCMS) algorithm used to extract ridges of regression
functions is briefly discussed."
['stat.TH'],Analyzing the tree-layer structure of Deep Forests,"Random forests on the one hand, and neural networks on the other hand, have
met great success in the machine learning community for their predictive
performance. Combinations of both have been proposed in the literature, notably
leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper,
our aim is not to benchmark DF performances but to investigate instead their
underlying mechanisms. Additionally, DF architecture can be generally
simplified into more simple and computationally efficient shallow forest
networks. Despite some instability, the latter may outperform standard
predictive tree-based methods. We exhibit a theoretical framework in which a
shallow tree network is shown to enhance the performance of classical decision
trees. In such a setting, we provide tight theoretical lower and upper bounds
on its excess risk. These theoretical results show the interest of tree-network
architectures for well-structured data provided that the first layer, acting as
a data encoder, is rich enough."
"['stat.ML', 'stat.CO', 'stat.ME']",Approximate Cross-Validation for Structured Models,"Many modern data analyses benefit from explicitly modeling dependence
structure in data -- such as measurements across time or space, ordered words
in a sentence, or genes in a genome. A gold standard evaluation technique is
structured cross-validation (CV), which leaves out some data subset (such as
data within a time interval or data in a geographic region) in each fold. But
CV here can be prohibitively slow due to the need to re-run already-expensive
learning algorithms many times. Previous work has shown approximate
cross-validation (ACV) methods provide a fast and provably accurate alternative
in the setting of empirical risk minimization. But this existing ACV work is
restricted to simpler models by the assumptions that (i) data across CV folds
are independent and (ii) an exact initial model fit is available. In structured
data analyses, both these assumptions are often untrue. In the present work, we
address (i) by extending ACV to CV schemes with dependence structure between
the folds. To address (ii), we verify -- both theoretically and empirically --
that ACV quality deteriorates smoothly with noise in the initial fit. We
demonstrate the accuracy and computational benefits of our proposed methods on
a diverse set of real-world applications."
"['stat.CO', 'stat.ME', 'stat.ML']",Cluster-Specific Predictions with Multi-Task Gaussian Processes,"A model involving Gaussian processes (GPs) is introduced to simultaneously
handle multi-task learning, clustering, and prediction for multiple functional
data. This procedure acts as a model-based clustering method for functional
data as well as a learning step for subsequent predictions for new tasks. The
model is instantiated as a mixture of multi-task GPs with common mean
processes. A variational EM algorithm is derived for dealing with the
optimisation of the hyper-parameters along with the hyper-posteriors'
estimation of latent variables and processes. We establish explicit formulas
for integrating the mean processes and the latent clustering variables within a
predictive distribution, accounting for uncertainty on both aspects. This
distribution is defined as a mixture of cluster-specific GP predictions, which
enhances the performances when dealing with group-structured data. The model
handles irregular grid of observations and offers different hypotheses on the
covariance structure for sharing additional information across tasks. The
performances on both clustering and prediction tasks are assessed through
various simulated scenarios and real datasets. The overall algorithm, called
MagmaClust, is publicly available as an R package."
"['stat.ML', 'stat.CO']",Splitting Methods for Convex Bi-Clustering and Co-Clustering,"Co-Clustering, the problem of simultaneously identifying clusters across
multiple aspects of a data set, is a natural generalization of clustering to
higher-order structured data. Recent convex formulations of bi-clustering and
tensor co-clustering, which shrink estimated centroids together using a convex
fusion penalty, allow for global optimality guarantees and precise theoretical
analysis, but their computational properties have been less well studied. In
this note, we present three efficient operator-splitting methods for the convex
co-clustering problem: a standard two-block ADMM, a Generalized ADMM which
avoids an expensive tensor Sylvester equation in the primal update, and a
three-block ADMM based on the operator splitting scheme of Davis and Yin.
Theoretical complexity analysis suggests, and experimental evidence confirms,
that the Generalized ADMM is far more efficient for large problems."
"['stat.ML', 'stat.ME']",Post Selection Inference with Kernels,"We propose a novel kernel based post selection inference (PSI) algorithm,
which can not only handle non-linearity in data but also structured output such
as multi-dimensional and multi-label outputs. Specifically, we develop a PSI
algorithm for independence measures, and propose the Hilbert-Schmidt
Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the
proposed algorithm is that it can handle non-linearity and/or structured data
through kernels. Namely, the proposed algorithm can be used for wider range of
applications including nonlinear multi-class classification and multi-variate
regressions, while existing PSI algorithms cannot handle them. Through
synthetic experiments, we show that the proposed approach can find a set of
statistically significant features for both regression and classification
problems. Moreover, we apply the hsicInf algorithm to a real-world data, and
show that hsicInf can successfully identify important features."
"['stat.ME', 'stat.ML', 'stat.TH']",A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification,"Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document."
"['stat.ML', 'stat.TH']",Learning-augmented count-min sketches via Bayesian nonparametrics,"The count-min sketch (CMS) is a time and memory efficient randomized data
structure that provides estimates of tokens' frequencies in a data stream, i.e.
point queries, based on random hashed data. Learning-augmented CMSs improve the
CMS by learning models that allow to better exploit data properties. In this
paper, we focus on the learning-augmented CMS of Cai, Mitzenmacher and Adams
(\textit{NeurIPS} 2018), which relies on Bayesian nonparametric (BNP) modeling
of a data stream via Dirichlet process (DP) priors. This is referred to as the
CMS-DP, and it leads to BNP estimates of a point query as posterior means of
the point query given the hashed data. While BNPs is proved to be a powerful
tool for developing robust learning-augmented CMSs, ideas and methods behind
the CMS-DP are tailored to point queries under DP priors, and they can not be
used for other priors or more general queries. In this paper, we present an
alternative, and more flexible, derivation of the CMS-DP such that: i) it
allows to make use of the Pitman-Yor process (PYP) prior, which is arguably the
most popular generalization of the DP prior; ii) it can be readily applied to
the more general problem of estimating range queries. This leads to develop a
novel learning-augmented CMS under power-law data streams, referred to as the
CMS-PYP, which relies on BNP modeling of the stream via PYP priors.
Applications to synthetic and real data show that the CMS-PYP outperforms the
CMS and the CMS-DP in the estimation of low-frequency tokens; this known to be
a critical feature in natural language processing, where it is indeed common to
encounter power-law data streams."
"['stat.ML', 'stat.CO']",Pólya Urn Latent Dirichlet Allocation: a doubly sparse massively parallel sampler,"Latent Dirichlet Allocation (LDA) is a topic model widely used in natural
language processing and machine learning. Most approaches to training the model
rely on iterative algorithms, which makes it difficult to run LDA on big
corpora that are best analyzed in parallel and distributed computational
environments. Indeed, current approaches to parallel inference either don't
converge to the correct posterior or require storage of large dense matrices in
memory. We present a novel sampler that overcomes both problems, and we show
that this sampler is faster, both empirically and theoretically, than previous
Gibbs samplers for LDA. We do so by employing a novel P\'olya-urn-based
approximation in the sparse partially collapsed sampler for LDA. We prove that
the approximation error vanishes with data size, making our algorithm
asymptotically exact, a property of importance for large-scale topic models. In
addition, we show, via an explicit example, that - contrary to popular belief
in the topic modeling literature - partially collapsed samplers can be more
efficient than fully collapsed samplers. We conclude by comparing the
performance of our algorithm with that of other approaches on well-known
corpora."
"['stat.ML', 'stat.TH']",Multiplicative noise and heavy tails in stochastic optimization,"Although stochastic optimization is central to modern machine learning, the
precise mechanisms underlying its success, and in particular, the precise role
of the stochasticity, still remain unclear. Modelling stochastic optimization
algorithms as discrete random recurrence relations, we show that multiplicative
noise, as it commonly arises due to variance in local rates of convergence,
results in heavy-tailed stationary behaviour in the parameters. A detailed
analysis is conducted for SGD applied to a simple linear regression problem,
followed by theoretical results for a much larger class of models (including
non-linear and non-convex) and optimizers (including momentum, Adam, and
stochastic Newton), demonstrating that our qualitative results hold much more
generally. In each case, we describe dependence on key factors, including step
size, batch size, and data variability, all of which exhibit similar
qualitative behavior to recent empirical results on state-of-the-art neural
network models from computer vision and natural language processing.
Furthermore, we empirically demonstrate how multiplicative noise and
heavy-tailed structure improve capacity for basin hopping and exploration of
non-convex loss surfaces, over commonly-considered stochastic dynamics with
only additive noise and light-tailed structure."
"['stat.ME', 'stat.ML']",Conditional Hierarchical Bayesian Tucker Decomposition,"Our research focuses on studying and developing methods for reducing the
dimensionality of large datasets, common in biomedical applications. A major
problem when learning information about patients based on genetic sequencing
data is that there are often more feature variables (genetic data) than
observations (patients). This makes direct supervised learning difficult. One
way of reducing the feature space is to use latent Dirichlet allocation in
order to group genetic variants in an unsupervised manner. Latent Dirichlet
allocation is a common model in natural language processing, which describes a
document as a mixture of topics, each with a probability of generating certain
words. This can be generalized as a Bayesian tensor decomposition to account
for multiple feature variables. While we made some progress improving and
modifying these methods, our significant contributions are with hierarchical
topic modeling. We developed distinct methods of incorporating hierarchical
topic modeling, based on nested Chinese restaurant processes and Pachinko
Allocation Machine, into Bayesian tensor decompositions. We apply these models
to predict whether or not patients have autism spectrum disorder based on
genetic sequencing data. We examine a dataset from National Database for Autism
Research consisting of paired siblings -- one with autism, and the other
without -- and counts of their genetic variants. Additionally, we linked the
genes with their Reactome biological pathways. We combine this information into
a tensor of patients, counts of their genetic variants, and the membership of
these genes in pathways. Once we decompose this tensor, we use logistic
regression on the reduced features in order to predict if patients have autism.
We also perform a similar analysis of a dataset of patients with one of four
common types of cancer (breast, lung, prostate, and colorectal)."
"['stat.ME', 'stat.ML']",Show Your Work: Improved Reporting of Experimental Results,"Research in natural language processing proceeds, in part, by demonstrating
that new models achieve superior performance (e.g., accuracy) on held-out test
data, compared to previous results. In this paper, we demonstrate that test-set
performance scores alone are insufficient for drawing accurate conclusions
about which model performs best. We argue for reporting additional details,
especially performance on validation data obtained during model development. We
present a novel technique for doing so: expected validation performance of the
best-found model as a function of computation budget (i.e., the number of
hyperparameter search trials or the overall training time). Using our approach,
we find multiple recent model comparisons where authors would have reached a
different conclusion if they had used more (or less) computation. Our approach
also allows us to estimate the amount of computation required to obtain a given
accuracy; applying it to several recently published results yields massive
variation across papers, from hours to weeks. We conclude with a set of best
practices for reporting experimental results which allow for robust future
comparisons, and provide code to allow researchers to use our technique."
"['stat.ML', 'stat.ME', 'stat.TH']",A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research."
"['stat.ML', 'stat.TH']",Testing Closeness With Unequal Sized Samples,"We consider the problem of closeness testing for two discrete distributions
in the practically relevant setting of \emph{unequal} sized samples drawn from
each of them. Specifically, given a target error parameter $\varepsilon > 0$,
$m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from
an unknown distribution $q$, we describe a test for distinguishing the case
that $p=q$ from the case that $||p-q||_1 \geq \varepsilon$. If $p$ and $q$ are
supported on at most $n$ elements, then our test is successful with high
probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 =
\Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt
n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this
range, to constant factors. These results extend the recent work of Chan et al.
who established the sample complexity when the two samples have equal sizes,
and tightens the results of Acharya et al. by polynomials factors in both $n$
and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the
mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses
$\tilde{O}(n^{3/2} \tau_{mix})$ queries to a ""next node"" oracle, improving upon
the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we
note that the core of our testing algorithm is a relatively simple statistic
that seems to perform well in practice, both on synthetic data and on natural
language data."
"['stat.ML', 'stat.TH']",Altitude Training: Strong Bounds for Single-Layer Dropout,"Dropout training, originally designed for deep neural networks, has been
successful on high-dimensional single-layer natural language tasks. This paper
proposes a theoretical explanation for this phenomenon: we show that, under a
generative Poisson topic model with long documents, dropout training improves
the exponent in the generalization bound for empirical risk minimization.
Dropout achieves this gain much like a marathon runner who practices at
altitude: once a classifier learns to perform reasonably well on training
examples that have been artificially corrupted by dropout, it will do very well
on the uncorrupted test set. We also show that, under similar conditions,
dropout preserves the Bayes decision boundary and should therefore induce
minimal bias in high dimensions."
['stat.ME'],Densely connected neural networks for nonlinear regression,"Densely connected convolutional networks (DenseNet) behave well in image
processing. However, for regression tasks, convolutional DenseNet may lose
essential information from independent input features. To tackle this issue, we
propose a novel DenseNet regression model where convolution and pooling layers
are replaced by fully connected layers and the original concatenation shortcuts
are maintained to reuse the feature. To investigate the effects of depth and
input dimension of proposed model, careful validations are performed by
extensive numerical simulation. The results give an optimal depth (19) and
recommend a limited input dimension (under 200). Furthermore, compared with the
baseline models including support vector regression, decision tree regression,
and residual regression, our proposed model with the optimal depth performs
best. Ultimately, DenseNet regression is applied to predict relative humidity,
and the outcome shows a high correlation (0.91) with observations, which
indicates that our model could advance environmental data analysis."
"['stat.ML', 'stat.TH']",Universal Consistency of Decision Trees in High Dimensions,"This paper shows that decision trees constructed with Classification and
Regression Trees (CART) methodology are universally consistent in an additive
model context, even when the number of predictor variables scales exponentially
with the sample size, under certain $1$-norm sparsity constraints. The
consistency is universal in the sense that there are no a priori assumptions on
the distribution of the predictor variables. Amazingly, this adaptivity to
(approximate or exact) sparsity is achieved with a single tree, as opposed to
what might be expected for an ensemble. Finally, we show that these qualitative
properties of individual trees are inherited by Breiman's random forests.
Another surprise is that consistency holds even when the ""mtry"" tuning
parameter vanishes as a fraction of the number of predictor variables, thus
speeding up computation of the forest. A key step in the analysis is the
establishment of an oracle inequality, which precisely characterizes the
goodness-of-fit and complexity tradeoff for a misspecified model."
"['stat.ML', 'stat.ME', 'stat.TH']",Adaptive transfer learning,"In transfer learning, we wish to make inference about a target population
when we have access to data both from the distribution itself, and from a
different but related source distribution. We introduce a flexible framework
for transfer learning in the context of binary classification, allowing for
covariate-dependent relationships between the source and target distributions
that are not required to preserve the Bayes decision boundary. Our main
contributions are to derive the minimax optimal rates of convergence (up to
poly-logarithmic factors) in this problem, and show that the optimal rate can
be achieved by an algorithm that adapts to key aspects of the unknown transfer
relationship, as well as the smoothness and tail parameters of our
distributional classes. This optimal rate turns out to have several regimes,
depending on the interplay between the relative sample sizes and the strength
of the transfer relationship, and our algorithm achieves optimality by careful,
decision tree-based calibration of local nearest-neighbour procedures."
['stat.TH'],Analyzing the tree-layer structure of Deep Forests,"Random forests on the one hand, and neural networks on the other hand, have
met great success in the machine learning community for their predictive
performance. Combinations of both have been proposed in the literature, notably
leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper,
our aim is not to benchmark DF performances but to investigate instead their
underlying mechanisms. Additionally, DF architecture can be generally
simplified into more simple and computationally efficient shallow forest
networks. Despite some instability, the latter may outperform standard
predictive tree-based methods. We exhibit a theoretical framework in which a
shallow tree network is shown to enhance the performance of classical decision
trees. In such a setting, we provide tight theoretical lower and upper bounds
on its excess risk. These theoretical results show the interest of tree-network
architectures for well-structured data provided that the first layer, acting as
a data encoder, is rich enough."
"['stat.ML', 'stat.TH']",SIRUS: Stable and Interpretable RUle Set for Classification,"State-of-the-art learning algorithms, such as random forests or neural
networks, are often qualified as ""black-boxes"" because of the high number and
complexity of operations involved in their prediction mechanism. This lack of
interpretability is a strong limitation for applications involving critical
decisions, typically the analysis of production processes in the manufacturing
industry. In such critical contexts, models have to be interpretable, i.e.,
simple, stable, and predictive. To address this issue, we design SIRUS (Stable
and Interpretable RUle Set), a new classification algorithm based on random
forests, which takes the form of a short list of rules. While simple models are
usually unstable with respect to data perturbation, SIRUS achieves a remarkable
stability improvement over cutting-edge methods. Furthermore, SIRUS inherits a
predictive accuracy close to random forests, combined with the simplicity of
decision trees. These properties are assessed both from a theoretical and
empirical point of view, through extensive numerical experiments based on our
R/C++ software implementation sirus available from CRAN."
"['stat.ML', 'stat.ME']",Hard and Soft EM in Bayesian Network Learning from Incomplete Data,"Incomplete data are a common feature in many domains, from clinical trials to
industrial applications. Bayesian networks (BNs) are often used in these
domains because of their graphical and causal interpretations. BN parameter
learning from incomplete data is usually implemented with the
Expectation-Maximisation algorithm (EM), which computes the relevant sufficient
statistics (""soft EM"") using belief propagation. Similarly, the Structural
Expectation-Maximisation algorithm (Structural EM) learns the network structure
of the BN from those sufficient statistics using algorithms designed for
complete data. However, practical implementations of parameter and structure
learning often impute missing data (""hard EM"") to compute sufficient statistics
instead of using belief propagation, for both ease of implementation and
computational speed. In this paper, we investigate the question: what is the
impact of using imputation instead of belief propagation on the quality of the
resulting BNs? From a simulation study using synthetic data and reference BNs,
we find that it is possible to recommend one approach over the other in several
scenarios based on the characteristics of the data. We then use this
information to build a simple decision tree to guide practitioners in choosing
the EM algorithm best suited to their problem."
"['stat.ML', 'stat.TH']",Sparse learning with CART,"Decision trees with binary splits are popularly constructed using
Classification and Regression Trees (CART) methodology. For regression models,
this approach recursively divides the data into two near-homogenous daughter
nodes according to a split point that maximizes the reduction in sum of squares
error (the impurity) along a particular variable. This paper aims to study the
statistical properties of regression trees constructed with CART methodology.
In doing so, we find that the training error is governed by the Pearson
correlation between the optimal decision stump and response data in each node,
which we bound by constructing a prior distribution on the split points and
solving a nonlinear optimization problem. We leverage this connection between
the training error and Pearson correlation to show that CART with
cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff
when the depth scales with the logarithm of the sample size. Data dependent
quantities, which adapt to the dimensionality and latent structure of the
regression model, are seen to govern the rates of convergence of the prediction
error."
['stat.CO'],TreeGen -- a Monte Carlo generator for data frames,"The typical problem in Data Science is creating a structure that encodes the
occurrence frequency of unique elements in rows and relations between different
rows of a data frame. We present the probability tree abstract data structure,
an extension of the decision tree, that facilitates more than two choices with
assigned probabilities. Such a tree represents statistical relations between
different rows of the data frame. The Probability Tree algorithmic structure is
supplied with the Generator module that is a Monte Carlo generator that
traverses through the tree. These two components are implemented in TreeGen
Python package. The package can be used in increasing data multiplicity,
compressing data preserving its statistical information, constructing
hierarchical models, exploring data, and in feature extraction."
"['stat.ML', 'stat.CO', 'stat.ME']",Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models,"Decision trees are flexible models that are well suited for many statistical
regression problems. In a Bayesian framework for regression trees, Markov Chain
Monte Carlo (MCMC) search algorithms are required to generate samples of tree
models according to their posterior probabilities. The critical component of
such an MCMC algorithm is to construct good Metropolis-Hastings steps for
updating the tree topology. However, such algorithms frequently suffering from
local mode stickiness and poor mixing. As a result, the algorithms are slow to
converge. Hitherto, authors have primarily used discrete-time birth/death
mechanisms for Bayesian (sums of) regression tree models to explore the model
space. These algorithms are efficient only if the acceptance rate is high which
is not always the case. Here we overcome this issue by developing a new search
algorithm which is based on a continuous-time birth-death Markov process. This
search algorithm explores the model space by jumping between parameter spaces
corresponding to different tree structures. In the proposed algorithm, the
moves between models are always accepted which can dramatically improve the
convergence and mixing properties of the MCMC algorithm. We provide theoretical
support of the algorithm for Bayesian regression tree models and demonstrate
its performance."
"['stat.ML', 'stat.TH']",Distributional Generalization: A New Kind of Generalization,"We introduce a new notion of generalization -- Distributional Generalization
-- which roughly states that outputs of a classifier at train and test time are
close *as distributions*, as opposed to close in just their average error. For
example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then
a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as
cats on the *test set* as well, while leaving other classes unaffected. This
behavior is not captured by classical generalization, which would only consider
the average error and not the distribution of errors over the input domain. Our
formal conjectures, which are much more general than this example, characterize
the form of distributional generalization that can be expected in terms of
problem parameters: model architecture, training procedure, number of samples,
and data distribution. We give empirical evidence for these conjectures across
a variety of domains in machine learning, including neural networks, kernel
machines, and decision trees. Our results thus advance our empirical
understanding of interpolating classifiers."
"['stat.ML', 'stat.TH']",Rare-Event Simulation for Neural Network and Random Forest Predictors,"We study rare-event simulation for a class of problems where the target
hitting sets of interest are defined via modern machine learning tools such as
neural networks and random forests. This problem is motivated from fast
emerging studies on the safety evaluation of intelligent systems, robustness
quantification of learning models, and other potential applications to
large-scale simulation in which machine learning tools can be used to
approximate complex rare-event set boundaries. We investigate an importance
sampling scheme that integrates the dominating point machinery in large
deviations and sequential mixed integer programming to locate the underlying
dominating points. Our approach works for a range of neural network
architectures including fully connected layers, rectified linear units,
normalization, pooling and convolutional layers, and random forests built from
standard decision trees. We provide efficiency guarantees and numerical
demonstration of our approach using a classification model in the UCI Machine
Learning Repository."
"['stat.ML', 'stat.ME']","Transparency, Auditability and eXplainability of Machine Learning Models in Credit Scoring","A major requirement for credit scoring models is to provide a maximally
accurate risk prediction. Additionally, regulators demand these models to be
transparent and auditable. Thus, in credit scoring, very simple predictive
models such as logistic regression or decision trees are still widely used and
the superior predictive power of modern machine learning algorithms cannot be
fully leveraged. Significant potential is therefore missed, leading to higher
reserves or more credit defaults. This paper works out different dimensions
that have to be considered for making credit scoring models understandable and
presents a framework for making ``black box'' machine learning models
transparent, auditable and explainable. Following this framework, we present an
overview of techniques, demonstrate how they can be applied in credit scoring
and how results compare to the interpretability of score cards. A real world
case study shows that a comparable degree of interpretability can be achieved
while machine learning techniques keep their ability to improve predictive
power."
"['stat.ML', 'stat.CO']",StructureBoost: Efficient Gradient Boosting for Structured Categorical Variables,"Gradient boosting methods based on Structured Categorical Decision Trees
(SCDT) have been demonstrated to outperform numerical and one-hot-encodings on
problems where the categorical variable has a known underlying structure.
However, the enumeration procedure in the SCDT is infeasible except for
categorical variables with low or moderate cardinality. We propose and
implement two methods to overcome the computational obstacles and efficiently
perform Gradient Boosting on complex structured categorical variables. The
resulting package, called StructureBoost, is shown to outperform established
packages such as CatBoost and LightGBM on problems with categorical predictors
that contain sophisticated structure. Moreover, we demonstrate that
StructureBoost can make accurate predictions on unseen categorical values due
to its knowledge of the underlying structure."
"['stat.ML', 'stat.TH']",On the consistency of supervised learning with missing values,"In many application settings, the data have missing entries which make
analysis challenging. An abundant literature addresses missing values in an
inferential framework: estimating parameters and their variance from incomplete
tables. Here, we consider supervised-learning settings: predicting a target
when missing values appear in both training and testing data. We show the
consistency of two approaches in prediction. A striking result is that the
widely-used method of imputing with a constant, such as the mean prior to
learning is consistent when missing values are not informative. This contrasts
with inferential settings where mean imputation is pointed at for distorting
the distribution of the data. That such a simple approach can be consistent is
important in practice. We also show that a predictor suited for complete
observations can predict optimally on incomplete data,through multiple
imputation.Finally, to compare imputation with learning directly with a model
that accounts for missing values, we analyze further decision trees. These can
naturally tackle empirical risk minimization with missing values, due to their
ability to handle the half-discrete nature of incomplete variables. After
comparing theoretically and empirically different missing values strategies in
trees, we recommend using the ""missing incorporated in attribute"" method as it
can handle both non-informative and informative missing values."
"['stat.ML', 'stat.TH']",Multi-Scale Vector Quantization with Reconstruction Trees,"We propose and study a multi-scale approach to vector quantization. We
develop an algorithm, dubbed reconstruction trees, inspired by decision trees.
Here the objective is parsimonious reconstruction of unsupervised data, rather
than classification. Contrasted to more standard vector quantization methods,
such as K-means, the proposed approach leverages a family of given partitions,
to quickly explore the data in a coarse to fine-- multi-scale-- fashion. Our
main technical contribution is an analysis of the expected distortion achieved
by the proposed algorithm, when the data are assumed to be sampled from a fixed
unknown distribution. In this context, we derive both asymptotic and finite
sample results under suitable regularity assumptions on the distribution. As a
special case, we consider the setting where the data generating distribution is
supported on a compact Riemannian sub-manifold. Tools from differential
geometry and concentration of measure are useful in our analysis."
"['stat.ML', 'stat.CO', 'stat.ME']",Subsampling Bias and The Best-Discrepancy Systematic Cross Validation,"Statistical machine learning models should be evaluated and validated before
putting to work. Conventional k-fold Monte Carlo Cross-Validation (MCCV)
procedure uses a pseudo-random sequence to partition instances into k subsets,
which usually causes subsampling bias, inflates generalization errors and
jeopardizes the reliability and effectiveness of cross-validation. Based on
ordered systematic sampling theory in statistics and low-discrepancy sequence
theory in number theory, we propose a new k-fold cross-validation procedure by
replacing a pseudo-random sequence with a best-discrepancy sequence, which
ensures low subsampling bias and leads to more precise
Expected-Prediction-Error estimates. Experiments with 156 benchmark datasets
and three classifiers (logistic regression, decision tree and naive bayes) show
that in general, our cross-validation procedure can extrude subsampling bias in
the MCCV by lowering the EPE around 7.18% and the variances around 26.73%. In
comparison, the stratified MCCV can reduce the EPE and variances of the MCCV
around 1.58% and 11.85% respectively. The Leave-One-Out (LOO) can lower the EPE
around 2.50% but its variances are much higher than the any other CV procedure.
The computational time of our cross-validation procedure is just 8.64% of the
MCCV, 8.67% of the stratified MCCV and 16.72% of the LOO. Experiments also show
that our approach is more beneficial for datasets characterized by relatively
small size and large aspect ratio. This makes our approach particularly
pertinent when solving bioscience classification problems. Our proposed
systematic subsampling technique could be generalized to other machine learning
algorithms that involve random subsampling mechanism."
"['stat.ML', 'stat.ME']",Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers,"There is a large literature explaining why AdaBoost is a successful
classifier. The literature on AdaBoost focuses on classifier margins and
boosting's interpretation as the optimization of an exponential likelihood
function. These existing explanations, however, have been pointed out to be
incomplete. A random forest is another popular ensemble method for which there
is substantially less explanation in the literature. We introduce a novel
perspective on AdaBoost and random forests that proposes that the two
algorithms work for similar reasons. While both classifiers achieve similar
predictive accuracy, random forests cannot be conceived as a direct
optimization procedure. Rather, random forests is a self-averaging,
interpolating algorithm which creates what we denote as a ""spikey-smooth""
classifier, and we view AdaBoost in the same light. We conjecture that both
AdaBoost and random forests succeed because of this mechanism. We provide a
number of examples and some theoretical justification to support this
explanation. In the process, we question the conventional wisdom that suggests
that boosting algorithms for classification require regularization or early
stopping and should be limited to low complexity classes of learners, such as
decision stumps. We conclude that boosting should be used like random forests:
with large decision trees and without direct regularization or early stopping."
"['stat.ML', 'stat.TH']",Random Forests for Big Data,"Big Data is one of the major challenges of statistical science and has
numerous consequences from algorithmic and theoretical viewpoints. Big Data
always involve massive data but they also often include online data and data
heterogeneity. Recently some statistical methods have been adapted to process
Big Data, like linear regression models, clustering methods and bootstrapping
schemes. Based on decision trees combined with aggregation and bootstrap ideas,
random forests were introduced by Breiman in 2001. They are a powerful
nonparametric statistical method allowing to consider in a single and versatile
framework regression problems, as well as two-class and multi-class
classification problems. Focusing on classification problems, this paper
proposes a selective review of available proposals that deal with scaling
random forests to Big Data problems. These proposals rely on parallel
environments or on online adaptations of random forests. We also describe how
related quantities -- such as out-of-bag error and variable importance -- are
addressed in these methods. Then, we formulate various remarks for random
forests in the Big Data context. Finally, we experiment five variants on two
massive datasets (15 and 120 millions of observations), a simulated one as well
as real world data. One variant relies on subsampling while three others are
related to parallel implementations of random forests and involve either
various adaptations of bootstrap to Big Data or to ""divide-and-conquer""
approaches. The fifth variant relates on online learning of random forests.
These numerical experiments lead to highlight the relative performance of the
different variants, as well as some of their limitations."
"['stat.ML', 'stat.CO']",Particle Gibbs for Bayesian Additive Regression Trees,"Additive regression trees are flexible non-parametric models and popular
off-the-shelf tools for real-world non-linear regression. In application
domains, such as bioinformatics, where there is also demand for probabilistic
predictions with measures of uncertainty, the Bayesian additive regression
trees (BART) model, introduced by Chipman et al. (2010), is increasingly
popular. As data sets have grown in size, however, the standard
Metropolis-Hastings algorithms used to perform inference in BART are proving
inadequate. In particular, these Markov chains make local changes to the trees
and suffer from slow mixing when the data are high-dimensional or the best
fitting trees are more than a few layers deep. We present a novel sampler for
BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a
top-down particle filtering algorithm for Bayesian decision trees
(Lakshminarayanan et al., 2013). Rather than making local changes to individual
trees, the PG sampler proposes a complete tree to fit the residual. Experiments
show that the PG sampler outperforms existing samplers in many settings."
"['stat.ML', 'stat.TH']",Analysis of a Random Forests Model,"Random forests are a scheme proposed by Leo Breiman in the 2000's for
building a predictor ensemble with a set of decision trees that grow in
randomly selected subspaces of data. Despite growing interest and practical
use, there has been little exploration of the statistical properties of random
forests, and little is known about the mathematical forces driving the
algorithm. In this paper, we offer an in-depth analysis of a random forests
model suggested by Breiman in \cite{Bre04}, which is very close to the original
algorithm. We show in particular that the procedure is consistent and adapts to
sparsity, in the sense that its rate of convergence depends only on the number
of strong features and not on how many noise variables are present."
"['stat.ML', 'stat.CO']",Dimension Reduction Using Rule Ensemble Machine Learning Methods: A Numerical Study of Three Ensemble Methods,"Ensemble methods for supervised machine learning have become popular due to
their ability to accurately predict class labels with groups of simple,
lightweight ""base learners."" While ensembles offer computationally efficient
models that have good predictive capability they tend to be large and offer
little insight into the patterns or structure in a dataset. We consider an
ensemble technique that returns a model of ranked rules. The model accurately
predicts class labels and has the advantage of indicating which parameter
constraints are most useful for predicting those labels. An example of the rule
ensemble method successfully ranking rules and selecting attributes is given
with a dataset containing images of potential supernovas where the number of
necessary features is reduced from 39 to 21. We also compare the rule ensemble
method on a set of multi-class problems with boosting and bagging, which are
two well known ensemble techniques that use decision trees as base learners,
but do not have a rule ranking scheme."
['stat.ME'],Towards a Rigorous Evaluation of Time-series Anomaly Detection,"In recent years, proposed studies on time-series anomaly detection (TAD)
report high F1 scores on benchmark TAD datasets, giving the impression of clear
improvements. However, most studies apply a peculiar evaluation protocol called
point adjustment (PA) before scoring. In this paper, we theoretically and
experimentally reveal that the PA protocol has a great possibility of
overestimating the detection performance; that is, even a random anomaly score
can easily turn into a state-of-the-art TAD method. Therefore, the comparison
of TAD methods with F1 scores after the PA protocol can lead to misguided
rankings. Furthermore, we question the potential of existing TAD methods by
showing that an untrained model obtains comparable detection performance to the
existing methods even without PA. Based on our findings, we propose a new
baseline and an evaluation protocol. We expect that our study will help a
rigorous evaluation of TAD and lead to further improvement in future
researches."
"['stat.CO', 'stat.TH']",Accurate shape and phase averaging of time series through Dynamic Time Warping,"We propose a novel time series averaging method based on Dynamic Time Warping
(DTW). In contrast to previous methods, our algorithm preserves durational
information and the distinctive durational features of the sequences due to a
simple conversion of the output of DTW into a time sequence and an innovative
iterative averaging process. We show that it accurately estimates the ground
truth mean sequences and mean temporal location of landmarks in synthetic and
real-world datasets and outperforms state-of-the-art methods."
['stat.ME'],High dimensional Bayesian Optimization Algorithm for Complex System in Time Series,"At present, high-dimensional global optimization problems with time-series
models have received much attention from engineering fields. Since it was
proposed, Bayesian optimization has quickly become a popular and promising
approach for solving global optimization problems. However, the standard
Bayesian optimization algorithm is insufficient to solving the global optimal
solution when the model is high-dimensional. Hence, this paper presents a novel
high dimensional Bayesian optimization algorithm by considering dimension
reduction and different dimension fill-in strategies. Most existing literature
about Bayesian optimization algorithms did not discuss the sampling strategies
to optimize the acquisition function. This study proposed a new sampling method
based on both the multi-armed bandit and random search methods while optimizing
the acquisition function. Besides, based on the time-dependent or
dimension-dependent characteristics of the model, the proposed algorithm can
reduce the dimension evenly. Then, five different dimension fill-in strategies
were discussed and compared in this study. Finally, to increase the final
accuracy of the optimal solution, the proposed algorithm adds a local search
based on a series of Adam-based steps at the final stage. Our computational
experiments demonstrated that the proposed Bayesian optimization algorithm
could achieve reasonable solutions with excellent performances for high
dimensional global optimization problems with a time-series optimal control
model."
['stat.ME'],Improving COVID-19 Forecasting using eXogenous Variables,"In this work, we study the pandemic course in the United States by
considering national and state levels data. We propose and compare multiple
time-series prediction techniques which incorporate auxiliary variables. One
type of approach is based on spatio-temporal graph neural networks which
forecast the pandemic course by utilizing a hybrid deep learning architecture
and human mobility data. Nodes in this graph represent the state-level deaths
due to COVID-19, edges represent the human mobility trend and temporal edges
correspond to node attributes across time. The second approach is based on a
statistical technique for COVID-19 mortality prediction in the United States
that uses the SARIMA model and eXogenous variables. We evaluate these
techniques on both state and national levels COVID-19 data in the United States
and claim that the SARIMA and MCP models generated forecast values by the
eXogenous variables can enrich the underlying model to capture complexity in
respectively national and state levels data. We demonstrate significant
enhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum
improvement in forecasting accuracy by 64.58% and 59.18% (on average) over the
GCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)
over the GCN-LSTM model in the state level data. Additionally, our proposed
model outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy
on average."
"['stat.ML', 'stat.OT']",Evolving-Graph Gaussian Processes,"Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph
structured domains. Existing approaches have focused on static structures,
whereas many real graph data represent a dynamic structure, limiting the
applications of GGPs. To overcome this we propose evolving-Graph Gaussian
Processes (e-GGPs). The proposed method is capable of learning the transition
function of graph vertices over time with a neighbourhood kernel to model the
connectivity and interaction changes between vertices. We assess the
performance of our method on time-series regression problems where graphs
evolve over time. We demonstrate the benefits of e-GGPs over static graph
Gaussian Process approaches."
"['stat.ML', 'stat.ME']",Improved Prediction and Network Estimation Using the Monotone Single Index Multi-variate Autoregressive Model,"Network estimation from multi-variate point process or time series data is a
problem of fundamental importance. Prior work has focused on parametric
approaches that require a known parametric model, which makes estimation
procedures less robust to model mis-specification, non-linearities and
heterogeneities. In this paper, we develop a semi-parametric approach based on
the monotone single-index multi-variate autoregressive model (SIMAM) which
addresses these challenges. We provide theoretical guarantees for dependent
data and an alternating projected gradient descent algorithm. Significantly we
do not explicitly assume mixing conditions on the process (although we do
require conditions analogous to restricted strong convexity) and we achieve
rates of the form $O(T^{-\frac{1}{3}} \sqrt{s\log(TM)})$ (optimal in the
independent design case) where $s$ is the threshold for the maximum in-degree
of the network that indicates the sparsity level, $M$ is the number of actors
and $T$ is the number of time points. In addition, we demonstrate the superior
performance both on simulated data and two real data examples where our SIMAM
approach out-performs state-of-the-art parametric methods both in terms of
prediction and network estimation."
"['stat.ML', 'stat.ME', 'stat.TH']",Random Forests for dependent data,"Random forest (RF) is one of the most popular methods for estimating
regression functions. The local nature of the RF algorithm, based on intra-node
means and variances, is ideal when errors are i.i.d. For dependent error
processes like time series and spatial settings where data in all the nodes
will be correlated, operating locally ignores this dependence. Also, RF will
involve resampling of correlated data, violating the principles of bootstrap.
Theoretically, consistency of RF has been established for i.i.d. errors, but
little is known about the case of dependent errors.
  We propose RF-GLS, a novel extension of RF for dependent error processes in
the same way Generalized Least Squares (GLS) fundamentally extends Ordinary
Least Squares (OLS) for linear models under dependence. The key to this
extension is the equivalent representation of the local decision-making in a
regression tree as a global OLS optimization which is then replaced with a GLS
loss to create a GLS-style regression tree. This also synergistically addresses
the resampling issue, as the use of GLS loss amounts to resampling uncorrelated
contrasts (pre-whitened data) instead of the correlated data. For spatial
settings, RF-GLS can be used in conjunction with Gaussian Process correlated
errors to generate kriging predictions at new locations. RF becomes a special
case of RF-GLS with an identity working covariance matrix.
  We establish consistency of RF-GLS under beta- (absolutely regular) mixing
error processes and show that this general result subsumes important cases like
autoregressive time series and spatial Matern Gaussian Processes. As a
byproduct, we also establish consistency of RF for beta-mixing processes, which
to our knowledge, is the first such result for RF under dependence.
  We empirically demonstrate the improvement achieved by RF-GLS over RF for
both estimation and prediction under dependence."
"['stat.ME', 'stat.ML']",Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal Stochastic Linear Mixing Model,"Many modern time-series datasets contain large numbers of output response
variables sampled for prolonged periods of time. For example, in neuroscience,
the activities of 100s-1000's of neurons are recorded during behaviors and in
response to sensory stimuli. Multi-output Gaussian process models leverage the
nonparametric nature of Gaussian processes to capture structure across multiple
outputs. However, this class of models typically assumes that the correlations
between the output response variables are invariant in the input space.
Stochastic linear mixing models (SLMM) assume the mixture coefficients depend
on input, making them more flexible and effective to capture complex output
dependence. However, currently, the inference for SLMMs is intractable for
large datasets, making them inapplicable to several modern time-series
problems. In this paper, we propose a new regression framework, the orthogonal
stochastic linear mixing model (OSLMM) that introduces an orthogonal constraint
amongst the mixing coefficients. This constraint reduces the computational
burden of inference while retaining the capability to handle complex output
dependence. We provide Markov chain Monte Carlo inference procedures for both
SLMM and OSLMM and demonstrate superior model scalability and reduced
prediction error of OSLMM compared with state-of-the-art methods on several
real-world applications. In neurophysiology recordings, we use the inferred
latent functions for compact visualization of population responses to auditory
stimuli, and demonstrate superior results compared to a competing method
(GPFA). Together, these results demonstrate that OSLMM will be useful for the
analysis of diverse, large-scale time-series datasets."
"['stat.ML', 'stat.TH']",Fast and Robust Online Inference with Stochastic Gradient Descent via Random Scaling,"We develop a new method of online inference for a vector of parameters
estimated by the Polyak-Ruppert averaging procedure of stochastic gradient
descent (SGD) algorithms. We leverage insights from time series regression in
econometrics and construct asymptotically pivotal statistics via random
scaling. Our approach is fully operational with online data and is rigorously
underpinned by a functional central limit theorem. Our proposed inference
method has a couple of key advantages over the existing methods. First, the
test statistic is computed in an online fashion with only SGD iterates and the
critical values can be obtained without any resampling methods, thereby
allowing for efficient implementation suitable for massive online data. Second,
there is no need to estimate the asymptotic variance and our inference method
is shown to be robust to changes in the tuning parameters for SGD algorithms in
simulation experiments with synthetic data."
"['stat.ML', 'stat.CO']",Spliced Binned-Pareto Distribution for Robust Modeling of Heavy-tailed Time Series,"This work proposes a novel method to robustly and accurately model time
series with heavy-tailed noise, in non-stationary scenarios. In many practical
application time series have heavy-tailed noise that significantly impacts the
performance of classical forecasting models; in particular, accurately modeling
a distribution over extreme events is crucial to performing accurate time
series anomaly detection. We propose a Spliced Binned-Pareto distribution which
is both robust to extreme observations and allows accurate modeling of the full
distribution. Our method allows the capture of time dependencies in the higher
order moments of the distribution such as the tail heaviness. We compare the
robustness and the accuracy of the tail estimation of our method to other state
of the art methods on Twitter mentions count time series."
"['stat.ML', 'stat.ME']",A Multilayered Block Network Model to Forecast Large Dynamic Transportation Graphs: an Application to US Air Transport,"Dynamic transportation networks have been analyzed for years by means of
static graph-based indicators in order to study the temporal evolution of
relevant network components, and to reveal complex dependencies that would not
be easily detected by a direct inspection of the data. This paper presents a
state-of-the-art latent network model to forecast multilayer dynamic graphs
that are increasingly common in transportation and proposes a community-based
extension to reduce the computational burden. Flexible time series analysis is
obtained by modeling the probability of edges between vertices through latent
Gaussian processes. The models and Bayesian inference are illustrated on a
sample of 10-year data from four major airlines within the US air
transportation system. Results show how the estimated latent parameters from
the models are related to the airline's connectivity dynamics, and their
ability to project the multilayer graph into the future for out-of-sample full
network forecasts, while stochastic blockmodeling allows for the identification
of relevant communities. Reliable network predictions would allow policy-makers
to better understand the dynamics of the transport system, and help in their
planning on e.g. route development, or the deployment of new regulations."
"['stat.ME', 'stat.ML']",Probability Paths and the Structure of Predictions over Time,"In settings ranging from weather forecasts to political prognostications to
financial projections, probability estimates of future binary outcomes often
evolve over time. For example, the estimated likelihood of rain on a specific
day changes by the hour as new information becomes available. Given a
collection of such probability paths, we introduce a Bayesian framework --
which we call the Gaussian latent information martingale, or GLIM -- for
modeling the structure of dynamic predictions over time. Suppose, for example,
that the likelihood of rain in a week is 50%, and consider two hypothetical
scenarios. In the first, one expects the forecast is equally likely to become
either 25% or 75% tomorrow; in the second, one expects the forecast to stay
constant for the next several days. A time-sensitive decision-maker might
select a course of action immediately in the latter scenario, but may postpone
their decision in the former, knowing that new information is imminent. We
model these trajectories by assuming predictions update according to a latent
process of information flow, which is inferred from historical data. In
contrast to general methods for time series analysis, this approach preserves
the martingale structure of probability paths and better quantifies future
uncertainties around probability paths. We show that GLIM outperforms three
popular baseline methods, producing better estimated posterior probability path
distributions measured by three different metrics. By elucidating the dynamic
structure of predictions over time, we hope to help individuals make more
informed choices."
"['stat.ME', 'stat.TH']",Inferring Granger Causality from Irregularly Sampled Time Series,"Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability."
['stat.ME'],Improving Neural Networks for Time Series Forecasting using Data Augmentation and AutoML,"Statistical methods such as the Box-Jenkins method for time-series
forecasting have been prominent since their development in 1970. Many
researchers rely on such models as they can be efficiently estimated and also
provide interpretability. However, advances in machine learning research
indicate that neural networks can be powerful data modeling techniques, as they
can give higher accuracy for a plethora of learning problems and datasets. In
the past, they have been tried on time-series forecasting as well, but their
overall results have not been significantly better than the statistical models
especially for intermediate length times series data. Their modeling capacities
are limited in cases where enough data may not be available to estimate the
large number of parameters that these non-linear models require. This paper
presents an easy to implement data augmentation method to significantly improve
the performance of such networks. Our method, Augmented-Neural-Network, which
involves using forecasts from statistical models, can help unlock the power of
neural networks on intermediate length time-series and produces competitive
results. It shows that data augmentation, when paired with Automated Machine
Learning techniques such as Neural Architecture Search, can help to find the
best neural architecture for a given time-series. Using the combination of
these, demonstrates significant enhancement in the forecasting accuracy of
three neural network-based models for a COVID-19 dataset, with a maximum
improvement in forecasting accuracy by 21.41%, 24.29%, and 16.42%,
respectively, over the neural networks that do not use augmented data."
"['stat.ML', 'stat.CO']",tsrobprep -- an R package for robust preprocessing of time series data,"Data cleaning is a crucial part of every data analysis exercise. Yet, the
currently available R packages do not provide fast and robust methods for
cleaning and preparation of time series data. The open source package tsrobprep
introduces efficient methods for handling missing values and outliers using
model based approaches. For data imputation a probabilistic replacement model
is proposed, which may consist of autoregressive components and external
inputs. For outlier detection a clustering algorithm based on finite mixture
modelling is introduced, which considers typical time series related properties
as features. By assigning to each observation a probability of being an
outlying data point, the degree of outlyingness can be determined. The methods
work robust and are fully tunable. Moreover, by providing the
auto_data_cleaning function the data preprocessing can be carried out in one
cast, without manual tuning and providing suitable results. The primary
motivation of the package is the preprocessing of energy system data, however,
the package is also suited for other moderate and large sized time series data
set. We present application for electricity load, wind and solar power data."
['stat.TH'],Stochastic Online Convex Optimization; Application to probabilistic time series forecasting,"Stochastic regret bounds for online algorithms are usually derived from an
''online to batch'' conversion. Inverting the reasoning, we start our analyze
by a ''batch to online'' conversion that applies in any Stochastic Online
Convex Optimization problem under stochastic exp-concavity condition. We obtain
fast rate stochastic regret bounds with high probability for non-convex loss
functions. Based on this approach, we provide prediction and probabilistic
forecasting methods for non-stationary unbounded time series."
"['stat.ML', 'stat.ME']",Probabilistic water demand forecasting using quantile regression algorithms,"Machine and statistical learning algorithms can be reliably automated and
applied at scale. Therefore, they can constitute a considerable asset for
designing practical forecasting systems, such as those related to urban water
demand. Quantile regression algorithms are statistical and machine learning
algorithms that can provide probabilistic forecasts in a straightforward way,
and have not been applied so far for urban water demand forecasting. In this
work, we aim to fill this gap by automating and extensively comparing several
quantile-regression-based practical systems for probabilistic one-day ahead
urban water demand forecasting. For designing the practical systems, we use
five individual algorithms (i.e., the quantile regression, linear boosting,
generalized random forest, gradient boosting machine and quantile regression
neural network algorithms), their mean combiner and their median combiner. The
comparison is conducted by exploiting a large urban water flow dataset, as well
as several types of hydrometeorological time series (which are considered as
exogenous predictor variables in the forecasting setting). The results mostly
favour the practical systems designed using the linear boosting algorithm,
probably due to the presence of trends in the urban water flow time series. The
forecasts of the mean and median combiners are also found to be skilful in
general terms."
"['stat.ML', 'stat.ME']",COVID-19 mortality analysis from soft-data multivariate curve regression and machine learning,"A multiple objective space-time forecasting approach is presented involving
cyclical curve log-regression, and multivariate time series spatial residual
correlation analysis. Specifically, the mean quadratic loss function is
minimized in the framework of trigonometric regression. While, in our
subsequent spatial residual correlation analysis, maximization of the
likelihood allows us to compute the posterior mode in a Bayesian multivariate
time series soft-data framework. The presented approach is applied to the
analysis of COVID-19 mortality in the first wave affecting the Spanish
Communities, since March, 8, 2020 until May, 13, 2020. An empirical comparative
study with Machine Learning (ML) regression, based on random k-fold
cross-validation, and bootstrapping confidence interval and probability density
estimation, is carried out. This empirical analysis also investigates the
performance of ML regression models in a hard- and soft- data frameworks. The
results could be extrapolated to other counts, countries, and posterior
COVID-19 waves."
"['stat.ML', 'stat.CO']",Probabilistic sequential matrix factorization,"We introduce the probabilistic sequential matrix factorization (PSMF) method
for factorizing time-varying and non-stationary datasets consisting of
high-dimensional time-series. In particular, we consider nonlinear Gaussian
state-space models where sequential approximate inference results in the
factorization of a data matrix into a dictionary and time-varying coefficients
with potentially nonlinear Markovian dependencies. The assumed Markovian
structure on the coefficients enables us to encode temporal dependencies into a
low-dimensional feature space. The proposed inference method is solely based on
an approximate extended Kalman filtering scheme, which makes the resulting
method particularly efficient. PSMF can account for temporal nonlinearities
and, more importantly, can be used to calibrate and estimate generic
differentiable nonlinear subspace models. We also introduce a robust version of
PSMF, called rPSMF, which uses Student-t filters to handle model
misspecification. We show that PSMF can be used in multiple contexts: modeling
time series with a periodic subspace, robustifying changepoint detection
methods, and imputing missing data in several high-dimensional time-series,
such as measurements of pollutants across London."
"['stat.ML', 'stat.ME']",Efficient Variational Bayesian Structure Learning of Dynamic Graphical Models,"Estimating time-varying graphical models are of paramount importance in
various social, financial, biological, and engineering systems, since the
evolution of such networks can be utilized for example to spot trends, detect
anomalies, predict vulnerability, and evaluate the impact of interventions.
Existing methods require extensive tuning of parameters that control the graph
sparsity and temporal smoothness. Furthermore, these methods are
computationally burdensome with time complexity O(NP^3) for P variables and N
time points. As a remedy, we propose a low-complexity tuning-free Bayesian
approach, named BADGE. Specifically, we impose temporally-dependent
spike-and-slab priors on the graphs such that they are sparse and varying
smoothly across time. A variational inference algorithm is then derived to
learn the graph structures from the data automatically. Owning to the
pseudo-likelihood and the mean-field approximation, the time complexity of
BADGE is only O(NP^2). Additionally, by identifying the frequency-domain
resemblance to the time-varying graphical models, we show that BADGE can be
extended to learning frequency-varying inverse spectral density matrices, and
yields graphical models for multivariate stationary time series. Numerical
results on both synthetic and real data show that that BADGE can better recover
the underlying true graphs, while being more efficient than the existing
methods, especially for high-dimensional cases."
"['stat.ML', 'stat.ME']",Automatic Registration and Clustering of Time Series,"Clustering of time series data exhibits a number of challenges not present in
other settings, notably the problem of registration (alignment) of observed
signals. Typical approaches include pre-registration to a user-specified
template or time warping approaches which attempt to optimally align series
with a minimum of distortion. For many signals obtained from recording or
sensing devices, these methods may be unsuitable as a template signal is not
available for pre-registration, while the distortion of warping approaches may
obscure meaningful temporal information. We propose a new method for automatic
time series alignment within a clustering problem. Our approach, Temporal
Registration using Optimal Unitary Transformations (TROUT), is based on a novel
dissimilarity measure between time series that is easy to compute and
automatically identifies optimal alignment between pairs of time series. By
embedding our new measure in a optimization formulation, we retain well-known
advantages of computational and statistical performance. We provide an
efficient algorithm for TROUT-based clustering and demonstrate its superior
performance over a range of competitors."
"['stat.ML', 'stat.TH']",Forecasting Nonnegative Time Series via Sliding Mask Method (SMM) and Latent Clustered Forecast (LCF),"We consider nonnegative time series forecasting framework. Based on recent
advances in Nonnegative Matrix Factorization (NMF) and Archetypal Analysis, we
introduce two procedures referred to as Sliding Mask Method (SMM) and Latent
Clustered Forecast (LCF). SMM is a simple and powerful method based on time
window prediction using Completion of Nonnegative Matrices. This new procedure
combines low nonnegative rank decomposition and matrix completion where the
hidden values are to be forecasted. LCF is two stage: it leverages archetypal
analysis for dimension reduction and clustering of time series, then it uses
any black-box supervised forecast solver on the clustered latent
representation. Theoretical guarantees on uniqueness and robustness of the
solution of NMF Completion-type problems are also provided for the first time.
Finally, numerical experiments on real-world and synthetic data-set confirms
forecasting accuracy for both the methodologies."
['stat.TH'],Few-shot time series segmentation using prototype-defined infinite hidden Markov models,"We propose a robust framework for interpretable, few-shot analysis of
non-stationary sequential data based on flexible graphical models to express
the structured distribution of sequential events, using prototype radial basis
function (RBF) neural network emissions. A motivational link is demonstrated
between prototypical neural network architectures for few-shot learning and the
proposed RBF network infinite hidden Markov model (RBF-iHMM). We show that RBF
networks can be efficiently specified via prototypes allowing us to express
complex nonstationary patterns, while hidden Markov models are used to infer
principled high-level Markov dynamics. The utility of the framework is
demonstrated on biomedical signal processing applications such as automated
seizure detection from EEG data where RBF networks achieve state-of-the-art
performance using a fraction of the data needed to train long-short-term memory
variational autoencoders."
"['stat.ME', 'stat.ML']",Time Series (re)sampling using Generative Adversarial Networks,"We propose a novel bootstrap procedure for dependent data based on Generative
Adversarial networks (GANs). We show that the dynamics of common stationary
time series processes can be learned by GANs and demonstrate that GANs trained
on a single sample path can be used to generate additional samples from the
process. We find that temporal convolutional neural networks provide a suitable
design for the generator and discriminator, and that convincing samples can be
generated on the basis of a vector of iid normal noise. We demonstrate the
finite sample properties of GAN sampling and the suggested bootstrap using
simulations where we compare the performance to circular block bootstrapping in
the case of resampling an AR(1) time series processes. We find that resampling
using the GAN can outperform circular block bootstrapping in terms of empirical
coverage."
"['stat.ML', 'stat.TH']",Fairness in Forecasting and Learning Linear Dynamical Systems,"In machine learning, training data often capture the behaviour of multiple
subgroups of some underlying human population. When the amounts of training
data for the subgroups are not controlled carefully, under-representation bias
arises. We introduce two natural notions of subgroup fairness and instantaneous
fairness to address such under-representation bias in time-series forecasting
problems. In particular, we consider the subgroup-fair and instant-fair
learning of a linear dynamical system (LDS) from multiple trajectories of
varying lengths, and the associated forecasting problems. We provide globally
convergent methods for the learning problems using hierarchies of
convexifications of non-commutative polynomial optimisation problems. Our
empirical results on a biased data set motivated by insurance applications and
the well-known COMPAS data set demonstrate both the beneficial impact of
fairness considerations on statistical performance and encouraging effects of
exploiting sparsity on run time."
"['stat.CO', 'stat.ME']",A Worrying Analysis of Probabilistic Time-series Models for Sales Forecasting,"Probabilistic time-series models become popular in the forecasting field as
they help to make optimal decisions under uncertainty. Despite the growing
interest, a lack of thorough analysis hinders choosing what is worth applying
for the desired task. In this paper, we analyze the performance of three
prominent probabilistic time-series models for sales forecasting. To remove the
role of random chance in architecture's performance, we make two experimental
principles; 1) Large-scale dataset with various cross-validation sets. 2) A
standardized training and hyperparameter selection. The experimental results
show that a simple Multi-layer Perceptron and Linear Regression outperform the
probabilistic models on RMSE without any feature engineering. Overall, the
probabilistic models fail to achieve better performance on point estimation,
such as RMSE and MAPE, than comparably simple baselines. We analyze and discuss
the performances of probabilistic time-series models."
"['stat.ML', 'stat.ME']",Meta-Learning for Time Series Forecasting Ensemble,"Amounts of historical data collected increase together with business
intelligence applicability and demands for automatic forecasting of time
series. While no single time series modeling method is universal to all types
of dynamics, forecasting using ensemble of several methods is often seen as a
compromise. Instead of fixing ensemble diversity and size we propose to
adaptively predict these aspects using meta-learning. Meta-learning here
considers two separate random forest regression models, built on 390 time
series features, to rank 22 univariate forecasting methods and to recommend
ensemble size. Forecasting ensemble is consequently formed from methods ranked
as the best and forecasts are pooled using either simple or weighted average
(with weight corresponding to reciprocal rank). Proposed approach was tested on
12561 micro-economic time series (expanded to 38633 for various forecasting
horizons) of M4 competition where meta-learning outperformed Theta and Comb
benchmarks by relative forecasting errors for all data types and horizons. Best
overall results were achieved by weighted pooling with symmetric mean absolute
percentage error of 9.21% versus 11.05% obtained using Theta method."
"['stat.ML', 'stat.TH']",Learning CHARME models with neural networks,"In this paper, we consider a model called CHARME (Conditional Heteroscedastic
Autoregressive Mixture of Experts), a class of generalized mixture of nonlinear
nonparametric AR-ARCH time series. Under certain Lipschitz-type conditions on
the autoregressive and volatility functions, we prove that this model is
stationary, ergodic and $\tau$-weakly dependent. These conditions are much
weaker than those presented in the literature that treats this model. Moreover,
this result forms the theoretical basis for deriving an asymptotic theory of
the underlying (non)parametric estimation, which we present for this model. As
an application, from the universal approximation property of neural networks
(NN), we develop a learning theory for the NN-based autoregressive functions of
the model, where the strong consistency and asymptotic normality of the
considered estimator of the NN weights and biases are guaranteed under weak
conditions."
['stat.TH'],Non-stationary Online Regression,"Online forecasting under a changing environment has been a problem of
increasing importance in many real-world applications. In this paper, we
consider the meta-algorithm presented in \citet{zhang2017dynamic} combined with
different subroutines. We show that an expected cumulative error of order
$\tilde{O}(n^{1/3} C_n^{2/3})$ can be obtained for non-stationary online linear
regression where the total variation of parameter sequence is bounded by $C_n$.
Our paper extends the result of online forecasting of one-dimensional
time-series as proposed in \cite{baby2019online} to general $d$-dimensional
non-stationary linear regression. We improve the rate $O(\sqrt{n C_n})$
obtained by Zhang et al. 2017 and Besbes et al. 2015. We further extend our
analysis to non-stationary online kernel regression. Similar to the
non-stationary online regression case, we use the meta-procedure of Zhang et
al. 2017 combined with Kernel-AWV (Jezequel et al. 2020) to achieve an expected
cumulative controlled by the effective dimension of the RKHS and the total
variation of the sequence. To the best of our knowledge, this work is the first
extension of non-stationary online regression to non-stationary kernel
regression. Lastly, we evaluate our method empirically with several existing
benchmarks and also compare it with the theoretical bound obtained in this
paper."
"['stat.ML', 'stat.ME']",Change point detection for graphical models in the presence of missing values,"We propose estimation methods for change points in high-dimensional
covariance structures with an emphasis on challenging scenarios with missing
values. We advocate three imputation like methods and investigate their
implications on common losses used for change point detection. We also discuss
how model selection methods have to be adapted to the setting of incomplete
data. The methods are compared in a simulation study and applied to a time
series from an environmental monitoring system. An implementation of our
proposals within the R-package hdcd is available via the Supplementary
materials."
"['stat.ML', 'stat.TH']",Multivariate Quantile Bayesian Structural Time Series (MQBSTS) Model,"In this paper, we propose the multivariate quantile Bayesian structural time
series (MQBSTS) model for the joint quantile time series forecast, which is the
first such model for correlated multivariate time series to the author's best
knowledge. The MQBSTS model also enables quantile based feature selection in
its regression component where each time series has its own pool of
contemporaneous external time series predictors, which is the first time that a
fully data-driven quantile feature selection technique applicable to time
series data to the author's best knowledge. Different from most machine
learning algorithms, the MQBSTS model has very few hyper-parameters to tune,
requires small datasets to train, converges fast, and is executable on ordinary
personal computers. Extensive examinations on simulated data and empirical data
confirmed that the MQBSTS model has superior performance in feature selection,
parameter estimation, and forecast."
"['stat.ML', 'stat.TH']",Universal time-series forecasting with mixture predictors,"This book is devoted to the problem of sequential probability forecasting,
that is, predicting the probabilities of the next outcome of a growing sequence
of observations given the past. This problem is considered in a very general
setting that unifies commonly used probabilistic and non-probabilistic
settings, trying to make as few as possible assumptions on the mechanism
generating the observations. A common form that arises in various formulations
of this problem is that of mixture predictors, which are formed as a
combination of a finite or infinite set of other predictors attempting to
combine their predictive powers. The main subject of this book are such mixture
predictors, and the main results demonstrate the universality of this method in
a very general probabilistic setting, but also show some of its limitations.
While the problems considered are motivated by practical applications,
involving, for example, financial, biological or behavioural data, this
motivation is left implicit and all the results exposed are theoretical.
  The book targets graduate students and researchers interested in the problem
of sequential prediction, and, more generally, in theoretical analysis of
problems in machine learning and non-parametric statistics, as well as
mathematical and philosophical foundations of these fields.
  The material in this volume is presented in a way that presumes familiarity
with basic concepts of probability and statistics, up to and including
probability distributions over spaces of infinite sequences. Familiarity with
the literature on learning or stochastic processes is not required."
"['stat.ML', 'stat.ME']",Computer Model Calibration with Time Series Data using Deep Learning and Quantile Regression,"Computer models play a key role in many scientific and engineering problems.
One major source of uncertainty in computer model experiment is input parameter
uncertainty. Computer model calibration is a formal statistical procedure to
infer input parameters by combining information from model runs and
observational data. The existing standard calibration framework suffers from
inferential issues when the model output and observational data are
high-dimensional dependent data such as large time series due to the difficulty
in building an emulator and the non-identifiability between effects from input
parameters and data-model discrepancy. To overcome these challenges we propose
a new calibration framework based on a deep neural network (DNN) with
long-short term memory layers that directly emulates the inverse relationship
between the model output and input parameters. Adopting the 'learning with
noise' idea we train our DNN model to filter out the effects from data model
discrepancy on input parameter inference. We also formulate a new way to
construct interval predictions for DNN using quantile regression to quantify
the uncertainty in input parameter estimates. Through a simulation study and
real data application with WRF-hydro model we show that our approach can yield
accurate point estimates and well calibrated interval estimates for input
parameters."
"['stat.ML', 'stat.ME', 'stat.TH']",Modeling of time series using random forests: theoretical developments,"In this paper we study asymptotic properties of random forests within the
framework of nonlinear time series modeling. While random forests have been
successfully applied in various fields, the theoretical justification has not
been considered for their use in a time series setting. Under mild conditions,
we prove a uniform concentration inequality for regression trees built on
nonlinear autoregressive processes and, subsequently, we use this result to
prove consistency for a large class of random forests. The results are
supported by various simulations."
"['stat.CO', 'stat.ME', 'stat.ML']",Novel semi-metrics for multivariate change point analysis and anomaly detection,"This paper proposes a new method for determining similarity and anomalies
between time series, most practically effective in large collections of (likely
related) time series, by measuring distances between structural breaks within
such a collection. We introduce a class of \emph{semi-metric} distance
measures, which we term \emph{MJ distances}. These semi-metrics provide an
advantage over existing options such as the Hausdorff and Wasserstein metrics.
We prove they have desirable properties, including better sensitivity to
outliers, while experiments on simulated data demonstrate that they uncover
similarity within collections of time series more effectively. Semi-metrics
carry a potential disadvantage: without the triangle inequality, they may not
satisfy a ""transitivity property of closeness."" We analyse this failure with
proof and introduce an computational method to investigate, in which we
demonstrate that our semi-metrics violate transitivity infrequently and mildly.
Finally, we apply our methods to cryptocurrency and measles data, introducing a
judicious application of eigenvalue analysis."
"['stat.ML', 'stat.ME']",Covariance-engaged Classification of Sets via Linear Programming,"Set classification aims to classify a set of observations as a whole, as
opposed to classifying individual observations separately. To formally
understand the unfamiliar concept of binary set classification, we first
investigate the optimal decision rule under the normal distribution, which
utilizes the empirical covariance of the set to be classified. We show that the
number of observations in the set plays a critical role in bounding the Bayes
risk. Under this framework, we further propose new methods of set
classification. For the case where only a few parameters of the model drive the
difference between two classes, we propose a computationally-efficient approach
to parameter estimation using linear programming, leading to the
Covariance-engaged LInear Programming Set (CLIPS) classifier. Its theoretical
properties are investigated for both independent case and various (short-range
and long-range dependent) time series structures among observations within each
set. The convergence rates of estimation errors and risk of the CLIPS
classifier are established to show that having multiple observations in a set
leads to faster convergence rates, compared to the standard classification
situation in which there is only one observation in the set. The applicable
domains in which the CLIPS performs better than competitors are highlighted in
a comprehensive simulation study. Finally, we illustrate the usefulness of the
proposed methods in classification of real image data in histopathology."
"['stat.ML', 'stat.CO']",Forecasting with time series imaging,"Feature-based time series representations have attracted substantial
attention in a wide range of time series analysis methods. Recently, the use of
time series features for forecast model averaging has been an emerging research
focus in the forecasting community. Nonetheless, most of the existing
approaches depend on the manual choice of an appropriate set of features.
Exploiting machine learning methods to extract features from time series
automatically becomes crucial in state-of-the-art time series analysis. In this
paper, we introduce an automated approach to extract time series features based
on time series imaging. We first transform time series into recurrence plots,
from which local features can be extracted using computer vision algorithms.
The extracted features are used for forecast model averaging. Our experiments
show that forecasting based on automatically extracted features, with less
human intervention and a more comprehensive view of the raw time series data,
yields highly comparable performances with the best methods in the largest
forecasting competition dataset (M4) and outperforms the top methods in the
Tourism forecasting competition dataset."
"['stat.ME', 'stat.ML']",Variable-lag Granger Causality and Transfer Entropy for Time Series Analysis,"Granger causality is a fundamental technique for causal inference in time
series data, commonly used in the social and biological sciences. Typical
operationalizations of Granger causality make a strong assumption that every
time point of the effect time series is influenced by a combination of other
time series with a fixed time delay. The assumption of fixed time delay also
exists in Transfer Entropy, which is considered to be a non-linear version of
Granger causality. However, the assumption of the fixed time delay does not
hold in many applications, such as collective behavior, financial markets, and
many natural phenomena. To address this issue, we develop Variable-lag Granger
causality and Variable-lag Transfer Entropy, generalizations of both Granger
causality and Transfer Entropy that relax the assumption of the fixed time
delay and allow causes to influence effects with arbitrary time delays. In
addition, we propose methods for inferring both variable-lag Granger causality
and Transfer Entropy relations. In our approaches, we utilize an optimal
warping path of Dynamic Time Warping (DTW) to infer variable-lag causal
relations. We demonstrate our approaches on an application for studying
coordinated collective behavior and other real-world casual-inference datasets
and show that our proposed approaches perform better than several existing
methods in both simulated and real-world datasets. Our approaches can be
applied in any domain of time series analysis. The software of this work is
available in the R-CRAN package: VLTimeCausality."
"['stat.ML', 'stat.ME']",An Evaluation of Change Point Detection Algorithms,"Change point detection is an important part of time series analysis, as the
presence of a change point indicates an abrupt and significant change in the
data generating process. While many algorithms for change point detection
exist, little attention has been paid to evaluating their performance on
real-world time series. Algorithms are typically evaluated on simulated data
and a small number of commonly-used series with unreliable ground truth.
Clearly this does not provide sufficient insight into the comparative
performance of these algorithms. Therefore, instead of developing yet another
change point detection method, we consider it vastly more important to properly
evaluate existing algorithms on real-world data. To achieve this, we present
the first data set specifically designed for the evaluation of change point
detection algorithms, consisting of 37 time series from various domains. Each
time series was annotated by five expert human annotators to provide ground
truth on the presence and location of change points. We analyze the consistency
of the human annotators, and describe evaluation metrics that can be used to
measure algorithm performance in the presence of multiple ground truth
annotations. Subsequently, we present a benchmark study where 14 existing
algorithms are evaluated on each of the time series in the data set. This study
shows that binary segmentation (Scott and Knott, 1974) and Bayesian online
change point detection (Adams and MacKay, 2007) are among the best performing
methods. Our aim is that this data set will serve as a proving ground in the
development of novel change point detection algorithms."
"['stat.ML', 'stat.ME']",Independence Testing for Multivariate Time Series,"Complex data structures such as time series are increasingly present in
modern data science problems. A fundamental question is whether two such
time-series are statistically dependent. Many current approaches make
parametric assumptions on the random processes, only detect linear association,
require multiple tests, or forfeit power in high-dimensional, nonlinear
settings. Estimating the distribution of any test statistic under the null is
non-trivial, as the permutation test is invalid. This work juxtaposes distance
correlation (Dcorr) and multiscale graph correlation (MGC) from independence
testing literature and block permutation from time series analysis to address
these challenges. The proposed nonparametric procedure is valid and consistent,
building upon prior work by characterizing the geometry of the relationship,
estimating the time lag at which dependence is maximized, avoiding the need for
multiple testing, and exhibiting superior power in high-dimensional, low sample
size, nonlinear settings. Neural connectivity is analyzed via fMRI data,
revealing linear dependence of signals within the visual network and default
mode network, and nonlinear relationships in other networks. This work uncovers
a first-resort data analysis tool with open-source code available, directly
impacting a wide range of scientific disciplines."
"['stat.ML', 'stat.CO', 'stat.TH']",Statistically Guided Divide-and-Conquer for Sparse Factorization of Large Matrix,"The sparse factorization of a large matrix is fundamental in modern
statistical learning. In particular, the sparse singular value decomposition
and its variants have been utilized in multivariate regression, factor
analysis, biclustering, vector time series modeling, among others. The appeal
of this factorization is owing to its power in discovering a
highly-interpretable latent association network, either between samples and
variables or between responses and predictors. However, many existing methods
are either ad hoc without a general performance guarantee, or are
computationally intensive, rendering them unsuitable for large-scale studies.
We formulate the statistical problem as a sparse factor regression and tackle
it with a divide-and-conquer approach. In the first stage of division, we
consider both sequential and parallel approaches for simplifying the task into
a set of co-sparse unit-rank estimation (CURE) problems, and establish the
statistical underpinnings of these commonly-adopted and yet poorly understood
deflation methods. In the second stage of division, we innovate a contended
stagewise learning technique, consisting of a sequence of simple incremental
updates, to efficiently trace out the whole solution paths of CURE. Our
algorithm has a much lower computational complexity than alternating convex
search, and the choice of the step size enables a flexible and principled
tradeoff between statistical accuracy and computational efficiency. Our work is
among the first to enable stagewise learning for non-convex problems, and the
idea can be applicable in many multi-convex problems. Extensive simulation
studies and an application in genetics demonstrate the effectiveness and
scalability of our approach."
"['stat.ML', 'stat.ME']","Context-dependent self-exciting point processes: models, methods, and risk bounds in high dimensions","High-dimensional autoregressive point processes model how current events
trigger or inhibit future events, such as activity by one member of a social
network can affect the future activity of his or her neighbors. While past work
has focused on estimating the underlying network structure based solely on the
times at which events occur on each node of the network, this paper examines
the more nuanced problem of estimating context-dependent networks that reflect
how features associated with an event (such as the content of a social media
post) modulate the strength of influences among nodes. Specifically, we
leverage ideas from compositional time series and regularization methods in
machine learning to conduct network estimation for high-dimensional marked
point processes. Two models and corresponding estimators are considered in
detail: an autoregressive multinomial model suited to categorical marks and a
logistic-normal model suited to marks with mixed membership in different
categories. Importantly, the logistic-normal model leads to a convex negative
log-likelihood objective and captures dependence across categories. We provide
theoretical guarantees for both estimators, which we validate by simulations
and a synthetic data-generating model. We further validate our methods through
two real data examples and demonstrate the advantages and disadvantages of both
approaches."
"['stat.ML', 'stat.ME']",Generalised learning of time-series: Ornstein-Uhlenbeck processes,"In machine learning, statistics, econometrics and statistical physics
cross-validation (CV) is used as a standard approach in quantifying the
generalization performance of a statistical model. In practice, direct usage of
CV is avoided for time-series due to several issues. A direct application of CV
in time-series leads to the loss of serial correlations, a requirement of
preserving any non-stationarity and the prediction of the past data using
future data. In this work, we propose a meta-algorithm called reconstructive
cross-validation (rCV ) that avoids all these issues. At first, k folds are
formed with non-overlapping randomly selected subsets of the original
time-series. Then, we generate k new partial time-series by removing data
points from a given fold: every new partial time-series have missing points at
random from a different entire fold. A suitable imputation or a smoothing
technique is used to reconstruct k time-series. We call these reconstructions
secondary models. Thereafter, we build the primary k time-series models using
new time-series coming from the secondary models. The performance of the
primary models is evaluated simultaneously by computing the deviations from the
originally removed data points and out-of-sample (OSS) data. These amounts to
reconstruction and prediction errors. If the secondary models use a technique
that retains the data points exactly, such as Gaussian process regression,
there will be no errors present on the data points that are not removed. By
this procedure serial correlations are retained, any non-stationarity is
preserved within models and there will be no prediction of past data using the
future data points. The cross-validation in time-series model can be practised
with rCV. Moreover, we can build time-series learning curves by repeating rCV
procedure with different k values."
"['stat.ML', 'stat.ME', 'stat.TH']",The Area of the Convex Hull of Sampled Curves: a Robust Functional Statistical Depth Measure,"With the ubiquity of sensors in the IoT era, statistical observations are
becoming increasingly available in the form of massive (multivariate)
time-series. Formulated as unsupervised anomaly detection tasks, an abundance
of applications like aviation safety management, the health monitoring of
complex infrastructures or fraud detection can now rely on such functional
data, acquired and stored with an ever finer granularity. The concept of
statistical depth, which reflects centrality of an arbitrary observation w.r.t.
a statistical population may play a crucial role in this regard, anomalies
corresponding to observations with 'small' depth. Supported by sound
theoretical and computational developments in the recent decades, it has proven
to be extremely useful, in particular in functional spaces. However, most
approaches documented in the literature consist in evaluating independently the
centrality of each point forming the time series and consequently exhibit a
certain insensitivity to possible shape changes. In this paper, we propose a
novel notion of functional depth based on the area of the convex hull of
sampled curves, capturing gradual departures from centrality, even beyond the
envelope of the data, in a natural fashion. We discuss practical relevance of
commonly imposed axioms on functional depths and investigate which of them are
satisfied by the notion of depth we promote here. Estimation and computational
issues are also addressed and various numerical experiments provide empirical
evidence of the relevance of the approach proposed."
"['stat.ML', 'stat.TH']",Unsupervised non-parametric change point detection in quasi-periodic signals,"We propose a new unsupervised and non-parametric method to detect change
points in intricate quasi-periodic signals. The detection relies on optimal
transport theory combined with topological analysis and the bootstrap
procedure. The algorithm is designed to detect changes in virtually any
harmonic or a partially harmonic signal and is verified on three different
sources of physiological data streams. We successfully find abnormal or
irregular cardiac cycles in the waveforms for the six of the most frequent
types of clinical arrhythmias using a single algorithm. The validation and the
efficiency of the method are shown both on synthetic and on real time series.
Our unsupervised approach reaches the level of performance of the supervised
state-of-the-art techniques. We provide conceptual justification for the
efficiency of the method and prove the convergence of the bootstrap procedure
theoretically."
"['stat.ME', 'stat.ML']",Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality,"Granger causality is a widely-used criterion for analyzing interactions in
large-scale networks. As most physical interactions are inherently nonlinear,
we consider the problem of inferring the existence of pairwise Granger
causality between nonlinearly interacting stochastic processes from their time
series measurements. Our proposed approach relies on modeling the embedded
nonlinearities in the measurements using a component-wise time series
prediction model based on Statistical Recurrent Units (SRUs). We make a case
that the network topology of Granger causal relations is directly inferrable
from a structured sparse estimate of the internal parameters of the SRU
networks trained to predict the processes$'$ time series measurements. We
propose a variant of SRU, called economy-SRU, which, by design has considerably
fewer trainable parameters, and therefore less prone to overfitting. The
economy-SRU computes a low-dimensional sketch of its high-dimensional hidden
state in the form of random projections to generate the feedback for its
recurrent processing. Additionally, the internal weight parameters of the
economy-SRU are strategically regularized in a group-wise manner to facilitate
the proposed network in extracting meaningful predictive features that are
highly time-localized to mimic real-world causal events. Extensive experiments
are carried out to demonstrate that the proposed economy-SRU based time series
prediction model outperforms the MLP, LSTM and attention-gated CNN-based time
series models considered previously for inferring Granger causality."
"['stat.ML', 'stat.CO']",GRATIS: GeneRAting TIme Series with diverse and controllable characteristics,"The explosion of time series data in recent years has brought a flourish of
new time series analysis methods, for forecasting, clustering, classification
and other tasks. The evaluation of these new methods requires either collecting
or simulating a diverse set of time series benchmarking data to enable reliable
comparisons against alternative approaches. We propose GeneRAting TIme Series
with diverse and controllable characteristics, named GRATIS, with the use of
mixture autoregressive (MAR) models. We simulate sets of time series using MAR
models and investigate the diversity and coverage of the generated time series
in a time series feature space. By tuning the parameters of the MAR models,
GRATIS is also able to efficiently generate new time series with controllable
features. In general, as a costless surrogate to the traditional data
collection approach, GRATIS can be used as an evaluation tool for tasks such as
time series forecasting and classification. We illustrate the usefulness of our
time series generation process through a time series forecasting application."
"['stat.ME', 'stat.ML']",Variable-lag Granger Causality for Time Series Analysis,"Granger causality is a fundamental technique for causal inference in time
series data, commonly used in the social and biological sciences. Typical
operationalizations of Granger causality make a strong assumption that every
time point of the effect time series is influenced by a combination of other
time series with a fixed time delay. However, the assumption of the fixed time
delay does not hold in many applications, such as collective behavior,
financial markets, and many natural phenomena. To address this issue, we
develop variable-lag Granger causality, a generalization of Granger causality
that relaxes the assumption of the fixed time delay and allows causes to
influence effects with arbitrary time delays. In addition, we propose a method
for inferring variable-lag Granger causality relations. We demonstrate our
approach on an application for studying coordinated collective behavior and
show that it performs better than several existing methods in both simulated
and real-world datasets. Our approach can be applied in any domain of time
series analysis."
"['stat.CO', 'stat.ML']",Differentiable Algorithm for Marginalising Changepoints,"We present an algorithm for marginalising changepoints in time-series models
that assume a fixed number of unknown changepoints. Our algorithm is
differentiable with respect to its inputs, which are the values of latent
random variables other than changepoints. Also, it runs in time O(mn) where n
is the number of time steps and m the number of changepoints, an improvement
over a naive marginalisation method with O(n^m) time complexity. We derive the
algorithm by identifying quantities related to this marginalisation problem,
showing that these quantities satisfy recursive relationships, and transforming
the relationships to an algorithm via dynamic programming. Since our algorithm
is differentiable, it can be applied to convert a model non-differentiable due
to changepoints to a differentiable one, so that the resulting models can be
analysed using gradient-based inference or learning techniques. We empirically
show the effectiveness of our algorithm in this application by tackling the
posterior inference problem on synthetic and real-world data."
"['stat.CO', 'stat.ML']",Variational Bayesian inference of hidden stochastic processes with unknown parameters,"Estimating hidden processes from non-linear noisy observations is
particularly difficult when the parameters of these processes are not known.
This paper adopts a machine learning approach to devise variational Bayesian
inference for such scenarios. In particular, a random process generated by the
autoregressive moving average (ARMA) linear model is inferred from
non-linearity noise observations. The posterior distribution of hidden states
are approximated by a set of weighted particles generated by the sequential
Monte carlo (SMC) algorithm involving sampling with importance sampling
resampling (SISR). Numerical efficiency and estimation accuracy of the proposed
inference method are evaluated by computer simulations. Furthermore, the
proposed inference method is demonstrated on a practical problem of estimating
the missing values in the gene expression time series assuming vector
autoregressive (VAR) data model."
"['stat.ML', 'stat.CO']",Stochastic Gradient MCMC for Nonlinear State Space Models,"State space models (SSMs) provide a flexible framework for modeling complex
time series via a latent stochastic process. Inference for nonlinear,
non-Gaussian SSMs is often tackled with particle methods that do not scale well
to long time series. The challenge is two-fold: not only do computations scale
linearly with time, as in the linear case, but particle filters additionally
suffer from increasing particle degeneracy with longer series. Stochastic
gradient MCMC methods have been developed to scale inference for hidden Markov
models (HMMs) and linear SSMs using buffered stochastic gradient estimates to
account for temporal dependencies. We extend these stochastic gradient
estimators to nonlinear SSMs using particle methods. We present error bounds
that account for both buffering error and particle error in the case of
nonlinear SSMs that are log-concave in the latent process. We evaluate our
proposed particle buffered stochastic gradient using SGMCMC for inference on
both long sequential synthetic and minute-resolution financial returns data,
demonstrating the importance of this class of methods."
"['stat.ML', 'stat.CO']",Stochastic Gradient MCMC for State Space Models,"State space models (SSMs) are a flexible approach to modeling complex time
series. However, inference in SSMs is often computationally prohibitive for
long time series. Stochastic gradient MCMC (SGMCMC) is a popular method for
scalable Bayesian inference for large independent data. Unfortunately when
applied to dependent data, such as in SSMs, SGMCMC's stochastic gradient
estimates are biased as they break crucial temporal dependencies. To alleviate
this, we propose stochastic gradient estimators that control this bias by
performing additional computation in a `buffer' to reduce breaking
dependencies. Furthermore, we derive error bounds for this bias and show a
geometric decay under mild conditions. Using these estimators, we develop novel
SGMCMC samplers for discrete, continuous and mixed-type SSMs with analytic
message passing. Our experiments on real and synthetic data demonstrate the
effectiveness of our SGMCMC algorithms compared to batch MCMC, allowing us to
scale inference to long time series with millions of time points."
"['stat.ML', 'stat.TH']",Clustering piecewise stationary processes,"The problem of time-series clustering is considered in the case where each
data-point is a sample generated by a piecewise stationary ergodic process.
Stationary processes are perhaps the most general class of processes considered
in non-parametric statistics and allow for arbitrary long-range dependence
between variables. Piecewise stationary processes studied here for the first
time in the context of clustering, relax the last remaining assumption in this
model: stationarity. A natural formulation is proposed for this problem and a
notion of consistency is introduced which requires the samples to be placed in
the same cluster if and only if the piecewise stationary distributions that
generate them have the same set of stationary distributions. Simple,
computationally efficient algorithms are proposed and are shown to be
consistent without any additional assumptions beyond piecewise stationarity."
"['stat.ML', 'stat.CO']",Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation,"We present a novel family of deep neural architectures, named partially
exchangeable networks (PENs) that leverage probabilistic symmetries. By design,
PENs are invariant to block-switch transformations, which characterize the
partial exchangeability properties of conditionally Markovian processes.
Moreover, we show that any block-switch invariant function has a PEN-like
representation. The DeepSets architecture is a special case of PEN and we can
therefore also target fully exchangeable data. We employ PENs to learn summary
statistics in approximate Bayesian computation (ABC). When comparing PENs to
previous deep learning methods for learning summary statistics, our results are
highly competitive, both considering time series and static models. Indeed,
PENs provide more reliable posterior samples even when using less training
data."
"['stat.ML', 'stat.ME']",Automatic Model Building in GEFCom 2017 Qualifying Match,"The Tangent Works team participated in GEFCom 2017 to test its automatic
model building strategy for time series known as Tangent Information Modeller
(TIM). Model building using TIM combined with historical temperature shuffling
resulted in winning the competition. This strategy involved one remaining
degree of freedom, a decision on using a trend variable. This paper describes
our modelling efforts in the competition, and furthermore outlines a fully
automated scenario where the decision on using the trend variable is handled by
TIM. The results show that such a setup would also win the competition."
"['stat.ML', 'stat.ME']",Optimal Combination Forecasts on Retail Multi-Dimensional Sales Data,"Time series data in the retail world are particularly rich in terms of
dimensionality, and these dimensions can be aggregated in groups or
hierarchies. Valuable information is nested in these complex structures, which
helps to predict the aggregated time series data. From a portfolio of brands
under HUUB's monitoring, we selected two to explore their sales behaviour,
leveraging the grouping properties of their product structure. Using
statistical models, namely SARIMA, to forecast each level of the hierarchy, an
optimal combination approach was used to generate more consistent forecasts in
the higher levels. Our results show that the proposed methods can indeed
capture nested information in the more granular series, helping to improve the
forecast accuracy of the aggregated series. The Weighted Least Squares (WLS)
method surpasses all other methods proposed in the study, including the Minimum
Trace (MinT) reconciliation."
"['stat.ML', 'stat.CO']",Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach,"We propose a general statistical framework for clustering multiple time
series that exhibit nonlinear dynamics into an a-priori-unknown number of
sub-groups. Our motivation comes from neuroscience, where an important problem
is to identify, within a large assembly of neurons, subsets that respond
similarly to a stimulus or contingency. Upon modeling the multiple time series
as the output of a Dirichlet process mixture of nonlinear state-space models,
we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that
alternates between sampling cluster assignments and sampling parameter values
that form the basis of the clustering. The Metropolis step employs recent
innovations in particle-based methods. We apply the framework to clustering
time series acquired from the prefrontal cortex of mice in an experiment
designed to characterize the neural underpinnings of fear."
"['stat.ML', 'stat.TH']",Approximate Newton-based statistical inference using only stochastic gradients,"We present a novel statistical inference framework for convex empirical risk
minimization, using approximate stochastic Newton steps. The proposed algorithm
is based on the notion of finite differences and allows the approximation of a
Hessian-vector product from first-order information. In theory, our method
efficiently computes the statistical error covariance in $M$-estimation, both
for unregularized convex learning problems and high-dimensional LASSO
regression, without using exact second order information, or resampling the
entire data set. We also present a stochastic gradient sampling scheme for
statistical inference in non-i.i.d. time series analysis, where we sample
contiguous blocks of indices. In practice, we demonstrate the effectiveness of
our framework on large-scale machine learning problems, that go even beyond
convexity: as a highlight, our work can be used to detect certain adversarial
attacks on neural networks."
"['stat.ML', 'stat.CO', 'stat.ME']",Deep Poisson gamma dynamical systems,"We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially
observed multivariate count data, improving previously proposed models by not
only mining deep hierarchical latent structure from the data, but also
capturing both first-order and long-range temporal dependencies. Using
sophisticated but simple-to-implement data augmentation techniques, we derived
closed-form Gibbs sampling update equations by first backward and upward
propagating auxiliary latent counts, and then forward and downward sampling
latent variables. Moreover, we develop stochastic gradient MCMC inference that
is scalable to very long multivariate count time series. Experiments on both
synthetic and a variety of real-world data demonstrate that the proposed model
not only has excellent predictive performance, but also provides highly
interpretable multilayer latent structure to represent hierarchical and
temporal information propagation."
"['stat.ML', 'stat.ME']",Robust multivariate and functional archetypal analysis with application to financial time series analysis,"Archetypal analysis approximates data by means of mixtures of actual extreme
cases (archetypoids) or archetypes, which are a convex combination of cases in
the data set. Archetypes lie on the boundary of the convex hull. This makes the
analysis very sensitive to outliers. A robust methodology by means of
M-estimators for classical multivariate and functional data is proposed. This
unsupervised methodology allows complex data to be understood even by
non-experts. The performance of the new procedure is assessed in a simulation
study, where a comparison with a previous methodology for the multivariate case
is also carried out, and our proposal obtains favorable results. Finally,
robust bivariate functional archetypoid analysis is applied to a set of
companies in the S\&P 500 described by two time series of stock quotes. A new
graphic representation is also proposed to visualize the results. The analysis
shows how the information can be easily interpreted and how even non-experts
can gain a qualitative understanding of the data."
"['stat.ML', 'stat.CO', 'stat.ME']",Dynamic Likelihood-free Inference via Ratio Estimation (DIRE),"Parametric statistical models that are implicitly defined in terms of a
stochastic data generating process are used in a wide range of scientific
disciplines because they enable accurate modeling. However, learning the
parameters from observed data is generally very difficult because their
likelihood function is typically intractable. Likelihood-free Bayesian
inference methods have been proposed which include the frameworks of
approximate Bayesian computation (ABC), synthetic likelihood, and its recent
generalization that performs likelihood-free inference by ratio estimation
(LFIRE). A major difficulty in all these methods is choosing summary statistics
that reduce the dimensionality of the data to facilitate inference. While
several methods for choosing summary statistics have been proposed for ABC, the
literature for synthetic likelihood and LFIRE is very thin to date. We here
address this gap in the literature, focusing on the important special case of
time-series models. We show that convolutional neural networks trained to
predict the input parameters from the data provide suitable summary statistics
for LFIRE. On a wide range of time-series models, a single neural network
architecture produced equally or more accurate posteriors than alternative
methods."
"['stat.ML', 'stat.CO']",Approximate Collapsed Gibbs Clustering with Expectation Propagation,"We develop a framework for approximating collapsed Gibbs sampling in
generative latent variable cluster models. Collapsed Gibbs is a popular MCMC
method, which integrates out variables in the posterior to improve mixing.
Unfortunately for many complex models, integrating out these variables is
either analytically or computationally intractable. We efficiently approximate
the necessary collapsed Gibbs integrals by borrowing ideas from expectation
propagation. We present two case studies where exact collapsed Gibbs sampling
is intractable: mixtures of Student-t's and time series clustering. Our
experiments on real and synthetic data show that our approximate sampler
enables a runtime-accuracy tradeoff in sampling these types of models,
providing results with competitive accuracy much more rapidly than the naive
Gibbs samplers one would otherwise rely on in these scenarios."
"['stat.ML', 'stat.ME']",Detecting Nonlinear Causality in Multivariate Time Series with Sparse Additive Models,"We propose a nonparametric method for detecting nonlinear causal relationship
within a set of multidimensional discrete time series, by using sparse additive
models (SpAMs). We show that, when the input to the SpAM is a $\beta$-mixing
time series, the model can be fitted by first approximating each unknown
function with a linear combination of a set of B-spline bases, and then solving
a group-lasso-type optimization problem with nonconvex regularization.
Theoretically, we characterize the oracle statistical properties of the
proposed sparse estimator in function estimation and model selection.
Numerically, we propose an efficient pathwise iterative shrinkage thresholding
algorithm (PISTA), which tames the nonconvexity and guarantees linear
convergence towards the desired sparse estimator with high probability."
"['stat.ME', 'stat.ML']",Causal Inference via Kernel Deviance Measures,"Discovering the causal structure among a set of variables is a fundamental
problem in many areas of science. In this paper, we propose Kernel Conditional
Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery
method based on purely observational data. From a novel interpretation of the
notion of asymmetry between cause and effect, we derive a corresponding
asymmetry measure using the framework of reproducing kernel Hilbert spaces.
Based on this, we propose three decision rules for causal discovery. We
demonstrate the wide applicability of our method across a range of diverse
synthetic datasets. Furthermore, we test our method on real-world time series
data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we
outperform existing state-of-the-art methods."
"['stat.ML', 'stat.ME']",Model-Based Clustering and Classification of Functional Data,"The problem of complex data analysis is a central topic of modern statistical
science and learning systems and is becoming of broader interest with the
increasing prevalence of high-dimensional data. The challenge is to develop
statistical models and autonomous algorithms that are able to acquire knowledge
from raw data for exploratory analysis, which can be achieved through
clustering techniques or to make predictions of future data via classification
(i.e., discriminant analysis) techniques. Latent data models, including mixture
model-based approaches are one of the most popular and successful approaches in
both the unsupervised context (i.e., clustering) and the supervised one (i.e,
classification or discrimination). Although traditionally tools of multivariate
analysis, they are growing in popularity when considered in the framework of
functional data analysis (FDA). FDA is the data analysis paradigm in which the
individual data units are functions (e.g., curves, surfaces), rather than
simple vectors. In many areas of application, the analyzed data are indeed
often available in the form of discretized values of functions or curves (e.g.,
time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data).
This functional aspect of the data adds additional difficulties compared to the
case of a classical multivariate (non-functional) data analysis. We review and
present approaches for model-based clustering and classification of functional
data. We derive well-established statistical models along with efficient
algorithmic tools to address problems regarding the clustering and the
classification of these high-dimensional data, including their heterogeneity,
missing information, and dynamical hidden structure. The presented models and
algorithms are illustrated on real-world functional data analysis problems from
several application area."
"['stat.ML', 'stat.ME']",Nonparametric Bayesian Sparse Graph Linear Dynamical Systems,"A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is
proposed to model sequentially observed multivariate data. SGLDS uses the
Bernoulli-Poisson link together with a gamma process to generate an infinite
dimensional sparse random graph to model state transitions. Depending on the
sparsity pattern of the corresponding row and column of the graph affinity
matrix, a latent state of SGLDS can be categorized as either a non-dynamic
state or a dynamic one. A normal-gamma construction is used to shrink the
energy captured by the non-dynamic states, while the dynamic states can be
further categorized into live, absorbing, or noise-injection states, which
capture different types of dynamical components of the underlying time series.
The state-of-the-art performance of SGLDS is demonstrated with experiments on
both synthetic and real data."
"['stat.ML', 'stat.TH']",Non-parametric Sparse Additive Auto-regressive Network Models,"Consider a multi-variate time series $(X_t)_{t=0}^{T}$ where $X_t \in
\mathbb{R}^d$ which may represent spike train responses for multiple neurons in
a brain, crime event data across multiple regions, and many others. An
important challenge associated with these time series models is to estimate an
influence network between the $d$ variables, especially when the number of
variables $d$ is large meaning we are in the high-dimensional setting. Prior
work has focused on parametric vector auto-regressive models. However,
parametric approaches are somewhat restrictive in practice. In this paper, we
use the non-parametric sparse additive model (SpAM) framework to address this
challenge. Using a combination of $\beta$ and $\phi$-mixing properties of
Markov chains and empirical process techniques for reproducing kernel Hilbert
spaces (RKHSs), we provide upper bounds on mean-squared error in terms of the
sparsity $s$, logarithm of the dimension $\log d$, number of time points $T$,
and the smoothness of the RKHSs. Our rates are sharp up to logarithm factors in
many cases. We also provide numerical experiments that support our theoretical
results and display potential advantages of using our non-parametric SpAM
framework for a Chicago crime dataset."
"['stat.ML', 'stat.ME']",Bayesian Belief Updating of Spatiotemporal Seizure Dynamics,"Epileptic seizure activity shows complicated dynamics in both space and time.
To understand the evolution and propagation of seizures spatially extended sets
of data need to be analysed. We have previously described an efficient
filtering scheme using variational Laplace that can be used in the Dynamic
Causal Modelling (DCM) framework [Friston, 2003] to estimate the temporal
dynamics of seizures recorded using either invasive or non-invasive electrical
recordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial
differential equation -- in contrast to the ordinary differential equation used
in our previous work on temporal estimation of seizure dynamics [Cooray, 2016].
We provide the requisite theoretical background for the method and test the
ensuing scheme on simulated seizure activity data and empirical invasive ECoG
data. The method provides a framework to assimilate the spatial and temporal
dynamics of seizure activity, an aspect of great physiological and clinical
importance."
"['stat.ML', 'stat.CO']",Robust Maximum Likelihood Estimation of Sparse Vector Error Correction Model,"In econometrics and finance, the vector error correction model (VECM) is an
important time series model for cointegration analysis, which is used to
estimate the long-run equilibrium variable relationships. The traditional
analysis and estimation methodologies assume the underlying Gaussian
distribution but, in practice, heavy-tailed data and outliers can lead to the
inapplicability of these methods. In this paper, we propose a robust model
estimation method based on the Cauchy distribution to tackle this issue. In
addition, sparse cointegration relations are considered to realize feature
selection and dimension reduction. An efficient algorithm based on the
majorization-minimization (MM) method is applied to solve the proposed
nonconvex problem. The performance of this algorithm is shown through numerical
simulations."
['stat.ME'],m-TSNE: A Framework for Visualizing High-Dimensional Multivariate Time Series,"Multivariate time series (MTS) have become increasingly common in healthcare
domains where human vital signs and laboratory results are collected for
predictive diagnosis. Recently, there have been increasing efforts to visualize
healthcare MTS data based on star charts or parallel coordinates. However, such
techniques might not be ideal for visualizing a large MTS dataset, since it is
difficult to obtain insights or interpretations due to the inherent high
dimensionality of MTS. In this paper, we propose 'm-TSNE': a simple and novel
framework to visualize high-dimensional MTS data by projecting them into a
low-dimensional (2-D or 3-D) space while capturing the underlying data
properties. Our framework is easy to use and provides interpretable insights
for healthcare professionals to understand MTS data. We evaluate our
visualization framework on two real-world datasets and demonstrate that the
results of our m-TSNE show patterns that are easy to understand while the other
methods' visualization may have limitations in interpretability."
"['stat.ML', 'stat.ME']",An Improved Multi-Output Gaussian Process RNN with Real-Time Validation for Early Sepsis Detection,"Sepsis is a poorly understood and potentially life-threatening complication
that can occur as a result of infection. Early detection and treatment improves
patient outcomes, and as such it poses an important challenge in medicine. In
this work, we develop a flexible classifier that leverages streaming lab
results, vitals, and medications to predict sepsis before it occurs. We model
patient clinical time series with multi-output Gaussian processes, maintaining
uncertainty about the physiological state of a patient while also imputing
missing values. The mean function takes into account the effects of medications
administered on the trajectories of the physiological variables. Latent
function values from the Gaussian process are then fed into a deep recurrent
neural network to classify patient encounters as septic or not, and the overall
model is trained end-to-end using back-propagation. We train and validate our
model on a large dataset of 18 months of heterogeneous inpatient stays from the
Duke University Health System, and develop a new ""real-time"" validation scheme
for simulating the performance of our model as it will actually be used. Our
proposed method substantially outperforms clinical baselines, and improves on a
previous related model for detecting sepsis. Our model's predictions will be
displayed in a real-time analytics dashboard to be used by a sepsis rapid
response team to help detect and improve treatment of sepsis."
"['stat.ML', 'stat.TH']",Hypotheses testing on infinite random graphs,"Drawing on some recent results that provide the formalism necessary to
definite stationarity for infinite random graphs, this paper initiates the
study of statistical and learning questions pertaining to these objects.
Specifically, a criterion for the existence of a consistent test for complex
hypotheses is presented, generalizing the corresponding results on time series.
As an application, it is shown how one can test that a tree has the Markov
property, or, more generally, to estimate its memory."
"['stat.ML', 'stat.TH']",Copy the dynamics using a learning machine,"Is it possible to generally construct a dynamical system to simulate a black
system without recovering the equations of motion of the latter? Here we show
that this goal can be approached by a learning machine. Trained by a set of
input-output responses or a segment of time series of a black system, a
learning machine can be served as a copy system to mimic the dynamics of
various black systems. It can not only behave as the black system at the
parameter set that the training data are made, but also recur the evolution
history of the black system. As a result, the learning machine provides an
effective way for prediction, and enables one to probe the global dynamics of a
black system. These findings have significance for practical systems whose
equations of motion cannot be approached accurately. Examples of copying the
dynamics of an artificial neural network, the Lorenz system, and a variable
star are given. Our idea paves a possible way towards copy a living brain."
"['stat.ML', 'stat.ME']",Causal Consistency of Structural Equation Models,"Complex systems can be modelled at various levels of detail. Ideally, causal
models of the same system should be consistent with one another in the sense
that they agree in their predictions of the effects of interventions. We
formalise this notion of consistency in the case of Structural Equation Models
(SEMs) by introducing exact transformations between SEMs. This provides a
general language to consider, for instance, the different levels of description
in the following three scenarios: (a) models with large numbers of variables
versus models in which the `irrelevant' or unobservable variables have been
marginalised out; (b) micro-level models versus macro-level models in which the
macro-variables are aggregate features of the micro-variables; (c) dynamical
time series models versus models of their stationary behaviour. Our analysis
stresses the importance of well specified interventions in the causal modelling
process and sheds light on the interpretation of cyclic SEMs."
"['stat.ML', 'stat.TH']",Inference of High-dimensional Autoregressive Generalized Linear Models,"Vector autoregressive models characterize a variety of time series in which
linear combinations of current and past observations can be used to accurately
predict future observations. For instance, each element of an observation
vector could correspond to a different node in a network, and the parameters of
an autoregressive model would correspond to the impact of the network structure
on the time series evolution. Often these models are used successfully in
practice to learn the structure of social, epidemiological, financial, or
biological neural networks. However, little is known about statistical
guarantees on estimates of such models in non-Gaussian settings. This paper
addresses the inference of the autoregressive parameters and associated network
structure within a generalized linear model framework that includes Poisson and
Bernoulli autoregressive processes. At the heart of this analysis is a
sparsity-regularized maximum likelihood estimator. While
sparsity-regularization is well-studied in the statistics and machine learning
communities, those analysis methods cannot be applied to autoregressive
generalized linear models because of the correlations and potential
heteroscedasticity inherent in the observations. Sample complexity bounds are
derived using a combination of martingale concentration inequalities and modern
empirical process techniques for dependent random variables. These bounds,
which are supported by several simulation studies, characterize the impact of
various network parameters on estimator performance."
"['stat.ML', 'stat.ME']",Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier,"We present a scalable end-to-end classifier that uses streaming physiological
and medication data to accurately predict the onset of sepsis, a
life-threatening complication from infections that has high mortality and
morbidity. Our proposed framework models the multivariate trajectories of
continuous-valued physiological time series using multitask Gaussian processes,
seamlessly accounting for the high uncertainty, frequent missingness, and
irregular sampling rates typically associated with real clinical data. The
Gaussian process is directly connected to a black-box classifier that predicts
whether a patient will become septic, chosen in our case to be a recurrent
neural network to account for the extreme variability in the length of patient
encounters. We show how to scale the computations associated with the Gaussian
process in a manner so that the entire system can be discriminatively trained
end-to-end using backpropagation. In a large cohort of heterogeneous inpatient
encounters at our university health system we find that it outperforms several
baselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under
the Receiver Operating Characteristic and Precision Recall curves as compared
to the NEWS score currently used by our hospital."
"['stat.ML', 'stat.CO', 'stat.ME']",Conformal k-NN Anomaly Detector for Univariate Data Streams,"Anomalies in time-series data give essential and often actionable information
in many applications. In this paper we consider a model-free anomaly detection
method for univariate time-series which adapts to non-stationarity in the data
stream and provides probabilistic abnormality scores based on the conformal
prediction paradigm. Despite its simplicity the method performs on par with
complex prediction-based models on the Numenta Anomaly Detection benchmark and
the Yahoo! S5 dataset."
['stat.OT'],Automated Diagnosis of Epilepsy Employing Multifractal Detrended Fluctuation Analysis Based Features,"This contribution reports an application of MultiFractal Detrended
Fluctuation Analysis, MFDFA based novel feature extraction technique for
automated detection of epilepsy. In fractal geometry, Multifractal Detrended
Fluctuation Analysis MFDFA is a popular technique to examine the
self-similarity of a nonlinear, chaotic and noisy time series. In the present
research work, EEG signals representing healthy, interictal (seizure free) and
ictal activities (seizure) are acquired from an existing available database.
The acquired EEG signals of different states are at first analyzed using MFDFA.
To requisite the time series singularity quantification at local and global
scales, a novel set of fourteen different features. Suitable feature ranking
employing students t-test has been done to select the most statistically
significant features which are henceforth being used as inputs to a support
vector machines (SVM) classifier for the classification of different EEG
signals. Eight different classification problems have been presented in this
paper and it has been observed that the overall classification accuracy using
MFDFA based features are reasonably satisfactory for all classification
problems. The performance of the proposed method are also found to be quite
commensurable and in some cases even better when compared with the results
published in existing literature studied on the similar data set."
['stat.ME'],Specific Differential Entropy Rate Estimation for Continuous-Valued Time Series,"We introduce a method for quantifying the inherent unpredictability of a
continuous-valued time series via an extension of the differential Shannon
entropy rate. Our extension, the specific entropy rate, quantifies the amount
of predictive uncertainty associated with a specific state, rather than
averaged over all states. We relate the specific entropy rate to popular
`complexity' measures such as Approximate and Sample Entropies. We provide a
data-driven approach for estimating the specific entropy rate of an observed
time series. Finally, we consider three case studies of estimating specific
entropy rate from synthetic and physiological data relevant to the analysis of
heart rate variability."
"['stat.ML', 'stat.ME']",Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices,"This study aimed to find temporal clusters for several commodity prices using
the threshold non-linear autoregressive model. It is expected that the process
of determining the commodity groups that are time-dependent will advance the
current knowledge about the dynamics of co-moving and coherent prices, and can
serve as a basis for multivariate time series analyses. The clustering of
commodity prices was examined using the proposed clustering approach based on
time series models to incorporate the time varying properties of price series
into the clustering scheme. Accordingly, the primary aim in this study was
grouping time series according to the similarity between their Data Generating
Mechanisms (DGMs) rather than comparing pattern similarities in the time series
traces. The approximation to the DGM of each series was accomplished using
threshold autoregressive models, which are recognized for their ability to
represent nonlinear features in time series, such as abrupt changes,
time-irreversibility and regime-shifting behavior. Through the use of the
proposed approach, one can determine and monitor the set of co-moving time
series variables across the time dimension. Furthermore, generating a time
varying commodity price index and sub-indexes can become possible.
Consequently, we conducted a simulation study to assess the effectiveness of
the proposed clustering approach and the results are presented for both the
simulated and real data sets."
['stat.ME'],Scalable Linear Causal Inference for Irregularly Sampled Time Series with Long Range Dependencies,"Linear causal analysis is central to a wide range of important application
spanning finance, the physical sciences, and engineering. Much of the existing
literature in linear causal analysis operates in the time domain.
Unfortunately, the direct application of time domain linear causal analysis to
many real-world time series presents three critical challenges: irregular
temporal sampling, long range dependencies, and scale. Moreover, real-world
data is often collected at irregular time intervals across vast arrays of
decentralized sensors and with long range dependencies which make naive time
domain correlation estimators spurious. In this paper we present a frequency
domain based estimation framework which naturally handles irregularly sampled
data and long range dependencies while enabled memory and communication
efficient distributed processing of time series data. By operating in the
frequency domain we eliminate the need to interpolate and help mitigate the
effects of long range dependencies. We implement and evaluate our new work-flow
in the distributed setting using Apache Spark and demonstrate on both Monte
Carlo simulations and high-frequency financial trading that we can accurately
recover causal structure at scale."
"['stat.ML', 'stat.ME', 'stat.TH']",Kernels for sequentially ordered data,"We present a novel framework for kernel learning with sequential data of any
kind, such as time series, sequences of graphs, or strings. Our approach is
based on signature features which can be seen as an ordered variant of sample
(cross-)moments; it allows to obtain a ""sequentialized"" version of any static
kernel. The sequential kernels are efficiently computable for discrete
sequences and are shown to approximate a continuous moment form in a sampling
sense.
  A number of known kernels for sequences arise as ""sequentializations"" of
suitable static kernels: string kernels may be obtained as a special case, and
alignment kernels are closely related up to a modification that resolves their
open non-definiteness issue. Our experiments indicate that our signature-based
sequential kernel framework may be a promising approach to learning with
sequential data, such as time series, that allows to avoid extensive manual
pre-processing."
"['stat.ML', 'stat.TH']",Statistical and Computational Guarantees for the Baum-Welch Algorithm,"The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling
of discrete time series, with applications including speech recognition,
computational biology, computer vision and econometrics. Estimating an HMM from
its observation process is often addressed via the Baum-Welch algorithm, which
is known to be susceptible to local optima. In this paper, we first give a
general characterization of the basin of attraction associated with any global
optimum of the population likelihood. By exploiting this characterization, we
provide non-asymptotic finite sample guarantees on the Baum-Welch updates,
guaranteeing geometric convergence to a small ball of radius on the order of
the minimax rate around a global optimum. As a concrete example, we prove a
linear rate of convergence for a hidden Markov mixture of two isotropic
Gaussians given a suitable mean separation and an initialization within a ball
of large radius around (one of) the true parameters. To our knowledge, these
are the first rigorous local convergence guarantees to global optima for the
Baum-Welch algorithm in a setting where the likelihood function is nonconvex.
We complement our theoretical results with thorough numerical simulations
studying the convergence of the Baum-Welch algorithm and illustrating the
accuracy of our predictions."
"['stat.ML', 'stat.TH']",Online Supervised Subspace Tracking,"We present a framework for supervised subspace tracking, when there are two
time series $x_t$ and $y_t$, one being the high-dimensional predictors and the
other being the response variables and the subspace tracking needs to take into
consideration of both sequences. It extends the classic online subspace
tracking work which can be viewed as tracking of $x_t$ only. Our online
sufficient dimensionality reduction (OSDR) is a meta-algorithm that can be
applied to various cases including linear regression, logistic regression,
multiple linear regression, multinomial logistic regression, support vector
machine, the random dot product model and the multi-scale union-of-subspace
model. OSDR reduces data-dimensionality on-the-fly with low-computational
complexity and it can also handle missing data and dynamic data. OSDR uses an
alternating minimization scheme and updates the subspace via gradient descent
on the Grassmannian manifold. The subspace update can be performed efficiently
utilizing the fact that the Grassmannian gradient with respect to the subspace
in many settings is rank-one (or low-rank in certain cases). The optimization
problem for OSDR is non-convex and hard to analyze in general; we provide
convergence analysis of OSDR in a simple linear regression setting. The good
performance of OSDR compared with the conventional unsupervised subspace
tracking are demonstrated via numerical examples on simulated and real data."
"['stat.ML', 'stat.ME']",Optimal model-free prediction from multivariate time series,"Forecasting a time series from multivariate predictors constitutes a
challenging problem, especially using model-free approaches. Most techniques,
such as nearest-neighbor prediction, quickly suffer from the curse of
dimensionality and overfitting for more than a few predictors which has limited
their application mostly to the univariate case. Therefore, selection
strategies are needed that harness the available information as efficiently as
possible. Since often the right combination of predictors matters, ideally all
subsets of possible predictors should be tested for their predictive power, but
the exponentially growing number of combinations makes such an approach
computationally prohibitive. Here a prediction scheme that overcomes this
strong limitation is introduced utilizing a causal pre-selection step which
drastically reduces the number of possible predictors to the most predictive
set of causal drivers making a globally optimal search scheme tractable. The
information-theoretic optimality is derived and practical selection criteria
are discussed. As demonstrated for multivariate nonlinear stochastic delay
processes, the optimal scheme can even be less computationally expensive than
commonly used sub-optimal schemes like forward selection. The method suggests a
general framework to apply the optimal model-free approach to select variables
and subsequently fit a model to further improve a prediction or learn
statistical dependencies. The performance of this framework is illustrated on a
climatological index of El Ni\~no Southern Oscillation."
"['stat.ML', 'stat.ME']",Model-based clustering with Hidden Markov Model regression for time series with regime changes,"This paper introduces a novel model-based clustering approach for clustering
time series which present changes in regime. It consists of a mixture of
polynomial regressions governed by hidden Markov chains. The underlying hidden
process for each cluster activates successively several polynomial regimes
during time. The parameter estimation is performed by the maximum likelihood
method through a dedicated Expectation-Maximization (EM) algorithm. The
proposed approach is evaluated using simulated time series and real-world time
series issued from a railway diagnosis application. Comparisons with existing
approaches for time series clustering, including the stand EM for Gaussian
mixtures, $K$-means clustering, the standard mixture of regression models and
mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed
approach."
"['stat.ML', 'stat.TH']",Multiple Change Point Estimation in Stationary Ergodic Time Series,"Given a heterogeneous time-series sample, the objective is to find points in
time (called change points) where the probability distribution generating the
data has changed. The data are assumed to have been generated by arbitrary
unknown stationary ergodic distributions. No modelling, independence or mixing
assumptions are made. A novel, computationally efficient, nonparametric method
is proposed, and is shown to be asymptotically consistent in this general
framework. The theoretical results are complemented with experimental
evaluations."
"['stat.ML', 'stat.ME']",Structure Discovery in Nonparametric Regression through Compositional Kernel Search,"Despite its importance, choosing the structural form of the kernel in
nonparametric regression remains a black art. We define a space of kernel
structures which are built compositionally by adding and multiplying a small
number of base kernels. We present a method for searching over this space of
structures which mirrors the scientific discovery process. The learned
structures can often decompose functions into interpretable components and
enable long-range extrapolation on time-series datasets. Our structure search
method outperforms many widely used kernels and kernel combination methods on a
variety of prediction tasks."
"['stat.ML', 'stat.TH']","Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory","We introduce a class of quadratic support (QS) functions, many of which play
a crucial role in a variety of applications, including machine learning, robust
statistical inference, sparsity promotion, and Kalman smoothing. Well known
examples include the l2, Huber, l1 and Vapnik losses. We build on a dual
representation for QS functions using convex analysis, revealing the structure
necessary for a QS function to be interpreted as the negative log of a
probability density, and providing the foundation for statistical
interpretation and analysis of QS loss functions. For a subclass of QS
functions called piecewise linear quadratic (PLQ) penalties, we also develop
efficient numerical estimation schemes. These components form a flexible
statistical modeling framework for a variety of learning applications, together
with a toolbox of efficient numerical methods for inference. In particular, for
PLQ densities, interior point (IP) methods can be used. IP methods solve
nonsmooth optimization problems by working directly with smooth systems of
equations characterizing their optimality. The efficiency of the IP approach
depends on the structure of particular applications. We consider the class of
dynamic inverse problems using Kalman smoothing, where the aim is to
reconstruct the state of a dynamical system with known process and measurement
models starting from noisy output samples. In the classical case, Gaussian
errors are assumed in the process and measurement models. The extended
framework allows arbitrary PLQ densities to be used, and the proposed IP
approach solves the generalized Kalman smoothing problem while maintaining the
linear complexity in the size of the time series, just as in the Gaussian case.
This extends the computational efficiency of classic algorithms to a much
broader nonsmooth setting, and includes many recently proposed robust and
sparse smoothers as special cases."
"['stat.ML', 'stat.TH']",A consistent clustering-based approach to estimating the number of change-points in highly dependent time-series,"The problem of change-point estimation is considered under a general
framework where the data are generated by unknown stationary ergodic process
distributions. In this context, the consistent estimation of the number of
change-points is provably impossible. However, it is shown that a consistent
clustering method may be used to estimate the number of change points, under
the additional constraint that the correct number of process distributions that
generate the data is provided. This additional parameter has a natural
interpretation in many real-world applications. An algorithm is proposed that
estimates the number of change-points and locates the changes. The proposed
algorithm is shown to be asymptotically consistent; its empirical evaluations
are provided."
"['stat.ML', 'stat.ME']",Change-Point Detection in Time-Series Data by Relative Density-Ratio Estimation,"The objective of change-point detection is to discover abrupt property
changes lying behind time-series data. In this paper, we present a novel
statistical change-point detection algorithm based on non-parametric divergence
estimation between time-series samples from two retrospective segments. Our
method uses the relative Pearson divergence as a divergence measure, and it is
accurately and efficiently estimated by a method of direct density-ratio
estimation. Through experiments on artificial and real-world datasets including
human-activity sensing, speech, and Twitter messages, we demonstrate the
usefulness of the proposed method."
"['stat.ML', 'stat.ME']",Gaussian process modelling of multiple short time series,"We present techniques for effective Gaussian process (GP) modelling of
multiple short time series. These problems are common when applying GP models
independently to each gene in a gene expression time series data set. Such sets
typically contain very few time points. Naive application of common GP
modelling techniques can lead to severe over-fitting or under-fitting in a
significant fraction of the fitted models, depending on the details of the data
set. We propose avoiding over-fitting by constraining the GP length-scale to
values that focus most of the energy spectrum to frequencies below the Nyquist
frequency corresponding to the sampling frequency in the data set.
Under-fitting can be avoided by more informative priors on observation noise.
Combining these methods allows applying GP methods reliably automatically to
large numbers of independent instances of short time series. This is
illustrated with experiments with both synthetic data and real gene expression
data."
"['stat.ML', 'stat.ME']",Causal Inference on Time Series using Structural Equation Models,"Causal inference uses observations to infer the causal structure of the data
generating system. We study a class of functional models that we call Time
Series Models with Independent Noise (TiMINo). These models require independent
residual time series, whereas traditional methods like Granger causality
exploit the variance of residuals. There are two main contributions: (1)
Theoretical: By restricting the model class (e.g. to additive noise) we can
provide a more general identifiability result than existing ones. This result
incorporates lagged and instantaneous effects that can be nonlinear and do not
need to be faithful, and non-instantaneous feedbacks between the time series.
(2) Practical: If there are no feedback loops between time series, we propose
an algorithm based on non-linear independence tests of time series. When the
data are causally insufficient, or the data generating process does not satisfy
the model assumptions, this algorithm may still give partial results, but
mostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks
is possible, but not discussed. It outperforms existing methods on artificial
and real data. Code can be provided upon request."
"['stat.ML', 'stat.ME']",A Nonparametric Frequency Domain EM Algorithm for Time Series Classification with Applications to Spike Sorting and Macro-Economics,"I propose a frequency domain adaptation of the Expectation Maximization (EM)
algorithm to group a family of time series in classes of similar dynamic
structure. It does this by viewing the magnitude of the discrete Fourier
transform (DFT) of each signal (or power spectrum) as a probability
density/mass function (pdf/pmf) on the unit circle: signals with similar
dynamics have similar pdfs; distinct patterns have distinct pdfs. An advantage
of this approach is that it does not rely on any parametric form of the dynamic
structure, but can be used for non-parametric, robust and model-free
classification. This new method works for non-stationary signals of similar
shape as well as stationary signals with similar auto-correlation structure.
Applications to neural spike sorting (non-stationary) and pattern-recognition
in socio-economic time series (stationary) demonstrate the usefulness and wide
applicability of the proposed method."
"['stat.ML', 'stat.ME']",Adapting to Non-stationarity with Growing Expert Ensembles,"When dealing with time series with complex non-stationarities, low
retrospective regret on individual realizations is a more appropriate goal than
low prospective risk in expectation. Online learning algorithms provide
powerful guarantees of this form, and have often been proposed for use with
non-stationary processes because of their ability to switch between different
forecasters or ``experts''. However, existing methods assume that the set of
experts whose forecasts are to be combined are all given at the start, which is
not plausible when dealing with a genuinely historical or evolutionary system.
We show how to modify the ``fixed shares'' algorithm for tracking the best
expert to cope with a steadily growing set of experts, obtained by fitting new
models to new data as it becomes available, and obtain regret bounds for the
growing ensemble."
"['stat.ML', 'stat.ME']",Dynamic Large Spatial Covariance Matrix Estimation in Application to Semiparametric Model Construction via Variable Clustering: the SCE approach,"To better understand the spatial structure of large panels of economic and
financial time series and provide a guideline for constructing semiparametric
models, this paper first considers estimating a large spatial covariance matrix
of the generalized $m$-dependent and $\beta$-mixing time series (with $J$
variables and $T$ observations) by hard thresholding regularization as long as
${{\log J \, \cx^*(\ct)}}/{T} = \Co(1)$ (the former scheme with some time
dependence measure $\cx^*(\ct)$) or $\log J /{T} = \Co(1)$ (the latter scheme
with some upper bounded mixing coefficient). We quantify the interplay between
the estimators' consistency rate and the time dependence level, discuss an
intuitive resampling scheme for threshold selection, and also prove a general
cross-validation result justifying this. Given a consistently estimated
covariance (correlation) matrix, by utilizing its natural links with graphical
models and semiparametrics, after ""screening"" the (explanatory) variables, we
implement a novel forward (and backward) label permutation procedure to cluster
the ""relevant"" variables and construct the corresponding semiparametric model,
which is further estimated by the groupwise dimension reduction method with
sign constraints. We call this the SCE (screen - cluster - estimate) approach
for modeling high dimensional data with complex spatial structure. Finally we
apply this method to study the spatial structure of large panels of economic
and financial time series and find the proper semiparametric structure for
estimating the consumer price index (CPI) to illustrate its superiority over
the linear models."
"['stat.ML', 'stat.CO', 'stat.TH']",Residual Component Analysis,"Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an
eigenvalue problem on the sample covariance matrix. In this paper we consider
the situation where the data variance is already partially explained by other
factors, e.g. covariates of interest, or temporal correlations leaving some
residual variance. We decompose the residual variance into its components
through a generalized eigenvalue problem, which we call residual component
analysis (RCA). We show that canonical covariates analysis (CCA) is a special
case of our algorithm and explore a range of new algorithms that arise from the
framework. We illustrate the ideas on a gene expression time series data set
and the recovery of human pose from silhouette."
"['stat.ML', 'stat.ME']",Large Vector Auto Regressions,"One popular approach for nonstructural economic and financial forecasting is
to include a large number of economic and financial variables, which has been
shown to lead to significant improvements for forecasting, for example, by the
dynamic factor models. A challenging issue is to determine which variables and
(their) lags are relevant, especially when there is a mixture of serial
correlation (temporal dynamics), high dimensional (spatial) dependence
structure and moderate sample size (relative to dimensionality and lags). To
this end, an \textit{integrated} solution that addresses these three challenges
simultaneously is appealing. We study the large vector auto regressions here
with three types of estimates. We treat each variable's own lags different from
other variables' lags, distinguish various lags over time, and is able to
select the variables and lags simultaneously. We first show the consequences of
using Lasso type estimate directly for time series without considering the
temporal dependence. In contrast, our proposed method can still produce an
estimate as efficient as an \textit{oracle} under such scenarios. The tuning
parameters are chosen via a data driven ""rolling scheme"" method to optimize the
forecasting performance. A macroeconomic and financial forecasting problem is
considered to illustrate its superiority over existing estimators."
"['stat.ML', 'stat.ME']",Estimating time-varying networks,"Stochastic networks are a plausible representation of the relational
information among entities in dynamic systems such as living cells or social
communities. While there is a rich literature in estimating a static or
temporally invariant network from observation data, little has been done toward
estimating time-varying networks from time series of entity attributes. In this
paper we present two new machine learning methods for estimating time-varying
networks, which both build on a temporally smoothed $l_1$-regularized logistic
regression formalism that can be cast as a standard convex-optimization problem
and solved efficiently using generic solvers scalable to large networks. We
report promising results on recovering simulated time-varying networks. For
real data sets, we reverse engineer the latent sequence of temporally rewiring
political networks between Senators from the US Senate voting records and the
latent evolving regulatory networks underlying 588 genes across the life cycle
of Drosophila melanogaster from the microarray time course."
"['stat.ML', 'stat.ME']",Discovering shared and individual latent structure in multiple time series,"This paper proposes a nonparametric Bayesian method for exploratory data
analysis and feature construction in continuous time series. Our method focuses
on understanding shared features in a set of time series that exhibit
significant individual variability. Our method builds on the framework of
latent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet
processes, which allows us to characterize each series as switching between
latent ``topics'', where each topic is characterized as a distribution over
``words'' that specify the series dynamics. However, unlike standard
applications of LDA, we discover the words as we learn the model. We apply this
model to the task of tracking the physiological signals of premature infants;
our model obtains clinically significant insights as well as useful features
for supervised learning tasks."
['stat.TH'],Time Series Forecasting: Obtaining Long Term Trends with Self-Organizing Maps,"Kohonen self-organisation maps are a well know classification tool, commonly
used in a wide variety of problems, but with limited applications in time
series forecasting context. In this paper, we propose a forecasting method
specifically designed for multi-dimensional long-term trends prediction, with a
double application of the Kohonen algorithm. Practical applications of the
method are also presented."
"['stat.ML', 'stat.TH']",Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning,"Cooperative multi-agent reinforcement learning is a decentralized paradigm in
sequential decision making where agents distributed over a network iteratively
collaborate with neighbors to maximize global (network-wide) notions of
rewards. Exact computations typically involve a complexity that scales
exponentially with the number of agents. To address this curse of
dimensionality, we design a scalable algorithm based on the Natural Policy
Gradient framework that uses local information and only requires agents to
communicate with neighbors within a certain range. Under standard assumptions
on the spatial decay of correlations for the transition dynamics of the
underlying Markov process and the localized learning policy, we show that our
algorithm converges to the globally optimal policy with a dimension-free
statistical and computational complexity, incurring a localization error that
does not depend on the number of agents and converges to zero exponentially
fast as a function of the range of communication."
"['stat.CO', 'stat.ML']",A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning,"This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided."
['stat.ME'],Projected State-action Balancing Weights for Offline Reinforcement Learning,"Offline policy evaluation (OPE) is considered a fundamental and challenging
problem in reinforcement learning (RL). This paper focuses on the value
estimation of a target policy based on pre-collected data generated from a
possibly different policy, under the framework of infinite-horizon Markov
decision processes. Motivated by the recently developed marginal importance
sampling method in RL and the covariate balancing idea in causal inference, we
propose a novel estimator with approximately projected state-action balancing
weights for the policy value estimation. We obtain the convergence rate of
these weights, and show that the proposed value estimator is semi-parametric
efficient under technical conditions. In terms of asymptotics, our results
scale with both the number of trajectories and the number of decision points at
each trajectory. As such, consistency can still be achieved with a limited
number of subjects when the number of decision points diverges. In addition, we
make a first attempt towards characterizing the difficulty of OPE problems,
which may be of independent interest. Numerical experiments demonstrate the
promising performance of our proposed estimator."
"['stat.ME', 'stat.ML']",Deep Reinforcement Learning at the Edge of the Statistical Precipice,"Deep reinforcement learning (RL) algorithms are predominantly evaluated by
comparing their relative performance on a large suite of tasks. Most published
results on deep RL benchmarks compare point estimates of aggregate performance
such as mean and median scores across tasks, ignoring the statistical
uncertainty implied by the use of a finite number of training runs. Beginning
with the Arcade Learning Environment (ALE), the shift towards
computationally-demanding benchmarks has led to the practice of evaluating only
a small number of runs per task, exacerbating the statistical uncertainty in
point estimates. In this paper, we argue that reliable evaluation in the few
run deep RL regime cannot ignore the uncertainty in results without running the
risk of slowing down progress in the field. We illustrate this point using a
case study on the Atari 100k benchmark, where we find substantial discrepancies
between conclusions drawn from point estimates alone versus a more thorough
statistical analysis. With the aim of increasing the field's confidence in
reported results with a handful of runs, we advocate for reporting interval
estimates of aggregate performance and propose performance profiles to account
for the variability in results, as well as present more robust and efficient
aggregate metrics, such as interquartile mean scores, to achieve small
uncertainty in results. Using such statistical tools, we scrutinize performance
evaluations of existing algorithms on other widely used RL benchmarks including
the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies
in prior comparisons. Our findings call for a change in how we evaluate
performance in deep RL, for which we present a more rigorous evaluation
methodology, accompanied with an open-source library rliable, to prevent
unreliable results from stagnating the field."
"['stat.ML', 'stat.TH']",Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning,"The recent emergence of reinforcement learning has created a demand for
robust statistical inference methods for the parameter estimates computed using
these algorithms. Existing methods for statistical inference in online learning
are restricted to settings involving independently sampled observations, while
existing statistical inference methods in reinforcement learning (RL) are
limited to the batch setting. The online bootstrap is a flexible and efficient
approach for statistical inference in linear stochastic approximation
algorithms, but its efficacy in settings involving Markov noise, such as RL,
has yet to be explored. In this paper, we study the use of the online bootstrap
method for statistical inference in RL. In particular, we focus on the temporal
difference (TD) learning and Gradient TD (GTD) learning algorithms, which are
themselves special instances of linear stochastic approximation under Markov
noise. The method is shown to be distributionally consistent for statistical
inference in policy evaluation, and numerical experiments are included to
demonstrate the effectiveness of this algorithm at statistical inference tasks
across a range of real RL environments."
"['stat.ME', 'stat.ML']",Debiased Off-Policy Evaluation for Recommendation Systems,"Efficient methods to evaluate new algorithms are critical for improving
interactive bandit and reinforcement learning systems such as recommendation
systems. A/B tests are reliable, but are time- and money-consuming, and entail
a risk of failure. In this paper, we develop an alternative method, which
predicts the performance of algorithms given historical data that may have been
generated by a different algorithm. Our estimator has the property that its
prediction converges in probability to the true performance of a counterfactual
algorithm at a rate of $\sqrt{N}$, as the sample size $N$ increases. We also
show a correct way to estimate the variance of our prediction, thus allowing
the analyst to quantify the uncertainty in the prediction. These properties
hold even when the analyst does not know which among a large number of
potentially important state variables are actually important. We validate our
method by a simulation experiment about reinforcement learning. We finally
apply it to improve advertisement design by a major advertisement company. We
find that our method produces smaller mean squared errors than state-of-the-art
methods."
"['stat.ML', 'stat.TH']",Pathwise Conditioning of Gaussian Processes,"As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning."
"['stat.ME', 'stat.ML', 'stat.TH']",A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification,"Black-box machine learning learning methods are now routinely used in
high-risk settings, like medical diagnostics, which demand uncertainty
quantification to avoid consequential model failures. Distribution-free
uncertainty quantification (distribution-free UQ) is a user-friendly paradigm
for creating statistically rigorous confidence intervals/sets for such
predictions. Critically, the intervals/sets are valid without distributional
assumptions or model assumptions, with explicit guarantees with finitely many
datapoints. Moreover, they adapt to the difficulty of the input; when the input
example is difficult, the uncertainty intervals/sets are large, signaling that
the model might be wrong. Without much work, one can use distribution-free
methods on any underlying algorithm, such as a neural network, to produce
confidence sets guaranteed to contain the ground truth with a user-specified
probability, such as 90%. Indeed, the methods are easy-to-understand and
general, applying to many modern prediction problems arising in the fields of
computer vision, natural language processing, deep reinforcement learning, and
so on. This hands-on introduction is aimed at a reader interested in the
practical implementation of distribution-free UQ, including conformal
prediction and related methods, who is not necessarily a statistician. We will
include many explanatory illustrations, examples, and code samples in Python,
with PyTorch syntax. The goal is to provide the reader a working understanding
of distribution-free UQ, allowing them to put confidence intervals on their
algorithms, with one self-contained document."
['stat.TH'],A Unified Off-Policy Evaluation Approach for General Value Function,"General Value Function (GVF) is a powerful tool to represent both the {\em
predictive} and {\em retrospective} knowledge in reinforcement learning (RL).
In practice, often multiple interrelated GVFs need to be evaluated jointly with
pre-collected off-policy samples. In the literature, the gradient temporal
difference (GTD) learning method has been adopted to evaluate GVFs in the
off-policy setting, but such an approach may suffer from a large estimation
error even if the function approximation class is sufficiently expressive.
Moreover, none of the previous work have formally established the convergence
guarantee to the ground truth GVFs under the function approximation settings.
In this paper, we address both issues through the lens of a class of GVFs with
causal filtering, which cover a wide range of RL applications such as reward
variance, value gradient, cost in anomaly detection, stationary distribution
gradient, etc. We propose a new algorithm called GenTD for off-policy GVFs
evaluation and show that GenTD learns multiple interrelated multi-dimensional
GVFs as efficiently as a single canonical scalar value function. We further
show that unlike GTD, the learned GVFs by GenTD are guaranteed to converge to
the ground truth GVFs as long as the function approximation power is
sufficiently large. To our best knowledge, GenTD is the first off-policy GVF
evaluation algorithm that has global optimality guarantee."
"['stat.ML', 'stat.TH']",Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model,"The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper."
"['stat.ML', 'stat.TH']",Sample-Efficient Reinforcement Learning Is Feasible for Linearly Realizable MDPs with Limited Revisiting,"Low-complexity models such as linear function representation play a pivotal
role in enabling sample-efficient reinforcement learning (RL). The current
paper pertains to a scenario with value-based linear representation, which
postulates the linear realizability of the optimal Q-function (also called the
""linear $Q^{\star}$ problem""). While linear realizability alone does not allow
for sample-efficient solutions in general, the presence of a large
sub-optimality gap is a potential game changer, depending on the sampling
mechanism in use. Informally, sample efficiency is achievable with a large
sub-optimality gap when a generative model is available but is unfortunately
infeasible when we turn to standard online RL settings.
  In this paper, we make progress towards understanding this linear $Q^{\star}$
problem by investigating a new sampling protocol, which draws samples in an
online/exploratory fashion but allows one to backtrack and revisit previous
states in a controlled and infrequent manner. This protocol is more flexible
than the standard online RL setting, while being practically relevant and far
more restrictive than the generative model. We develop an algorithm tailored to
this setting, achieving a sample complexity that scales polynomially with the
feature dimension, the horizon, and the inverse sub-optimality gap, but not the
size of the state/action space. Our findings underscore the fundamental
interplay between sampling protocols and low-complexity structural
representation in RL."
"['stat.ML', 'stat.TH']",Is Pessimism Provably Efficient for Offline RL?,"We study offline reinforcement learning (RL), which aims to learn an optimal
policy based on a dataset collected a priori. Due to the lack of further
interactions with the environment, offline RL suffers from the insufficient
coverage of the dataset, which eludes most existing theoretical analysis. In
this paper, we propose a pessimistic variant of the value iteration algorithm
(PEVI), which incorporates an uncertainty quantifier as the penalty function.
Such a penalty function simply flips the sign of the bonus function for
promoting exploration in online RL, which makes it easily implementable and
compatible with general function approximators.
  Without assuming the sufficient coverage of the dataset, we establish a
data-dependent upper bound on the suboptimality of PEVI for general Markov
decision processes (MDPs). When specialized to linear MDPs, it matches the
information-theoretic lower bound up to multiplicative factors of the dimension
and horizon. In other words, pessimism is not only provably efficient but also
minimax optimal. In particular, given the dataset, the learned policy serves as
the ""best effort"" among all policies, as no other policies can do better. Our
theoretical analysis identifies the critical role of pessimism in eliminating a
notion of spurious correlation, which emerges from the ""irrelevant""
trajectories that are less covered by the dataset and not informative for the
optimal policy."
"['stat.ML', 'stat.CO']",Variational inference for the multi-armed contextual bandit,"In many biomedical, science, and engineering problems, one must sequentially
decide which action to take next so as to maximize rewards. One general class
of algorithms for optimizing interactions with the world, while simultaneously
learning how the world operates, is the multi-armed bandit setting and, in
particular, the contextual bandit case. In this setting, for each executed
action, one observes rewards that are dependent on a given 'context', available
at each interaction with the world. The Thompson sampling algorithm has
recently been shown to enjoy provable optimality properties for this set of
problems, and to perform well in real-world settings. It facilitates generative
and interpretable modeling of the problem at hand. Nevertheless, the design and
complexity of the model limit its application, since one must both sample from
the distributions modeled and calculate their expected rewards. We here show
how these limitations can be overcome using variational inference to
approximate complex models, applying to the reinforcement learning case
advances developed for the inference case in the machine learning community
over the past two decades. We consider contextual multi-armed bandit
applications where the true reward distribution is unknown and complex, which
we approximate with a mixture model whose parameters are inferred via
variational inference. We show how the proposed variational Thompson sampling
approach is accurate in approximating the true distribution, and attains
reduced regrets even with complex reward distributions. The proposed algorithm
is valuable for practical scenarios where restrictive modeling assumptions are
undesirable."
"['stat.CO', 'stat.ML']",Bandit-Based Monte Carlo Optimization for Nearest Neighbors,"The celebrated Monte Carlo method estimates an expensive-to-compute quantity
by random sampling. Bandit-based Monte Carlo optimization is a general
technique for computing the minimum of many such expensive-to-compute
quantities by adaptive random sampling. The technique converts an optimization
problem into a statistical estimation problem which is then solved via
multi-armed bandits. We apply this technique to solve the problem of
high-dimensional $k$-nearest neighbors, developing an algorithm which we prove
is able to identify exact nearest neighbors with high probability. We show that
under regularity assumptions on a dataset of $n$ points in $d$-dimensional
space, the complexity of our algorithm scales logarithmically with the
dimension of the data as $O\left((n+d)\log^2
\left(\frac{nd}{\delta}\right)\right)$ for error probability $\delta$, rather
than linearly as in exact computation requiring $O(nd)$. We corroborate our
theoretical results with numerical simulations, showing that our algorithm
outperforms both exact computation and state-of-the-art algorithms such as
kGraph, NGT, and LSH on real datasets."
"['stat.ME', 'stat.ML']",Outcome-Driven Reinforcement Learning via Variational Inference,"While reinforcement learning algorithms provide automated acquisition of
optimal policies, practical application of such methods requires a number of
design decisions, such as manually designing reward functions that not only
define the task, but also provide sufficient shaping to accomplish it. In this
paper, we discuss a new perspective on reinforcement learning, recasting it as
the problem of inferring actions that achieve desired outcomes, rather than a
problem of maximizing rewards. To solve the resulting outcome-directed
inference problem, we establish a novel variational inference formulation that
allows us to derive a well-shaped reward function which can be learned directly
from environment interactions. From the corresponding variational objective, we
also derive a new probabilistic Bellman backup operator reminiscent of the
standard Bellman backup operator and use it to develop an off-policy algorithm
to solve goal-directed tasks. We empirically demonstrate that this method
eliminates the need to design reward functions and leads to effective
goal-directed behaviors."
"['stat.ML', 'stat.TH']",Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism,"Offline (or batch) reinforcement learning (RL) algorithms seek to learn an
optimal policy from a fixed dataset without active data collection. Based on
the composition of the offline dataset, two main categories of methods are
used: imitation learning which is suitable for expert datasets and vanilla
offline RL which often requires uniform coverage datasets. From a practical
standpoint, datasets often deviate from these two extremes and the exact data
composition is usually unknown a priori. To bridge this gap, we present a new
offline RL framework that smoothly interpolates between the two extremes of
data composition, hence unifying imitation learning and vanilla offline RL. The
new framework is centered around a weak version of the concentrability
coefficient that measures the deviation from the behavior policy to the expert
policy alone.
  Under this new framework, we further investigate the question on algorithm
design: can one develop an algorithm that achieves a minimax optimal rate and
also adapts to unknown data composition? To address this question, we consider
a lower confidence bound (LCB) algorithm developed based on pessimism in the
face of uncertainty in offline RL. We study finite-sample properties of LCB as
well as information-theoretic limits in multi-armed bandits, contextual
bandits, and Markov decision processes (MDPs). Our analysis reveals surprising
facts about optimality rates. In particular, in all three settings, LCB
achieves a faster rate of $1/N$ for nearly-expert datasets compared to the
usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in
the batch dataset. In the case of contextual bandits with at least two
contexts, we prove that LCB is adaptively optimal for the entire data
composition range, achieving a smooth transition from imitation learning to
offline RL. We further show that LCB is almost adaptively optimal in MDPs."
"['stat.ML', 'stat.TH']",Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis,"Q-learning, which seeks to learn the optimal Q-function of a Markov decision
process (MDP) in a model-free fashion, lies at the heart of reinforcement
learning. When it comes to the synchronous setting (such that independent
samples for all state-action pairs are drawn from a generative model in each
iteration), substantial progress has been made recently towards understanding
the sample efficiency of Q-learning. Take a $\gamma$-discounted
infinite-horizon MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$: to yield an entrywise $\varepsilon$-accurate estimate of the
optimal Q-function, state-of-the-art theory for Q-learning proves that a sample
size on the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$ is sufficient,
which, however, fails to match with the existing minimax lower bound. This
gives rise to natural questions: what is the sharp sample complexity of
Q-learning? Is Q-learning provably sub-optimal? In this work, we settle these
questions by (1) demonstrating that the sample complexity of Q-learning is at
most on the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to some log
factor) for any $0<\varepsilon <1$, and (2) developing a matching lower bound
to confirm the sharpness of our result. Our findings unveil both the
effectiveness and limitation of Q-learning: its sample complexity matches that
of speedy Q-learning without requiring extra computation and storage, albeit
still being considerably higher than the minimax lower bound."
"['stat.ML', 'stat.ME']",Optimal sequential decision making with probabilistic digital twins,"Digital twins are emerging in many industries, typically consisting of
simulation models and data associated with a specific physical system. One of
the main reasons for developing a digital twin, is to enable the simulation of
possible consequences of a given action, without the need to interfere with the
physical system itself. Physical systems of interest, and the environments they
operate in, do not always behave deterministically. Moreover, information about
the system and its environment is typically incomplete or imperfect.
Probabilistic representations of systems and environments may therefore be
called for, especially to support decisions in application areas where actions
may have severe consequences.
  In this paper we introduce the probabilistic digital twin (PDT). We will
start by discussing how epistemic uncertainty can be treated using measure
theory, by modelling epistemic information via $\sigma$-algebras. Based on
this, we give a formal definition of how epistemic uncertainty can be updated
in a PDT. We then study the problem of optimal sequential decision making. That
is, we consider the case where the outcome of each decision may inform the
next. Within the PDT framework, we formulate this optimization problem. We
discuss how this problem may be solved (at least in theory) via the maximum
principle method or the dynamic programming principle. However, due to the
curse of dimensionality, these methods are often not tractable in practice. To
mend this, we propose a generic approximate solution using deep reinforcement
learning together with neural networks defined on sets. We illustrate the
method on a practical problem, considering optimal information gathering for
the estimation of a failure probability."
"['stat.ME', 'stat.ML']",A Reinforcement Learning Based Approach to Play Calling in Football,"With the vast amount of data collected on football and the growth of
computing abilities, many games involving decision choices can be optimized.
The underlying rule is the maximization of an expected utility of outcomes and
the law of large numbers. The data available allows us to compute with high
accuracy the probabilities of outcomes of decisions and the well defined points
system in the game allows us to have the necessary terminal utilities. With
some well established theory we can then optimize choices at a single play
level."
"['stat.ME', 'stat.ML']",Semi-Supervised Off Policy Reinforcement Learning,"Reinforcement learning (RL) has shown great success in estimating sequential
treatment strategies which take into account patient heterogeneity. However,
health-outcome information, which is used as the reward for reinforcement
learning methods, is often not well coded but rather embedded in clinical
notes. Extracting precise outcome information is a resource intensive task, so
most of the available well-annotated cohorts are small. To address this issue,
we propose a semi-supervised learning (SSL) approach that efficiently leverages
a small sized labeled data with true outcome observed, and a large unlabeled
data with outcome surrogates. In particular, we propose a semi-supervised,
efficient approach to Q-learning and doubly robust off policy value estimation.
Generalizing SSL to sequential treatment regimes brings interesting challenges:
1) Feature distribution for Q-learning is unknown as it includes previous
outcomes. 2) The surrogate variables we leverage in the modified SSL framework
are predictive of the outcome but not informative to the optimal policy or
value function. We provide theoretical results for our Q-function and value
function estimators to understand to what degree efficiency can be gained from
SSL. Our method is at least as efficient as the supervised approach, and
moreover safe as it robust to mis-specification of the imputation models."
"['stat.ML', 'stat.TH']",Upper Counterfactual Confidence Bounds: a New Optimism Principle for Contextual Bandits,"The principle of optimism in the face of uncertainty is one of the most
widely used and successful ideas in multi-armed bandits and reinforcement
learning. However, existing optimistic algorithms (primarily UCB and its
variants) are often unable to deal with large context spaces. Essentially all
existing well performing algorithms for general contextual bandit problems rely
on weighted action allocation schemes; and theoretical guarantees for
optimism-based algorithms are only known for restricted formulations. In this
paper we study general contextual bandits under the realizability condition,
and propose a simple generic principle to design optimistic algorithms, dubbed
""Upper Counterfactual Confidence Bounds"" (UCCB). We show that these algorithms
are provably optimal and efficient in the presence of large context spaces. Key
components of UCCB include: 1) a systematic analysis of confidence bounds in
policy space rather than in action space; and 2) the potential function
perspective that is used to express the power of optimism in the contextual
setting. We further show how the UCCB principle can be extended to infinite
action spaces, by constructing confidence bounds via the newly introduced
notion of ""counterfactual action divergence."""
"['stat.ML', 'stat.TH']",Online Sparse Reinforcement Learning,"We investigate the hardness of online reinforcement learning in fixed
horizon, sparse linear Markov decision process (MDP), with a special focus on
the high-dimensional regime where the ambient dimension is larger than the
number of episodes. Our contribution is two-fold. First, we provide a lower
bound showing that linear regret is generally unavoidable in this case, even if
there exists a policy that collects well-conditioned data. The lower bound
construction uses an MDP with a fixed number of states while the number of
actions scales with the ambient dimension. Note that when the horizon is fixed
to one, the case of linear stochastic bandits, the linear regret can be
avoided. Second, we show that if the learner has oracle access to a policy that
collects well-conditioned data then a variant of Lasso fitted Q-iteration
enjoys a nearly dimension-free regret of $\tilde{O}( s^{2/3} N^{2/3})$ where
$N$ is the number of episodes and $s$ is the sparsity level. This shows that in
the large-action setting, the difficulty of learning can be attributed to the
difficulty of finding a good exploratory policy."
"['stat.ML', 'stat.TH']",Bootstrapping Statistical Inference for Off-Policy Evaluation,"Bootstrapping provides a flexible and effective approach for assessing the
quality of batch reinforcement learning, yet its theoretical property is less
understood. In this paper, we study the use of bootstrapping in off-policy
evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE)
that is known to be minimax-optimal in the tabular and linear-model cases. We
propose a bootstrapping FQE method for inferring the distribution of the policy
evaluation error and show that this method is asymptotically efficient and
distributionally consistent for off-policy statistical inference. To overcome
the computation limit of bootstrapping, we further adapt a subsampling
procedure that improves the runtime by an order of magnitude. We numerically
evaluate the bootrapping method in classical RL environments for confidence
interval estimation, estimating the variance of off-policy evaluator, and
estimating the correlation between multiple off-policy evaluators."
"['stat.ML', 'stat.TH']","Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-Order Efficiency","We offer a theoretical characterization of off-policy evaluation (OPE) in
reinforcement learning using function approximation for marginal importance
weights and $q$-functions when these are estimated using recent minimax
methods. Under various combinations of realizability and completeness
assumptions, we show that the minimax approach enables us to achieve a fast
rate of convergence for weights and quality functions, characterized by the
critical inequality \citep{bartlett2005}. Based on this result, we analyze
convergence rates for OPE. In particular, we introduce novel alternative
completeness conditions under which OPE is feasible and we present the first
finite-sample result with first-order efficiency in non-tabular environments,
i.e., having the minimal coefficient in the leading term."
"['stat.ML', 'stat.TH']",On the Stability of Random Matrix Product with Markovian Noise: Application to Linear Stochastic Approximation and TD Learning,"This paper studies the exponential stability of random matrix products driven
by a general (possibly unbounded) state space Markov chain. It is a cornerstone
in the analysis of stochastic algorithms in machine learning (e.g. for
parameter tracking in online learning or reinforcement learning). The existing
results impose strong conditions such as uniform boundedness of the
matrix-valued functions and uniform ergodicity of the Markov chains. Our main
contribution is an exponential stability result for the $p$-th moment of random
matrix product, provided that (i) the underlying Markov chain satisfies a
super-Lyapunov drift condition, (ii) the growth of the matrix-valued functions
is controlled by an appropriately defined function (related to the drift
condition). Using this result, we give finite-time $p$-th moment bounds for
constant and decreasing stepsize linear stochastic approximation schemes with
Markovian noise on general state space. We illustrate these findings for linear
value-function estimation in reinforcement learning. We provide finite-time
$p$-th moment bound for various members of temporal difference (TD) family of
algorithms."
"['stat.ML', 'stat.TH']",On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces,"The classical theory of reinforcement learning (RL) has focused on tabular
and linear representations of value functions. Further progress hinges on
combining RL with modern function approximators such as kernel functions and
deep neural networks, and indeed there have been many empirical successes that
have exploited such combinations in large-scale applications. There are
profound challenges, however, in developing a theory to support this
enterprise, most notably the need to take into consideration the
exploration-exploitation tradeoff at the core of RL in conjunction with the
computational and statistical tradeoffs that arise in modern
function-approximation-based learning systems. We approach these challenges by
studying an optimistic modification of the least-squares value iteration
algorithm, in the context of the action-value function
  represented by a kernel function or an overparameterized neural network. We
establish both polynomial runtime complexity and polynomial sample complexity
for this algorithm, without additional assumptions on the data-generating
model. In particular, we prove that the algorithm incurs an
$\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where
$\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function
class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total
number of episodes. Our regret bounds are independent of the number of states,
a result which exhibits clearly the benefit of function approximation in RL."
"['stat.ML', 'stat.OT']",Regret Bound Balancing and Elimination for Model Selection in Bandits and RL,"We propose a simple model selection approach for algorithms in stochastic
bandit and reinforcement learning problems. As opposed to prior work that
(implicitly) assumes knowledge of the optimal regret, we only require that each
base algorithm comes with a candidate regret bound that may or may not hold
during all rounds. In each round, our approach plays a base algorithm to keep
the candidate regret bounds of all remaining base algorithms balanced, and
eliminates algorithms that violate their candidate bound. We prove that the
total regret of this approach is bounded by the best valid candidate regret
bound times a multiplicative factor. This factor is reasonably small in several
applications, including linear bandits and MDPs with nested function classes,
linear bandits with unknown misspecification, and LinUCB applied to linear
bandits with different confidence parameters. We further show that, under a
suitable gap-assumption, this factor only scales with the number of base
algorithms and not their complexity when the number of rounds is large enough.
Finally, unlike recent efforts in model selection for linear stochastic
bandits, our approach is versatile enough to also cover cases where the context
information is generated by an adversarial environment, rather than a
stochastic one."
"['stat.ML', 'stat.TH']",An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization,"The evaluation of hyperparameters, neural architectures, or data augmentation
policies becomes a critical model selection problem in advanced deep learning
with a large hyperparameter search space. In this paper, we propose an
efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the
scenario of hyperparameter search evaluation. It evaluates the potential of
hyperparameters by the sub-samples of observations and is theoretically proved
to be optimal under the criterion of cumulative regret. We further combine SS
with Bayesian Optimization and develop a novel hyperparameter optimization
algorithm called BOSS. Empirical studies validate our theoretical arguments of
SS and demonstrate the superior performance of BOSS on a number of
applications, including Neural Architecture Search (NAS), Data Augmentation
(DA), Object Detection (OD), and Reinforcement Learning (RL)."
"['stat.ML', 'stat.TH']",Optimal oracle inequalities for solving projected fixed-point equations,"Linear fixed point equations in Hilbert spaces arise in a variety of
settings, including reinforcement learning, and computational methods for
solving differential and integral equations. We study methods that use a
collection of random observations to compute approximate solutions by searching
over a known low-dimensional subspace of the Hilbert space. First, we prove an
instance-dependent upper bound on the mean-squared error for a linear
stochastic approximation scheme that exploits Polyak--Ruppert averaging. This
bound consists of two terms: an approximation error term with an
instance-dependent approximation factor, and a statistical error term that
captures the instance-specific complexity of the noise when projected onto the
low-dimensional subspace. Using information theoretic methods, we also
establish lower bounds showing that both of these terms cannot be improved,
again in an instance-dependent sense. A concrete consequence of our
characterization is that the optimal approximation factor in this problem can
be much larger than a universal constant. We show how our results precisely
characterize the error of a class of temporal difference learning methods for
the policy evaluation problem with linear function approximation, establishing
their optimality."
"['stat.ML', 'stat.TH']",Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient,"This paper provides a statistical analysis of high-dimensional batch
Reinforcement Learning (RL) using sparse linear function approximation. When
there is a large number of candidate features, our result sheds light on the
fact that sparsity-aware methods can make batch RL more sample efficient. We
first consider the off-policy policy evaluation problem. To evaluate a new
target policy, we analyze a Lasso fitted Q-evaluation method and establish a
finite-sample error bound that has no polynomial dependence on the ambient
dimension. To reduce the Lasso bias, we further propose a post model-selection
estimator that applies fitted Q-evaluation to the features selected via group
Lasso. Under an additional signal strength assumption, we derive a sharper
instance-dependent error bound that depends on a divergence function measuring
the distribution mismatch between the data distribution and occupancy measure
of the target policy. Further, we study the Lasso fitted Q-iteration for batch
policy optimization and establish a finite-sample error bound depending on the
ratio between the number of relevant features and restricted minimal eigenvalue
of the data's covariance. In the end, we complement the results with minimax
lower bounds for batch-data policy evaluation/optimization that nearly match
our upper bounds. The results suggest that having well-conditioned data is
crucial for sparse batch policy learning."
['stat.TH'],Exact Asymptotics for Linear Quadratic Adaptive Control,"Recent progress in reinforcement learning has led to remarkable performance
in a range of applications, but its deployment in high-stakes settings remains
quite rare. One reason is a limited understanding of the behavior of
reinforcement algorithms, both in terms of their regret and their ability to
learn the underlying system dynamics---existing work is focused almost
exclusively on characterizing rates, with little attention paid to the
constants multiplying those rates that can be critically important in practice.
To start to address this challenge, we study perhaps the simplest non-bandit
reinforcement learning problem: linear quadratic adaptive control (LQAC). By
carefully combining recent finite-sample performance bounds for the LQAC
problem with a particular (less-recent) martingale central limit theorem, we
are able to derive asymptotically-exact expressions for the regret, estimation
error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm.
In simulations on both stable and unstable systems, we find that our asymptotic
theory also describes the algorithm's finite-sample behavior remarkably well."
"['stat.ME', 'stat.ML']",Expert-Supervised Reinforcement Learning for Offline Policy Learning and Evaluation,"Offline Reinforcement Learning (RL) is a promising approach for learning
optimal policies in environments where direct exploration is expensive or
unfeasible. However, the adoption of such policies in practice is often
challenging, as they are hard to interpret within the application context, and
lack measures of uncertainty for the learned policy value and its decisions. To
overcome these issues, we propose an Expert-Supervised RL (ESRL) framework
which uses uncertainty quantification for offline policy learning. In
particular, we have three contributions: 1) the method can learn safe and
optimal policies through hypothesis testing, 2) ESRL allows for different
levels of risk averse implementations tailored to the application context, and
finally, 3) we propose a way to interpret ESRL's policy at every state through
posterior distributions, and use this framework to compute off-policy value
function posteriors. We provide theoretical guarantees for our estimators and
regret bounds consistent with Posterior Sampling for RL (PSRL). Sample
efficiency of ESRL is independent of the chosen risk aversion threshold and
quality of the behavior policy."
"['stat.ML', 'stat.TH']",Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective,"In the classical multi-armed bandit problem, instance-dependent algorithms
attain improved performance on ""easy"" problems with a gap between the best and
second-best arm. Are similar guarantees possible for contextual bandits? While
positive results are known for certain special cases, there is no general
theory characterizing when and how instance-dependent regret bounds for
contextual bandits can be achieved for rich, general classes of policies. We
introduce a family of complexity measures that are both sufficient and
necessary to obtain instance-dependent regret bounds. We then introduce new
oracle-efficient algorithms which adapt to the gap whenever possible, while
also attaining the minimax rate in the worst case. Finally, we provide
structural results that tie together a number of complexity measures previously
proposed throughout contextual bandits, reinforcement learning, and active
learning and elucidate their role in determining the optimal instance-dependent
regret. In a large-scale empirical evaluation, we find that our approach often
gives superior results for challenging exploration problems.
  Turning our focus to reinforcement learning with function approximation, we
develop new oracle-efficient algorithms for reinforcement learning with rich
observations that obtain optimal gap-dependent sample complexity."
"['stat.ML', 'stat.TH']",Instance-dependent $\ell_\infty$-bounds for policy evaluation in tabular reinforcement learning,"Markov reward processes (MRPs) are used to model stochastic phenomena arising
in operations research, control engineering, robotics, and artificial
intelligence, as well as communication and transportation networks. In many of
these cases, such as in the policy evaluation problem encountered in
reinforcement learning, the goal is to estimate the long-term value function of
such a process without access to the underlying population transition and
reward functions. Working with samples generated under the synchronous model,
we study the problem of estimating the value function of an infinite-horizon,
discounted MRP on finitely many states in the $\ell_\infty$-norm. We analyze
both the standard plug-in approach to this problem and a more robust variant,
and establish non-asymptotic bounds that depend on the (unknown) problem
instance, as well as data-dependent bounds that can be evaluated based on the
observations of state-transitions and rewards. We show that these approaches
are minimax-optimal up to constant factors over natural sub-classes of MRPs.
Our analysis makes use of a leave-one-out decoupling argument tailored to the
policy evaluation problem, one which may be of independent interest."
"['stat.ML', 'stat.TH']",Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model,"We investigate the sample efficiency of reinforcement learning in a
$\gamma$-discounted infinite-horizon Markov decision process (MDP) with state
space $\mathcal{S}$ and action space $\mathcal{A}$, assuming access to a
generative model. Despite a number of prior work tackling this problem, a
complete picture of the trade-offs between sample complexity and statistical
accuracy is yet to be determined. In particular, prior results suffer from a
sample size barrier, in the sense that their claimed statistical guarantees
hold only when the sample size exceeds at least
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$ (up to some log factor). The
current paper overcomes this barrier by certifying the minimax optimality of
model-based reinforcement learning as soon as the sample size exceeds the order
of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). More
specifically, a perturbed model-based planning algorithm provably finds an
$\varepsilon$-optimal policy with an order of $\frac{|\mathcal{S}||\mathcal{A}|
}{(1-\gamma)^3\varepsilon^2}\log\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\varepsilon}$
samples for any $\varepsilon \in (0, \frac{1}{1-\gamma}]$. Along the way, we
derive improved (instance-dependent) guarantees for model-based policy
evaluation. To the best of our knowledge, this work provides the first
minimax-optimal guarantee in a generative model that accommodates the entire
range of sample sizes (beyond which finding a meaningful policy is information
theoretically impossible)."
"['stat.ML', 'stat.TH']",Adaptive Estimator Selection for Off-Policy Evaluation,"We develop a generic data-driven method for estimator selection in off-policy
policy evaluation settings. We establish a strong performance guarantee for the
method, showing that it is competitive with the oracle estimator, up to a
constant factor. Via in-depth case studies in contextual bandits and
reinforcement learning, we demonstrate the generality and applicability of the
method. We also perform comprehensive experiments, demonstrating the empirical
efficacy of our approach and comparing with related approaches. In both case
studies, our method compares favorably with existing methods."
"['stat.ML', 'stat.TH']",On Linear Stochastic Approximation: Fine-grained Polyak-Ruppert and Non-Asymptotic Concentration,"We undertake a precise study of the asymptotic and non-asymptotic properties
of stochastic approximation procedures with Polyak-Ruppert averaging for
solving a linear system $\bar{A} \theta = \bar{b}$. When the matrix $\bar{A}$
is Hurwitz, we prove a central limit theorem (CLT) for the averaged iterates
with fixed step size and number of iterations going to infinity. The CLT
characterizes the exact asymptotic covariance matrix, which is the sum of the
classical Polyak-Ruppert covariance and a correction term that scales with the
step size. Under assumptions on the tail of the noise distribution, we prove a
non-asymptotic concentration inequality whose main term matches the covariance
in CLT in any direction, up to universal constants. When the matrix $\bar{A}$
is not Hurwitz but only has non-negative real parts in its eigenvalues, we
prove that the averaged LSA procedure actually achieves an $O(1/T)$ rate in
mean-squared error. Our results provide a more refined understanding of linear
stochastic approximation in both the asymptotic and non-asymptotic settings. We
also show various applications of the main results, including the study of
momentum-based stochastic gradient methods as well as temporal difference
algorithms in reinforcement learning."
"['stat.ML', 'stat.TH']",Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation,"This paper studies the statistical theory of batch data reinforcement
learning with function approximation. Consider the off-policy evaluation
problem, which is to estimate the cumulative value of a new target policy from
logged history generated by unknown behavioral policies. We study a
regression-based fitted Q iteration method, and show that it is equivalent to a
model-based method that estimates a conditional mean embedding of the
transition operator. We prove that this method is information-theoretically
optimal and has nearly minimal estimation error. In particular, by leveraging
contraction property of Markov processes and martingale concentration, we
establish a finite-sample instance-dependent error upper bound and a
nearly-matching minimax lower bound. The policy evaluation error depends
sharply on a restricted $\chi^2$-divergence over the function class between the
long-term distribution of the target policy and the distribution of past data.
This restricted $\chi^2$-divergence is both instance-dependent and
function-class-dependent. It characterizes the statistical limit of off-policy
evaluation. Further, we provide an easily computable confidence bound for the
policy evaluator, which may be useful for optimistic planning and safe policy
improvement."
"['stat.ME', 'stat.ML']",More Efficient Off-Policy Evaluation through Regularized Targeted Learning,"We study the problem of off-policy evaluation (OPE) in Reinforcement Learning
(RL), where the aim is to estimate the performance of a new policy given
historical data that may have been generated by a different policy, or
policies. In particular, we introduce a novel doubly-robust estimator for the
OPE problem in RL, based on the Targeted Maximum Likelihood Estimation
principle from the statistical causal inference literature. We also introduce
several variance reduction techniques that lead to impressive performance gains
in off-policy evaluation. We show empirically that our estimator uniformly wins
over existing off-policy evaluation methods across multiple RL environments and
various levels of model misspecification. Finally, we further the existing
theoretical analysis of estimators for the RL off-policy estimation problem by
showing their $O_P(1/\sqrt{n})$ rate of convergence and characterizing their
asymptotic distribution."
['stat.ME'],SURREAL-System: Fully-Integrated Stack for Distributed Deep Reinforcement Learning,"We present an overview of SURREAL-System, a reproducible, flexible, and
scalable framework for distributed reinforcement learning (RL). The framework
consists of a stack of four layers: Provisioner, Orchestrator, Protocol, and
Algorithms. The Provisioner abstracts away the machine hardware and node pools
across different cloud providers. The Orchestrator provides a unified interface
for scheduling and deploying distributed algorithms by high-level description,
which is capable of deploying to a wide range of hardware from a personal
laptop to full-fledged cloud clusters. The Protocol provides network
communication primitives optimized for RL. Finally, the SURREAL algorithms,
such as Proximal Policy Optimization (PPO) and Evolution Strategies (ES), can
easily scale to 1000s of CPU cores and 100s of GPUs. The learning performances
of our distributed algorithms establish new state-of-the-art on OpenAI Gym and
Robotics Suites tasks."
"['stat.ML', 'stat.TH']",Multi-Point Bandit Algorithms for Nonstationary Online Nonconvex Optimization,"Bandit algorithms have been predominantly analyzed in the convex setting with
function-value based stationary regret as the performance measure. In this
paper, motivated by online reinforcement learning problems, we propose and
analyze bandit algorithms for both general and structured nonconvex problems
with nonstationary (or dynamic) regret as the performance measure, in both
stochastic and non-stochastic settings. First, for general nonconvex functions,
we consider nonstationary versions of first-order and second-order stationary
solutions as a regret measure, motivated by similar performance measures for
offline nonconvex optimization. In the case of second-order stationary solution
based regret, we propose and analyze online and bandit versions of the cubic
regularized Newton's method. The bandit version is based on estimating the
Hessian matrices in the bandit setting, based on second-order Gaussian Stein's
identity. Our nonstationary regret bounds in terms of second-order stationary
solutions have interesting consequences for avoiding saddle points in the
bandit setting. Next, for weakly quasi convex functions and monotone weakly
submodular functions we consider nonstationary regret measures in terms of
function-values; such structured classes of nonconvex functions enable one to
consider regret measure defined in terms of function values, similar to convex
functions. For this case of function-value, and first-order stationary solution
based regret measures, we provide regret bounds in both the low- and
high-dimensional settings, for some scenarios."
"['stat.ML', 'stat.CO']",Bayesian bandits: balancing the exploration-exploitation tradeoff via double sampling,"Reinforcement learning studies how to balance exploration and exploitation in
real-world systems, optimizing interactions with the world while simultaneously
learning how the world operates. One general class of algorithms for such
learning is the multi-armed bandit setting. Randomized probability matching,
based upon the Thompson sampling approach introduced in the 1930s, has recently
been shown to perform well and to enjoy provable optimality properties. It
permits generative, interpretable modeling in a Bayesian setting, where prior
knowledge is incorporated, and the computed posteriors naturally capture the
full state of knowledge. In this work, we harness the information contained in
the Bayesian posterior and estimate its sufficient statistics via sampling. In
several application domains, for example in health and medicine, each
interaction with the world can be expensive and invasive, whereas drawing
samples from the model is relatively inexpensive. Exploiting this viewpoint, we
develop a double sampling technique driven by the uncertainty in the learning
process: it favors exploitation when certain about the properties of each arm,
exploring otherwise. The proposed algorithm does not make any distributional
assumption and it is applicable to complex reward distributions, as long as
Bayesian posterior updates are computable. Utilizing the estimated posterior
sufficient statistics, double sampling autonomously balances the
exploration-exploitation tradeoff to make better informed decisions. We
empirically show its reduced cumulative regret when compared to
state-of-the-art alternatives in representative bandit settings."
"['stat.ML', 'stat.CO']",Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"Uncertainty computation in deep learning is essential to design robust and
reliable systems. Variational inference (VI) is a promising approach for such
computation, but requires more effort to implement and execute compared to
maximum-likelihood methods. In this paper, we propose new natural-gradient
algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms
can be implemented within the Adam optimizer by perturbing the network weights
during gradient evaluations, and uncertainty estimates can be cheaply obtained
by using the vector that adapts the learning rate. This requires lower memory,
computation, and implementation effort than existing VI methods, while
obtaining uncertainty estimates of comparable quality. Our empirical results
confirm this and further suggest that the weight-perturbation in our algorithm
could be useful for exploration in reinforcement learning and stochastic
optimization."
"['stat.ML', 'stat.TH']",What Doubling Tricks Can and Can't Do for Multi-Armed Bandits,"An online reinforcement learning algorithm is anytime if it does not need to
know in advance the horizon T of the experiment. A well-known technique to
obtain an anytime algorithm from any non-anytime algorithm is the ""Doubling
Trick"". In the context of adversarial or stochastic multi-armed bandits, the
performance of an algorithm is measured by its regret, and we study two
families of sequences of growing horizons (geometric and exponential) to
generalize previously known results that certain doubling tricks can be used to
conserve certain regret bounds. In a broad setting, we prove that a geometric
doubling trick can be used to conserve (minimax) bounds in $R\_T = O(\sqrt{T})$
but cannot conserve (distribution-dependent) bounds in $R\_T = O(\log T)$. We
give insights as to why exponential doubling tricks may be better, as they
conserve bounds in $R\_T = O(\log T)$, and are close to conserving bounds in
$R\_T = O(\sqrt{T})$."
"['stat.ME', 'stat.ML']",Doubly Robust Off-policy Value Evaluation for Reinforcement Learning,"We study the problem of off-policy value evaluation in reinforcement learning
(RL), where one aims to estimate the value of a new policy based on data
collected by a different policy. This problem is often a critical step when
applying RL in real-world problems. Despite its importance, existing general
methods either have uncontrolled bias or suffer high variance. In this work, we
extend the doubly robust estimator for bandits to sequential decision-making
problems, which gets the best of both worlds: it is guaranteed to be unbiased
and can have a much lower variance than the popular importance sampling
estimators. We demonstrate the estimator's accuracy in several benchmark
problems, and illustrate its use as a subroutine in safe policy improvement. We
also provide theoretical results on the hardness of the problem, and show that
our estimator can match the lower bound in certain scenarios."
"['stat.ML', 'stat.CO']",Bayesian learning of noisy Markov decision processes,"We consider the inverse reinforcement learning problem, that is, the problem
of learning from, and then predicting or mimicking a controller based on
state/action data. We propose a statistical model for such data, derived from
the structure of a Markov decision process. Adopting a Bayesian approach to
inference, we show how latent variables of the model can be estimated, and how
predictions about actions can be made, in a unified framework. A new Markov
chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior
distribution. This step includes a parameter expansion step, which is shown to
be essential for good convergence properties of the MCMC sampler. As an
illustration, the method is applied to learning a human controller."
"['stat.ML', 'stat.TH']",Optimism in Reinforcement Learning and Kullback-Leibler Divergence,"We consider model-based reinforcement learning in finite Markov De- cision
Processes (MDPs), focussing on so-called optimistic strategies. In MDPs,
optimism can be implemented by carrying out extended value it- erations under a
constraint of consistency with the estimated model tran- sition probabilities.
The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this
strategy, has recently been shown to guarantee near-optimal regret bounds. In
this paper, we strongly argue in favor of using the Kullback-Leibler (KL)
divergence for this purpose. By studying the linear maximization problem under
KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for
solving KL-optimistic extended value iteration. Using recent deviation bounds
on the KL divergence, we prove that KL-UCRL provides the same guarantees as
UCRL2 in terms of regret. However, numerical experiments on classical
benchmarks show a significantly improved behavior, particularly when the MDP
has reduced connectivity. To support this observation, we provide elements of
com- parison between the two algorithms based on geometric considerations."
"['stat.CO', 'stat.ML']",A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning,"This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided."
"['stat.ML', 'stat.ME']",Nonparametric posterior learning for emission tomography with multimodal data,"In this work we continue studies of the uncertainty quantification problem in
emission tomographies such as PET or SPECT. In particular, we consider a
scenario when additional multimodal data (e.g., anatomical MRI images) are
available. To solve the aforementioned problem we adapt the recently proposed
nonparametric posterior learning technique to the context of Poisson-type data
in emission tomography. Using this approach we derive sampling algorithms which
are trivially parallelizable, scalable and very easy to implement. In addition,
we prove conditional consistency and tightness for the distribution of produced
samples in the small noise limit (i.e., when the acquisition time tends to
infinity) and derive new geometrical and necessary condition on how MRI images
must be used. This condition arises naturally in the context of misspecified
generalized Poisson models. We also contrast our approach with bayesian MCMC
sampling based on one data augmentation scheme which is very popular in the
context of EM-type algorithms for PET or SPECT. We show theoretically and also
numerically that such data augmentation significantly increases mixing times
for the Markov chain. In view of this, our algorithms seem to give a reasonable
trade-off between design complexity, scalability, numerical load and asessement
for the uncertainty quantification."
"['stat.ML', 'stat.CO']","Ensemble Slice Sampling: Parallel, black-box and gradient-free inference for correlated & multimodal distributions","Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm
that adapts to the characteristics of the target distribution with minimal
hand-tuning. However, Slice Sampling's performance is highly sensitive to the
user-specified initial length scale hyperparameter and the method generally
struggles with poorly scaled or strongly correlated distributions. This paper
introduces Ensemble Slice Sampling (ESS), a new class of algorithms that
bypasses such difficulties by adaptively tuning the initial length scale and
utilising an ensemble of parallel walkers in order to efficiently handle strong
correlations between parameters. These affine-invariant algorithms are trivial
to construct, require no hand-tuning, and can easily be implemented in parallel
computing environments. Empirical tests show that Ensemble Slice Sampling can
improve efficiency by more than an order of magnitude compared to conventional
MCMC methods on a broad range of highly correlated target distributions. In
cases of strongly multimodal target distributions, Ensemble Slice Sampling can
sample efficiently even in high dimensions. We argue that the parallel,
black-box and gradient-free nature of the method renders it ideal for use in
scientific fields such as physics, astrophysics and cosmology which are
dominated by a wide variety of computationally expensive and non-differentiable
models."
['stat.ME'],Densely connected neural networks for nonlinear regression,"Densely connected convolutional networks (DenseNet) behave well in image
processing. However, for regression tasks, convolutional DenseNet may lose
essential information from independent input features. To tackle this issue, we
propose a novel DenseNet regression model where convolution and pooling layers
are replaced by fully connected layers and the original concatenation shortcuts
are maintained to reuse the feature. To investigate the effects of depth and
input dimension of proposed model, careful validations are performed by
extensive numerical simulation. The results give an optimal depth (19) and
recommend a limited input dimension (under 200). Furthermore, compared with the
baseline models including support vector regression, decision tree regression,
and residual regression, our proposed model with the optimal depth performs
best. Ultimately, DenseNet regression is applied to predict relative humidity,
and the outcome shows a high correlation (0.91) with observations, which
indicates that our model could advance environmental data analysis."
"['stat.ML', 'stat.ME']","Receiver operating characteristic (ROC) movies, universal ROC (UROC) curves, and coefficient of predictive ability (CPA)","Throughout science and technology, receiver operating characteristic (ROC)
curves and associated area under the curve (AUC) measures constitute powerful
tools for assessing the predictive abilities of features, markers and tests in
binary classification problems. Despite its immense popularity, ROC analysis
has been subject to a fundamental restriction, in that it applies to
dichotomous (yes or no) outcomes only. Here we introduce ROC movies and
universal ROC (UROC) curves that apply to just any linearly ordered outcome,
along with an associated coefficient of predictive ability (CPA) measure. CPA
equals the area under the UROC curve, and admits appealing interpretations in
terms of probabilities and rank based covariances. For binary outcomes CPA
equals AUC, and for pairwise distinct outcomes CPA relates linearly to
Spearman's coefficient, in the same way that the C index relates linearly to
Kendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the
tools of classical ROC analysis, and are bound to supersede them in a wealth of
applications. Their usage is illustrated in data examples from biomedicine and
meteorology, where rank based measures yield new insights in the WeatherBench
comparison of the predictive performance of convolutional neural networks and
physical-numerical models for weather prediction."
"['stat.ME', 'stat.ML']","Distribution-Free, Risk-Controlling Prediction Sets","While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning."
"['stat.ML', 'stat.TH']",Computing Valid p-values for Image Segmentation by Selective Inference,"Image segmentation is one of the most fundamental tasks of computer vision.
In many practical applications, it is essential to properly evaluate the
reliability of individual segmentation results. In this study, we propose a
novel framework to provide the statistical significance of segmentation results
in the form of p-values. Specifically, we consider a statistical hypothesis
test for determining the difference between the object and the background
regions. This problem is challenging because the difference can be deceptively
large (called segmentation bias) due to the adaptation of the segmentation
algorithm to the data. To overcome this difficulty, we introduce a statistical
approach called selective inference, and develop a framework to compute valid
p-values in which the segmentation bias is properly accounted for. Although the
proposed framework is potentially applicable to various segmentation
algorithms, we focus in this paper on graph cut-based and threshold-based
segmentation algorithms, and develop two specific methods to compute valid
p-values for the segmentation results obtained by these algorithms. We prove
the theoretical validity of these two methods and demonstrate their
practicality by applying them to segmentation problems for medical images."
['stat.CO'],A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using Adaptive Plant Propagation Algorithm,"One of the most straightforward, direct and efficient approaches to Image
Segmentation is Image Thresholding. Multi-level Image Thresholding is an
essential viewpoint in many image processing and Pattern Recognition based
real-time applications which can effectively and efficiently classify the
pixels into various groups denoting multiple regions in an Image. Thresholding
based Image Segmentation using fuzzy entropy combined with intelligent
optimization approaches are commonly used direct methods to properly identify
the thresholds so that they can be used to segment an Image accurately. In this
paper a novel approach for multi-level image thresholding is proposed using
Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).
Obtaining the optimal thresholds for an image by maximizing the entropy is
extremely tedious and time consuming with increase in the number of thresholds.
Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based
on plant intelligence, is used for fast and efficient selection of optimal
thresholds. This fact is reasonably justified by comparing the accuracy of the
outcomes and computational time consumed by other modern state-of-the-art
algorithms such as Particle Swarm Optimization (PSO), Gravitational Search
Algorithm (GSA) and Genetic Algorithm (GA)."
"['stat.ML', 'stat.ME']",Improving Gibbs Sampler Scan Quality with DoGS,"The pairwise influence matrix of Dobrushin has long been used as an
analytical tool to bound the rate of convergence of Gibbs sampling. In this
work, we use Dobrushin influence as the basis of a practical tool to certify
and efficiently improve the quality of a discrete Gibbs sampler. Our
Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection
orders for a given sampling budget and variable subset of interest, explicit
bounds on total variation distance to stationarity, and certifiable
improvements over the standard systematic and uniform random scan Gibbs
samplers. In our experiments with joint image segmentation and object
recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising
model inference, DoGS consistently deliver higher-quality inferences with
significantly smaller sampling budgets than standard Gibbs samplers."
"['stat.ML', 'stat.ME']",A deep-structured fully-connected random field model for structured inference,"There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation."
['stat.CO'],Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means,"A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm
is proposed with iterative reweighted least squares (IRLS) algorithm, and
penalty constraint and kernelization extensions of MFCM algorithms are also
developed. Introducing penalty information to the object functions of MFCM
algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to
penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean
distance with kernel method, the MFCM and pMFCM algorithms are extended to
kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.
The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in
three tasks: pattern recognition on 10 standard data sets from UCI Machine
Learning databases, noise image segmentation performances on a synthetic image,
a magnetic resonance brain image (MRI), and image segmentation of a standard
images from Berkeley Segmentation Dataset and Benchmark. The experimental
results demonstrate the effectiveness of our proposed algorithms in pattern
recognition and image segmentation."
"['stat.ML', 'stat.TH']",Plugin procedure in segmentation and application to hyperspectral image segmentation,"In this article we give our contribution to the problem of segmentation with
plug-in procedures. We give general sufficient conditions under which plug in
procedure are efficient. We also give an algorithm that satisfy these
conditions. We give an application of the used algorithm to hyperspectral
images segmentation. Hyperspectral images are images that have both spatial and
spectral coherence with thousands of spectral bands on each pixel. In the
proposed procedure we combine a reduction dimension technique and a spatial
regularisation technique. This regularisation is based on the mixlet
modelisation of Kolaczyck and Al."
"['stat.ML', 'stat.CO']",A geometric perspective on functional outlier detection,"We consider functional outlier detection from a geometric perspective,
specifically: for functional data sets drawn from a functional manifold which
is defined by the data's modes of variation in amplitude and phase. Based on
this manifold, we develop a conceptualization of functional outlier detection
that is more widely applicable and realistic than previously proposed. Our
theoretical and experimental analyses demonstrate several important advantages
of this perspective: It considerably improves theoretical understanding and
allows to describe and analyse complex functional outlier scenarios
consistently and in full generality, by differentiating between structurally
anomalous outlier data that are off-manifold and distributionally outlying data
that are on-manifold but at its margins. This improves practical feasibility of
functional outlier detection: We show that simple manifold learning methods can
be used to reliably infer and visualize the geometric structure of functional
data sets. We also show that standard outlier detection methods requiring
tabular data inputs can be applied to functional data very successfully by
simply using their vector-valued representations learned from manifold learning
methods as input features. Our experiments on synthetic and real data sets
demonstrate that this approach leads to outlier detection performances at least
on par with existing functional data-specific methods in a large variety of
settings, without the highly specialized, complex methodology and narrow domain
of application these methods often entail."
"['stat.ML', 'stat.ME']",Desiderata for Representation Learning: A Causal Perspective,"Representation learning constructs low-dimensional representations to
summarize essential features of high-dimensional data. This learning problem is
often approached by describing various desiderata associated with learned
representations; e.g., that they be non-spurious, efficient, or disentangled.
It can be challenging, however, to turn these intuitive desiderata into formal
criteria that can be measured and enhanced based on observed data. In this
paper, we take a causal perspective on representation learning, formalizing
non-spuriousness and efficiency (in supervised representation learning) and
disentanglement (in unsupervised representation learning) using counterfactual
quantities and observable consequences of causal assertions. This yields
computable metrics that can be used to assess the degree to which
representations satisfy the desiderata of interest and learn non-spurious and
disentangled representations from single observational datasets."
"['stat.ML', 'stat.ME']",Interpretable Network Representation Learning with Principal Component Analysis,"We consider the problem of interpretable network representation learning for
samples of network-valued data. We propose the Principal Component Analysis for
Networks (PCAN) algorithm to identify statistically meaningful low-dimensional
representations of a network sample via subgraph count statistics. The PCAN
procedure provides an interpretable framework for which one can readily
visualize, explore, and formulate predictive models for network samples. We
furthermore introduce a fast sampling-based algorithm, sPCAN, which is
significantly more computationally efficient than its counterpart, but still
enjoys advantages of interpretability. We investigate the relationship between
these two methods and analyze their large-sample properties under the common
regime where the sample of networks is a collection of kernel-based random
graphs. We show that under this regime, the embeddings of the sPCAN method
enjoy a central limit theorem and moreover that the population level embeddings
of PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,
cluster, and classify observations in network samples arising in nature,
including functional connectivity network samples and dynamic networks
describing the political co-voting habits of the U.S. Senate. Our analyses
reveal that our proposed algorithm provides informative and discriminatory
features describing the networks in each sample. The PCAN and sPCAN methods
build on the current literature of network representation learning and set the
stage for a new line of research in interpretable learning on network-valued
data. Publicly available software for the PCAN and sPCAN methods are available
at https://www.github.com/jihuilee/."
"['stat.ML', 'stat.ME']",Matching in Selective and Balanced Representation Space for Treatment Effects Estimation,"The dramatically growing availability of observational data is being
witnessed in various domains of science and technology, which facilitates the
study of causal inference. However, estimating treatment effects from
observational data is faced with two major challenges, missing counterfactual
outcomes and treatment selection bias. Matching methods are among the most
widely used and fundamental approaches to estimating treatment effects, but
existing matching methods have poor performance when facing data with high
dimensional and complicated variables. We propose a feature selection
representation matching (FSRM) method based on deep representation learning and
matching, which maps the original covariate space into a selective, nonlinear,
and balanced representation space, and then conducts matching in the learned
representation space. FSRM adopts deep feature selection to minimize the
influence of irrelevant variables for estimating treatment effects and
incorporates a regularizer based on the Wasserstein distance to learn balanced
representations. We evaluate the performance of our FSRM method on three
datasets, and the results demonstrate superiority over the state-of-the-art
methods."
"['stat.ME', 'stat.ML']",NCoRE: Neural Counterfactual Representation Learning for Combinations of Treatments,"Estimating an individual's potential response to interventions from
observational data is of high practical relevance for many domains, such as
healthcare, public policy or economics. In this setting, it is often the case
that combinations of interventions may be applied simultaneously, for example,
multiple prescriptions in healthcare or different fiscal and monetary measures
in economics. However, existing methods for counterfactual inference are
limited to settings in which actions are not used simultaneously. Here, we
present Neural Counterfactual Relation Estimation (NCoRE), a new method for
learning counterfactual representations in the combination treatment setting
that explicitly models cross-treatment interactions. NCoRE is based on a novel
branched conditional neural representation that includes learnt treatment
interaction modulators to infer the potential causal generative process
underlying the combination of multiple treatments. Our experiments show that
NCoRE significantly outperforms existing state-of-the-art methods for
counterfactual treatment effect estimation that do not account for the effects
of combining multiple treatments across several synthetic, semi-synthetic and
real-world benchmarks."
"['stat.ME', 'stat.ML']",Neural Methods for Point-wise Dependency Estimation,"Since its inception, the neural estimation of mutual information (MI) has
demonstrated the empirical success of modeling expected dependency between
high-dimensional random variables. However, MI is an aggregate statistic and
cannot be used to measure point-wise dependency between different events. In
this work, instead of estimating the expected dependency, we focus on
estimating point-wise dependency (PD), which quantitatively measures how likely
two outcomes co-occur. We show that we can naturally obtain PD when we are
optimizing MI neural variational bounds. However, optimizing these bounds is
challenging due to its large variance in practice. To address this issue, we
develop two methods (free of optimizing MI variational bounds): Probabilistic
Classifier and Density-Ratio Fitting. We demonstrate the effectiveness of our
approaches in 1) MI estimation, 2) self-supervised representation learning, and
3) cross-modal retrieval task."
"['stat.ML', 'stat.TH']",PAC-Bayesian Contrastive Unsupervised Representation Learning,"Contrastive unsupervised representation learning (CURL) is the
state-of-the-art technique to learn representations (as a set of features) from
unlabelled data. While CURL has collected several empirical successes recently,
theoretical understanding of its performance was still missing. In a recent
work, Arora et al. (2019) provide the first generalisation bounds for CURL,
relying on a Rademacher complexity. We extend their framework to the flexible
PAC-Bayes setting, allowing us to deal with the non-iid setting. We present
PAC-Bayesian generalisation bounds for CURL, which are then used to derive a
new representation learning algorithm. Numerical experiments on real-life
datasets illustrate that our algorithm achieves competitive accuracy, and
yields non-vacuous generalisation bounds."
"['stat.ME', 'stat.ML']",Quantifying Error in the Presence of Confounders for Causal Inference,"Estimating average causal effect (ACE) is useful whenever we want to know the
effect of an intervention on a given outcome. In the absence of a randomized
experiment, many methods such as stratification and inverse propensity
weighting have been proposed to estimate ACE. However, it is hard to know which
method is optimal for a given dataset or which hyperparameters to use for a
chosen method. To this end, we provide a framework to characterize the loss of
a causal inference method against the true ACE, by framing causal inference as
a representation learning problem. We show that many popular methods, including
back-door methods can be considered as weighting or representation learning
algorithms, and provide general error bounds for their causal estimates. In
addition, we consider the case when unobserved variables can confound the
causal estimate and extend proposed bounds using principles of robust
statistics, considering confounding as contamination under the Huber
contamination model. These bounds are also estimable; as an example, we provide
empirical bounds for the Inverse Propensity Weighting (IPW) estimator and show
how the bounds can be used to optimize the threshold of clipping extreme
propensity scores. Our work provides a new way to reason about competing
estimators, and opens up the potential of deriving new methods by minimizing
the proposed error bounds."
"['stat.ML', 'stat.ME']",Co-manifold learning with missing data,"Representation learning is typically applied to only one mode of a data
matrix, either its rows or columns. Yet in many applications, there is an
underlying geometry to both the rows and the columns. We propose utilizing this
coupled structure to perform co-manifold learning: uncovering the underlying
geometry of both the rows and the columns of a given matrix, where we focus on
a missing data setting. Our unsupervised approach consists of three components.
We first solve a family of optimization problems to estimate a complete matrix
at multiple scales of smoothness. We then use this collection of smooth matrix
estimates to compute pairwise distances on the rows and columns based on a new
multi-scale metric that implicitly introduces a coupling between the rows and
the columns. Finally, we construct row and column representations from these
multi-scale metrics. We demonstrate that our approach outperforms competing
methods in both data visualization and clustering."
"['stat.ML', 'stat.CO']",Information Dropout: Learning Optimal Representations Through Noisy Computation,"The cross-entropy loss commonly used in deep learning is closely related to
the defining properties of optimal representations, but does not enforce some
of the key properties. We show that this can be solved by adding a
regularization term, which is in turn related to injecting multiplicative noise
in the activations of a Deep Neural Network, a special case of which is the
common practice of dropout. We show that our regularized loss function can be
efficiently minimized using Information Dropout, a generalization of dropout
rooted in information theoretic principles that automatically adapts to the
data and can better exploit architectures of limited capacity. When the task is
the reconstruction of the input, we show that our loss function yields a
Variational Autoencoder as a special case, thus providing a link between
representation learning, information theory and variational inference. Finally,
we prove that we can promote the creation of disentangled representations
simply by enforcing a factorized prior, a fact that has been observed
empirically in recent work. Our experiments validate the theoretical intuitions
behind our method, and we find that information dropout achieves a comparable
or better generalization performance than binary dropout, especially on smaller
models, since it can automatically adapt the noise to the structure of the
network, as well as to the test sample."
"['stat.ML', 'stat.ME']",Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural Networks,"Several popular graph embedding techniques for representation learning and
dimensionality reduction rely on performing computationally expensive
eigendecompositions to derive a nonlinear transformation of the input data
space. The resulting eigenvectors encode the embedding coordinates for the
training samples only, and so the embedding of novel data samples requires
further costly computation. In this paper, we present a method for the
out-of-sample extension of graph embeddings using deep neural networks (DNN) to
parametrically approximate these nonlinear maps. Compared with traditional
nonparametric out-of-sample extension methods, we demonstrate that the DNNs can
generalize with equal or better fidelity and require orders of magnitude less
computation at test time. Moreover, we find that unsupervised pretraining of
the DNNs improves optimization for larger network sizes, thus removing
sensitivity to model selection."
"['stat.ML', 'stat.CO']",The Variational Gaussian Process,"Variational inference is a powerful tool for approximate inference, and it
has been recently applied for representation learning with deep generative
models. We develop the variational Gaussian process (VGP), a Bayesian
nonparametric variational family, which adapts its shape to match complex
posterior distributions. The VGP generates approximate posterior samples by
generating latent inputs and warping them through random non-linear mappings;
the distribution over random mappings is learned during inference, enabling the
transformed outputs to adapt to varying complexity. We prove a universal
approximation theorem for the VGP, demonstrating its representative power for
learning any model. For inference we present a variational objective inspired
by auto-encoders and perform black box inference over a wide class of models.
The VGP achieves new state-of-the-art results for unsupervised learning,
inferring models such as the deep latent Gaussian model and the recently
proposed DRAW."
"['stat.ML', 'stat.TH']",An Asymptotically Optimal Multi-Armed Bandit Algorithm and Hyperparameter Optimization,"The evaluation of hyperparameters, neural architectures, or data augmentation
policies becomes a critical model selection problem in advanced deep learning
with a large hyperparameter search space. In this paper, we propose an
efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the
scenario of hyperparameter search evaluation. It evaluates the potential of
hyperparameters by the sub-samples of observations and is theoretically proved
to be optimal under the criterion of cumulative regret. We further combine SS
with Bayesian Optimization and develop a novel hyperparameter optimization
algorithm called BOSS. Empirical studies validate our theoretical arguments of
SS and demonstrate the superior performance of BOSS on a number of
applications, including Neural Architecture Search (NAS), Data Augmentation
(DA), Object Detection (OD), and Reinforcement Learning (RL)."
"['stat.ML', 'stat.TH']",Estimation of a function of low local dimensionality by deep neural networks,"Deep neural networks (DNNs) achieve impressive results for complicated tasks
like object detection on images and speech recognition. Motivated by this
practical success, there is now a strong interest in showing good theoretical
properties of DNNs. To describe for which tasks DNNs perform well and when they
fail, it is a key challenge to understand their performance. The aim of this
paper is to contribute to the current statistical theory of DNNs. We apply DNNs
on high dimensional data and we show that the least squares regression
estimates using DNNs are able to achieve dimensionality reduction in case that
the regression function has locally low dimensionality. Consequently, the rate
of convergence of the estimate does not depend on its input dimension $d$, but
on its local dimension $d^*$ and the DNNs are able to circumvent the curse of
dimensionality in case that $d^*$ is much smaller than $d$. In our simulation
study we provide numerical experiments to support our theoretical result and we
compare our estimate with other conventional nonparametric regression
estimates. The performance of our estimates is also validated in experiments
with real data."
"['stat.ML', 'stat.TH']",Robust Generalization of Quadratic Neural Networks via Function Identification,"A key challenge facing deep learning is that neural networks are often not
robust to shifts in the underlying data distribution. We study this problem
from the perspective of the statistical concept of parameter identification.
Generalization bounds from learning theory often assume that the test
distribution is close to the training distribution. In contrast, if we can
identify the ""true"" parameters, then the model generalizes to arbitrary
distribution shifts. However, neural networks are typically overparameterized,
making parameter identification impossible. We show that for quadratic neural
networks, we can identify the function represented by the model even though we
cannot identify its parameters. Thus, we can obtain robust generalization
bounds even in the overparameterized setting. We leverage this result to obtain
new bounds for contextual bandits and transfer learning with quadratic neural
networks. Overall, our results suggest that we can improve robustness of neural
networks by designing models that can represent the true data generating
process. In practice, the true data generating process is often very complex;
thus, we study how our framework might connect to neural module networks, which
are designed to break down complex tasks into compositions of simpler ones. We
prove robust generalization bounds when individual neural modules are
identifiable."
['stat.ME'],Challenges for cognitive decoding using deep learning methods,"In cognitive decoding, researchers aim to characterize a brain region's
representations by identifying the cognitive states (e.g., accepting/rejecting
a gamble) that can be identified from the region's activity. Deep learning (DL)
methods are highly promising for cognitive decoding, with their unmatched
ability to learn versatile representations of complex data. Yet, their
widespread application in cognitive decoding is hindered by their general lack
of interpretability as well as difficulties in applying them to small datasets
and in ensuring their reproducibility and robustness. We propose to approach
these challenges by leveraging recent advances in explainable artificial
intelligence and transfer learning, while also providing specific
recommendations on how to improve the reproducibility and robustness of DL
modeling results."
"['stat.ML', 'stat.ME', 'stat.TH']",Adaptive transfer learning,"In transfer learning, we wish to make inference about a target population
when we have access to data both from the distribution itself, and from a
different but related source distribution. We introduce a flexible framework
for transfer learning in the context of binary classification, allowing for
covariate-dependent relationships between the source and target distributions
that are not required to preserve the Bayes decision boundary. Our main
contributions are to derive the minimax optimal rates of convergence (up to
poly-logarithmic factors) in this problem, and show that the optimal rate can
be achieved by an algorithm that adapts to key aspects of the unknown transfer
relationship, as well as the smoothness and tail parameters of our
distributional classes. This optimal rate turns out to have several regimes,
depending on the interplay between the relative sample sizes and the strength
of the transfer relationship, and our algorithm achieves optimality by careful,
decision tree-based calibration of local nearest-neighbour procedures."
"['stat.ML', 'stat.ME']",Transfer Learning under High-dimensional Generalized Linear Models,"In this work, we study the transfer learning problem under high-dimensional
generalized linear models (GLMs), which aim to improve the fit on target data
by borrowing information from useful source data. Given which sources to
transfer, we propose an oracle algorithm and derive its $\ell_2$-estimation
error bounds. The theoretical analysis shows that under certain conditions,
when the target and source are sufficiently close to each other, the estimation
error bound could be improved over that of the classical penalized estimator
using only target data. When we don't know which sources to transfer, an
algorithm-free transferable source detection approach is introduced to detect
informative sources. The detection consistency is proved under the
high-dimensional GLM transfer learning setting. Extensive simulations and a
real-data experiment verify the effectiveness of our algorithms."
"['stat.ML', 'stat.ME']",Two-sample Testing Using Deep Learning,"We propose a two-sample testing procedure based on learned deep neural
network representations. To this end, we define two test statistics that
perform an asymptotic location test on data samples mapped onto a hidden layer.
The tests are consistent and asymptotically control the type-1 error rate.
Their test statistics can be evaluated in linear time (in the sample size).
Suitable data representations are obtained in a data-driven way, by solving a
supervised or unsupervised transfer-learning task on an auxiliary (potentially
distinct) data set. If no auxiliary data is available, we split the data into
two chunks: one for learning representations and one for computing the test
statistic. In experiments on audio samples, natural images and
three-dimensional neuroimaging data our tests yield significant decreases in
type-2 error rate (up to 35 percentage points) compared to state-of-the-art
two-sample tests such as kernel-methods and classifier two-sample tests."
"['stat.ML', 'stat.CO']",On transfer learning of neural networks using bi-fidelity data for uncertainty propagation,"Due to their high degree of expressiveness, neural networks have recently
been used as surrogate models for mapping inputs of an engineering system to
outputs of interest. Once trained, neural networks are computationally
inexpensive to evaluate and remove the need for repeated evaluations of
computationally expensive models in uncertainty quantification applications.
However, given the highly parameterized construction of neural networks,
especially deep neural networks, accurate training often requires large amounts
of simulation data that may not be available in the case of computationally
expensive systems. In this paper, to alleviate this issue for uncertainty
propagation, we explore the application of transfer learning techniques using
training data generated from both high- and low-fidelity models. We explore two
strategies for coupling these two datasets during the training procedure,
namely, the standard transfer learning and the bi-fidelity weighted learning.
In the former approach, a neural network model mapping the inputs to the
outputs of interest is trained based on the low-fidelity data. The
high-fidelity data is then used to adapt the parameters of the upper layer(s)
of the low-fidelity network, or train a simpler neural network to map the
output of the low-fidelity network to that of the high-fidelity model. In the
latter approach, the entire low-fidelity network parameters are updated using
data generated via a Gaussian process model trained with a small high-fidelity
dataset. The parameter updates are performed via a variant of stochastic
gradient descent with learning rates given by the Gaussian process model. Using
three numerical examples, we illustrate the utility of these bi-fidelity
transfer learning methods where we focus on accuracy improvement achieved by
transfer learning over standard training approaches."
"['stat.ML', 'stat.ME', 'stat.TH']",Relative Density-Ratio Estimation for Robust Distribution Comparison,"Divergence estimators based on direct approximation of density-ratios without
going through separate approximation of numerator and denominator densities
have been successfully applied to machine learning tasks that involve
distribution comparison such as outlier detection, transfer learning, and
two-sample homogeneity test. However, since density-ratio functions often
possess high fluctuation, divergence estimation is still a challenging task in
practice. In this paper, we propose to use relative divergences for
distribution comparison, which involves approximation of relative
density-ratios. Since relative density-ratios are always smoother than
corresponding ordinary density-ratios, our proposed method is favorable in
terms of the non-parametric convergence speed. Furthermore, we show that the
proposed divergence estimator has asymptotic variance independent of the model
complexity under a parametric setup, implying that the proposed estimator
hardly overfits even with complex models. Through experiments, we demonstrate
the usefulness of the proposed approach."
['stat.ME'],Using Persistent Homology Topological Features to Characterize Medical Images: Case Studies on Lung and Brain Cancers,"Tumor shape is a key factor that affects tumor growth and metastasis. This
paper proposes a topological feature computed by persistent homology to
characterize tumor progression from digital pathology and radiology images and
examines its effect on the time-to-event data. The proposed topological
features are invariant to scale-preserving transformation and can summarize
various tumor shape patterns. The topological features are represented in
functional space and used as functional predictors in a functional Cox
proportional hazards model. The proposed model enables interpretable inference
about the association between topological shape features and survival risks.
Two case studies are conducted using consecutive 143 lung cancer and 77 brain
tumor patients. The results of both studies show that the topological features
predict survival prognosis after adjusting clinical variables, and the
predicted high-risk groups have significantly (at the level of 0.001) worse
survival outcomes than the low-risk groups. Also, the topological shape
features found to be positively associated with survival hazards are irregular
and heterogeneous shape patterns, which are known to be related to tumor
progression."
"['stat.ME', 'stat.ML']",Policy Optimization Using Semiparametric Models for Dynamic Pricing,"In this paper, we study the contextual dynamic pricing problem where the
market value of a product is linear in its observed features plus some market
noise. Products are sold one at a time, and only a binary response indicating
success or failure of a sale is observed. Our model setting is similar to
Javanmard and Nazerzadeh [2019] except that we expand the demand curve to a
semiparametric model and need to learn dynamically both parametric and
nonparametric components. We propose a dynamic statistical learning and
decision-making policy that combines semiparametric estimation from a
generalized linear model with an unknown link and online decision-making to
minimize regret (maximize revenue). Under mild conditions, we show that for a
market noise c.d.f. $F(\cdot)$ with $m$-th order derivative ($m\geq 2$), our
policy achieves a regret upper bound of $\tilde{O}_{d}(T^{\frac{2m+1}{4m-1}})$,
where $T$ is time horizon and $\tilde{O}_{d}$ is the order that hides
logarithmic terms and the dimensionality of feature $d$. The upper bound is
further reduced to $\tilde{O}_{d}(\sqrt{T})$ if $F$ is super smooth whose
Fourier transform decays exponentially. In terms of dependence on the horizon
$T$, these upper bounds are close to $\Omega(\sqrt{T})$, the lower bound where
$F$ belongs to a parametric class. We further generalize these results to the
case with dynamically dependent product features under the strong mixing
condition."
"['stat.ML', 'stat.ME']",Deep State-Space Gaussian Processes,"This paper is concerned with a state-space approach to deep Gaussian process
(DGP) regression. We construct the DGP by hierarchically putting transformed
Gaussian process (GP) priors on the length scales and magnitudes of the next
level of Gaussian processes in the hierarchy. The idea of the state-space
approach is to represent the DGP as a non-linear hierarchical system of linear
stochastic differential equations (SDEs), where each SDE corresponds to a
conditional GP. The DGP regression problem then becomes a state estimation
problem, and we can estimate the state efficiently with sequential methods by
using the Markov property of the state-space DGP. The computational complexity
scales linearly with respect to the number of measurements. Based on this, we
formulate state-space MAP as well as Bayesian filtering and smoothing solutions
to the DGP regression problem. We demonstrate the performance of the proposed
models and methods on synthetic non-stationary signals and apply the
state-space DGP to detection of the gravitational waves from LIGO measurements."
"['stat.ML', 'stat.ME']",Dirac Delta Regression: Conditional Density Estimation with Clinical Trials,"Personalized medicine seeks to identify the causal effect of treatment for a
particular patient as opposed to a clinical population at large. Most
investigators estimate such personalized treatment effects by regressing the
outcome of a randomized clinical trial (RCT) on patient covariates. The
realized value of the outcome may however lie far from the conditional
expectation. We therefore introduce a method called Dirac Delta Regression
(DDR) that estimates the entire conditional density from RCT data in order to
visualize the probabilities across all possible outcome values. DDR transforms
the outcome into a set of asymptotically Dirac delta distributions and then
estimates the density using non-linear regression. The algorithm can identify
significant differences in patient-specific outcomes even when no population
level effect exists. Moreover, DDR outperforms state-of-the-art algorithms in
conditional density estimation by a large margin even in the small sample
regime. An R package is available at https://github.com/ericstrobl/DDR."
"['stat.ML', 'stat.CO']",Deep composition of tensor-trains using squared inverse Rosenblatt transports,"Characterising intractable high-dimensional random variables is one of the
fundamental challenges in stochastic computation. The recent surge of transport
maps offers a mathematical foundation and new insights for tackling this
challenge by coupling intractable random variables with tractable reference
random variables. This paper generalises the functional tensor-train
approximation of the inverse Rosenblatt transport recently developed by Dolgov
et al. (Stat Comput 30:603--625, 2020) to a wide class of high-dimensional
non-negative functions, such as unnormalised probability density functions.
First, we extend the inverse Rosenblatt transform to enable the transport to
general reference measures other than the uniform measure. We develop an
efficient procedure to compute this transport from a squared tensor-train
decomposition which preserves the monotonicity. More crucially, we integrate
the proposed order-preserving functional tensor-train transport into a nested
variable transformation framework inspired by the layered structure of deep
neural networks. The resulting deep inverse Rosenblatt transport significantly
expands the capability of tensor approximations and transport maps to random
variables with complicated nonlinear interactions and concentrated density
functions. We demonstrate the efficiency of the proposed approach on a range of
applications in statistical learning and uncertainty quantification, including
parameter estimation for dynamical systems and inverse problems constrained by
partial differential equations."
"['stat.ML', 'stat.CO']",Quantile-Quantile Embedding for Distribution Transformation and Manifold Embedding with Ability to Choose the Embedding Distribution,"We propose a new embedding method, named Quantile-Quantile Embedding (QQE),
for distribution transformation and manifold embedding with the ability to
choose the embedding distribution. QQE, which uses the concept of
quantile-quantile plot from visual statistical tests, can transform the
distribution of data to any theoretical desired distribution or empirical
reference sample. Moreover, QQE gives the user a choice of embedding
distribution in embedding the manifold of data into the low dimensional
embedding space. It can also be used for modifying the embedding distribution
of other dimensionality reduction methods, such as PCA, t-SNE, and deep metric
learning, for better representation or visualization of data. We propose QQE in
both unsupervised and supervised forms. QQE can also transform a distribution
to either an exact reference distribution or its shape. We show that QQE allows
for better discrimination of classes in some cases. Our experiments on
different synthetic and image datasets show the effectiveness of the proposed
embedding method."
"['stat.ML', 'stat.CO']",Deep Gaussian Process Emulation using Stochastic Imputation,"We propose a novel deep Gaussian process (DGP) inference method for computer
model emulation using stochastic imputation. By stochastically imputing the
latent layers, the approach transforms the DGP into the linked GP, a
state-of-the-art surrogate model formed by linking a system of feed-forward
coupled GPs. This transformation renders a simple while efficient DGP training
procedure that only involves optimizations of conventional stationary GPs. In
addition, the analytically tractable mean and variance of the linked GP allows
one to implement predictions from DGP emulators in a fast and accurate manner.
We demonstrate the method in a series of synthetic examples and real-world
applications, and show that it is a competitive candidate for efficient DGP
surrogate modeling in comparison to the variational inference and the
fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$
implementing the method is also produced and available at
https://github.com/mingdeyu/DGP."
"['stat.ME', 'stat.ML']",Challenges and Opportunities in High-dimensional Variational Inference,"Current black-box variational inference (BBVI) methods require the user to
make numerous design choices -- such as the selection of variational objective
and approximating family -- yet there is little principled guidance on how to
do so. We develop a conceptual framework and set of experimental tools to
understand the effects of these choices, which we leverage to propose best
practices for maximizing posterior approximation accuracy. Our approach is
based on studying the pre-asymptotic tail behavior of the density ratios
between the joint distribution and the variational approximation, then
exploiting insights and tools from the importance sampling literature. Our
framework and supporting experiments help to distinguish between the behavior
of BBVI methods for approximating low-dimensional versus
moderate-to-high-dimensional posteriors. In the latter case, we show that
mass-covering variational objectives are difficult to optimize and do not
improve accuracy, but flexible variational families can improve accuracy and
the effectiveness of importance sampling -- at the cost of additional
optimization challenges. Therefore, for moderate-to-high-dimensional posteriors
we recommend using the (mode-seeking) exclusive KL divergence since it is the
easiest to optimize, and improving the variational family or using model
parameter transformations to make the posterior and optimal variational
approximation more similar. On the other hand, in low-dimensional settings, we
show that heavy-tailed variational families and mass-covering divergences are
effective and can increase the chances that the approximation can be improved
by importance sampling."
"['stat.ML', 'stat.ME', 'stat.TH']",BELT: Block-wise Missing Embedding Learning Transformer,"Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of multiple
Electronic Health Record (EHR) datasets, we propose the method {\bf B}lock-wise
missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT) to treat
row-wise/column-wise missingness. Specifically, BELT can recover block-wise
missing matrices efficiently when every pair of matrices has an overlap. Our
idea is to exploit the orthogonal Procrustes problem to align the eigenspace of
the two sub-matrices using their overlap, then complete the missing blocks by
the inner product of the two low-rank components. Besides, we prove the
statistical rate for the eigenspace of the underlying matrix, which is
comparable to the rate under the independently missing assumption. Simulation
studies show that the method performs well under a variety of configurations.
In the real data analysis, the method is applied to two tasks: (i) the
integrating of several point-wise mutual information matrices built by English
EHR and Chinese medical text data, and (ii) the machine translation between
English and Chinese medical concepts. Our method shows an advantage over
existing methods."
"['stat.ML', 'stat.ME']",Achieving Efficiency in Black Box Simulation of Distribution Tails with Self-structuring Importance Samplers,"Motivated by the increasing adoption of models which facilitate greater
automation in risk management and decision-making, this paper presents a novel
Importance Sampling (IS) scheme for measuring distribution tails of objectives
modelled with enabling tools such as feature-based decision rules, mixed
integer linear programs, deep neural networks, etc. Conventional efficient IS
approaches suffer from feasibility and scalability concerns due to the need to
intricately tailor the sampler to the underlying probability distribution and
the objective. This challenge is overcome in the proposed black-box scheme by
automating the selection of an effective IS distribution with a transformation
that implicitly learns and replicates the concentration properties observed in
less rare samples. This novel approach is guided by a large deviations
principle that brings out the phenomenon of self-similarity of optimal IS
distributions. The proposed sampler is the first to attain asymptotically
optimal variance reduction across a spectrum of multivariate distributions
despite being oblivious to the underlying structure. The large deviations
principle additionally results in new distribution tail asymptotics capable of
yielding operational insights. The applicability is illustrated by considering
product distribution networks and portfolio credit risk models informed by
neural networks as examples."
['stat.CO'],Orbital MCMC,"Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex
deterministic transformations to generate proposal points that are then
filtered by the Metropolis-Hastings-Green (MHG) test. However, the condition of
the target measure invariance puts restrictions on the design of these
transformations. In this paper, we first derive the acceptance test for the
stochastic Markov kernel considering arbitrary deterministic maps as proposal
generators. When applied to the transformations with orbits of period two
(involutions), the test reduces to the MHG test. Based on the derived test we
propose two practical algorithms: one operates by constructing periodic orbits
from any diffeomorphism, another on contractions of the state space (such as
optimization trajectories). Finally, we perform an empirical study
demonstrating the practical advantages of both kernels."
"['stat.ML', 'stat.TH']",Learning curves of generic features maps for realistic datasets with a teacher-student model,"Teacher-student models provide a framework in which the typical-case
performance of high-dimensional supervised learning can be described in closed
form. The assumptions of Gaussian i.i.d. input data underlying the canonical
teacher-student model may, however, be perceived as too restrictive to capture
the behaviour of realistic data sets. In this paper, we introduce a Gaussian
covariate generalisation of the model where the teacher and student can act on
different spaces, generated with fixed, but generic feature maps. While still
solvable in a closed form, this generalization is able to capture the learning
curves for a broad range of realistic data sets, thus redeeming the potential
of the teacher-student framework. Our contribution is then two-fold: First, we
prove a rigorous formula for the asymptotic training loss and generalisation
error. Second, we present a number of situations where the learning curve of
the model captures the one of a realistic data set learned with kernel
regression and classification, with out-of-the-box feature maps such as random
projections or scattering transforms, or with pre-learned ones - such as the
features learned by training multi-layer neural networks. We discuss both the
power and the limitations of the framework."
"['stat.ML', 'stat.TH']",On the capacity of deep generative networks for approximating distributions,"We study the efficacy and efficiency of deep generative networks for
approximating probability distributions. We prove that neural networks can
transform a low-dimensional source distribution to a distribution that is
arbitrarily close to a high-dimensional target distribution, when the closeness
are measured by Wasserstein distances and maximum mean discrepancy. Upper
bounds of the approximation error are obtained in terms of the width and depth
of neural network. Furthermore, it is shown that the approximation error in
Wasserstein distance grows at most linearly on the ambient dimension and that
the approximation order only depends on the intrinsic dimension of the target
distribution. On the contrary, when $f$-divergences are used as metrics of
distributions, the approximation property is different. We show that in order
to approximate the target distribution in $f$-divergences, the dimension of the
source distribution cannot be smaller than the intrinsic dimension of the
target distribution."
"['stat.ML', 'stat.ME', 'stat.TH']",Nonparametric Trace Regression in High Dimensions via Sign Series Representation,"Learning of matrix-valued data has recently surged in a range of scientific
and business applications. Trace regression is a widely used method to model
effects of matrix predictors and has shown great success in matrix learning.
However, nearly all existing trace regression solutions rely on two
assumptions: (i) a known functional form of the conditional mean, and (ii) a
global low-rank structure in the entire range of the regression function, both
of which may be violated in practice. In this article, we relax these
assumptions by developing a general framework for nonparametric trace
regression models via structured sign series representations of high
dimensional functions. The new model embraces both linear and nonlinear trace
effects, and enjoys rank invariance to order-preserving transformations of the
response. In the context of matrix completion, our framework leads to a
substantially richer model based on what we coin as the ""sign rank"" of a
matrix. We show that the sign series can be statistically characterized by
weighted classification tasks. Based on this connection, we propose a learning
reduction approach to learn the regression model via a series of classifiers,
and develop a parallelable computation algorithm to implement sign series
aggregations. We establish the excess risk bounds, estimation error rates, and
sample complexities. Our proposal provides a broad nonparametric paradigm to
many important matrix learning problems, including matrix regression, matrix
completion, multi-task learning, and compressed sensing. We demonstrate the
advantages of our method through simulations and two applications, one on brain
connectivity study and the other on high-rank image completion."
"['stat.ML', 'stat.ME']","Graph matching between bipartite and unipartite networks: to collapse, or not to collapse, that is the question","Graph matching consists of aligning the vertices of two unlabeled graphs in
order to maximize the shared structure across networks; when the graphs are
unipartite, this is commonly formulated as minimizing their edge disagreements.
In this paper, we address the common setting in which one of the graphs to
match is a bipartite network and one is unipartite. Commonly, the bipartite
networks are collapsed or projected into a unipartite graph, and graph matching
proceeds as in the classical setting. This potentially leads to noisy edge
estimates and loss of information. We formulate the graph matching problem
between a bipartite and a unipartite graph using an undirected graphical model,
and introduce methods to find the alignment with this model without collapsing.
We theoretically demonstrate that our methodology is consistent, and provide
non-asymptotic conditions that ensure exact recovery of the matching solution.
In simulations and real data examples, we show how our methods can result in a
more accurate matching than the naive approach of transforming the bipartite
networks into unipartite, and we demonstrate the performance gains achieved by
our method in simulated and real data networks, including a
co-authorship-citation network pair, and brain structural and functional data."
"['stat.ML', 'stat.CO']",Machine Learning Assisted Orthonormal Basis Selection for Functional Data Analysis,"In implementations of the functional data methods, the effect of the initial
choice of an orthonormal basis has not gained much attention in the past.
Typically, several standard bases such as Fourier, wavelets, splines, etc. are
considered to transform observed functional data and a choice is made without
any formal criteria indicating which of the bases is preferable for the initial
transformation of the data into functions. In an attempt to address this issue,
we propose a strictly data-driven method of orthogonal basis selection. The
method uses recently introduced orthogonal spline bases called the splinets
obtained by efficient orthogonalization of the B-splines. The algorithm learns
from the data in the machine learning style to efficiently place knots. The
optimality criterion is based on the average (per functional data point) mean
square error and is utilized both in the learning algorithms and in comparison
studies. The latter indicates efficiency that is particularly evident for the
sparse functional data and to a lesser degree in analyses of responses to
complex physical systems."
"['stat.ML', 'stat.ME']",Efficient Causal Inference from Combined Observational and Interventional Data through Causal Reductions,"Unobserved confounding is one of the main challenges when estimating causal
effects. We propose a novel causal reduction method that replaces an arbitrary
number of possibly high-dimensional latent confounders with a single latent
confounder that lives in the same space as the treatment variable without
changing the observational and interventional distributions entailed by the
causal model. After the reduction, we parameterize the reduced causal model
using a flexible class of transformations, so-called normalizing flows. We
propose a learning algorithm to estimate the parameterized reduced model
jointly from observational and interventional data. This allows us to estimate
the causal effect in a principled way from combined data. We perform a series
of experiments on data simulated using nonlinear causal mechanisms and find
that we can often substantially reduce the number of interventional samples
when adding observational training samples without sacrificing accuracy. Thus,
adding observational data may help to more accurately estimate causal effects
even in the presence of unobserved confounders."
"['stat.ML', 'stat.TH']",Learning with invariances in random features and kernel models,"A number of machine learning tasks entail a high degree of invariance: the
data distribution does not change if we act on the data with a certain group of
transformations. For instance, labels of images are invariant under
translations of the images. Certain neural network architectures -- for
instance, convolutional networks -- are believed to owe their success to the
fact that they exploit such invariance properties. With the objective of
quantifying the gain achieved by invariant architectures, we introduce two
classes of models: invariant random features and invariant kernel methods. The
latter includes, as a special case, the neural tangent kernel for convolutional
networks with global average pooling. We consider uniform covariates
distributions on the sphere and hypercube and a general invariant target
function. We characterize the test error of invariant methods in a
high-dimensional regime in which the sample size and number of hidden units
scale as polynomials in the dimension, for a class of groups that we call
`degeneracy $\alpha$', with $\alpha \leq 1$. We show that exploiting invariance
in the architecture saves a $d^\alpha$ factor ($d$ stands for the dimension) in
sample size and number of hidden units to achieve the same test error as for
unstructured architectures.
  Finally, we show that output symmetrization of an unstructured kernel
estimator does not give a significant statistical improvement; on the other
hand, data augmentation with an unstructured kernel estimator is equivalent to
an invariant kernel estimator and enjoys the same improvement in statistical
efficiency."
"['stat.ML', 'stat.CO']",Conditional Sampling With Monotone GANs,"We present a new approach for sampling conditional probability measures,
enabling consistent uncertainty quantification in supervised learning tasks. We
construct a mapping that transforms a reference measure to the measure of the
output conditioned on new inputs. The mapping is trained via a modification of
generative adversarial networks (GANs), called monotone GANs, that imposes
monotonicity and a block triangular structure. We present theoretical
guarantees for the consistency of our proposed method, as well as numerical
experiments demonstrating the ability of our method to accurately sample
conditional measures in applications ranging from inverse problems to image
in-painting."
"['stat.ML', 'stat.ME']",Automatic Registration and Clustering of Time Series,"Clustering of time series data exhibits a number of challenges not present in
other settings, notably the problem of registration (alignment) of observed
signals. Typical approaches include pre-registration to a user-specified
template or time warping approaches which attempt to optimally align series
with a minimum of distortion. For many signals obtained from recording or
sensing devices, these methods may be unsuitable as a template signal is not
available for pre-registration, while the distortion of warping approaches may
obscure meaningful temporal information. We propose a new method for automatic
time series alignment within a clustering problem. Our approach, Temporal
Registration using Optimal Unitary Transformations (TROUT), is based on a novel
dissimilarity measure between time series that is easy to compute and
automatically identifies optimal alignment between pairs of time series. By
embedding our new measure in a optimization formulation, we retain well-known
advantages of computational and statistical performance. We provide an
efficient algorithm for TROUT-based clustering and demonstrate its superior
performance over a range of competitors."
['stat.TH'],A Convenient Infinite Dimensional Framework for Generative Adversarial Learning,"In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. Assuming
the class of uniformly bounded $k$-times $\alpha$-H\""older differentiable and
uniformly positive densities, we show that the Rosenblatt transformation
induces an optimal generator, which is realizable in the hypothesis space of
$\alpha$-H\""older differentiable generators. With a consistent definition of
the hypothesis space of discriminators, we further show that in our framework
the Jensen-Shannon divergence between the distribution induced by the generator
from the adversarial learning procedure and the data generating distribution
converges to zero. Under sufficiently strict regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on concentration and chaining."
['stat.TH'],Gaussian Process Regression constrained by Boundary Value Problems,"We develop a framework for Gaussian processes regression constrained by
boundary value problems. The framework may be applied to infer the solution of
a well-posed boundary value problem with a known second-order differential
operator and boundary conditions, but for which only scattered observations of
the source term are available. Scattered observations of the solution may also
be used in the regression. The framework combines co-kriging with the linear
transformation of a Gaussian process together with the use of kernels given by
spectral expansions in eigenfunctions of the boundary value problem. Thus, it
benefits from a reduced-rank property of covariance matrices. We demonstrate
that the resulting framework yields more accurate and stable solution inference
as compared to physics-informed Gaussian process regression without boundary
condition constraints."
"['stat.ML', 'stat.ME']",Artificial Neural Networks to Impute Rounded Zeros in Compositional Data,"Methods of deep learning have become increasingly popular in recent years,
but they have not arrived in compositional data analysis. Imputation methods
for compositional data are typically applied on additive, centered or isometric
log-ratio representations of the data. Generally, methods for compositional
data analysis can only be applied to observed positive entries in a data
matrix. Therefore one tries to impute missing values or measurements that were
below a detection limit. In this paper, a new method for imputing rounded zeros
based on artificial neural networks is shown and compared with conventional
methods. We are also interested in the question whether for ANNs, a
representation of the data in log-ratios for imputation purposes, is relevant.
It can be shown, that ANNs are competitive or even performing better when
imputing rounded zeros of data sets with moderate size. They deliver better
results when data sets are big. Also, we can see that log-ratio transformations
within the artificial neural network imputation procedure nevertheless help to
improve the results. This proves that the theory of compositional data analysis
and the fulfillment of all properties of compositional data analysis is still
very important in the age of deep learning."
"['stat.ML', 'stat.TH']",Concentration bounds for linear Monge mapping estimation and optimal transport domain adaptation,"This article investigates the quality of the estimator of the linear Monge
mapping between distributions. We provide the first concentration result on the
linear mapping operator and prove a sample complexity of $n^{-1/2}$ when using
empirical estimates of first and second order moments. This result is then used
to derive a generalization bound for domain adaptation with optimal transport.
As a consequence, this method approaches the performance of theoretical Bayes
predictor under mild conditions on the covariance structure of the problem. We
also discuss the computational complexity of the linear mapping estimation and
show that when the source and target are stationary the mapping is a
convolution that can be estimated very efficiently using fast Fourier
transforms. Numerical experiments reproduce the behavior of the proven bounds
on simulated and real data for mapping estimation and domain adaptation on
images."
"['stat.ML', 'stat.ME']",A partition-based similarity for classification distributions,"Herein we define a measure of similarity between classification distributions
that is both principled from the perspective of statistical pattern recognition
and useful from the perspective of machine learning practitioners. In
particular, we propose a novel similarity on classification distributions,
dubbed task similarity, that quantifies how an optimally-transformed optimal
representation for a source distribution performs when applied to inference
related to a target distribution. The definition of task similarity allows for
natural definitions of adversarial and orthogonal distributions. We highlight
limiting properties of representations induced by (universally) consistent
decision rules and demonstrate in simulation that an empirical estimate of task
similarity is a function of the decision rule deployed for inference. We
demonstrate that for a given target distribution, both transfer efficiency and
semantic similarity of candidate source distributions correlate with empirical
task similarity."
"['stat.ML', 'stat.ME']",Tensor Canonical Correlation Analysis with Convergence and Statistical Guarantees,"In many applications, such as classification of images or videos, it is of
interest to develop a framework for tensor data instead of an ad-hoc way of
transforming data to vectors due to the computational and under-sampling
issues. In this paper, we study convergence and statistical properties of
two-dimensional canonical correlation analysis \citep{Lee2007Two} under an
assumption that data come from a probabilistic model. We show that carefully
initialized the power method converges to the optimum and provide a finite
sample bound. Then we extend this framework to tensor-valued data and propose
the higher-order power method, which is commonly used in tensor decomposition,
to extract the canonical directions. Our method can be used effectively in a
large-scale data setting by solving the inner least squares problem with a
stochastic gradient descent, and we justify convergence via the theory of
Lojasiewicz's inequalities without any assumption on data generating process
and initialization. For practical applications, we further develop (a) an
inexact updating scheme which allows us to use the state-of-the-art stochastic
gradient descent algorithm, (b) an effective initialization scheme which
alleviates the problem of local optimum in non-convex optimization, and (c) a
deflation procedure for extracting several canonical components. Empirical
analyses on challenging data including gene expression and air pollution
indexes in Taiwan, show the effectiveness and efficiency of the proposed
methodology. Our results fill a missing, but crucial, part in the literature on
tensor data."
"['stat.ML', 'stat.TH']",A Group-Theoretic Framework for Data Augmentation,"Data augmentation is a widely used trick when training deep neural networks:
in addition to the original data, properly transformed data are also added to
the training set. However, to the best of our knowledge, a clear mathematical
framework to explain the performance benefits of data augmentation is not
available. In this paper, we develop such a theoretical framework. We show data
augmentation is equivalent to an averaging operation over the orbits of a
certain group that keeps the data distribution approximately invariant. We
prove that it leads to variance reduction. We study empirical risk
minimization, and the examples of exponential families, linear regression, and
certain two-layer neural networks. We also discuss how data augmentation could
be used in problems with symmetry where other approaches are prevalent, such as
in cryo-electron microscopy (cryo-EM)."
"['stat.ML', 'stat.TH']",Adaptive Geo-Topological Independence Criterion,"Testing two potentially multivariate variables for statistical dependence on
the basis finite samples is a fundamental statistical challenge. Here we
explore a family of tests that adapt to the complexity of the relationship
between the variables, promising robust power across scenarios. Building on the
distance correlation, we introduce a family of adaptive independence criteria
based on nonlinear monotonic transformations of distances. We show that these
criteria, like the distance correlation and RKHS-based criteria, provide
dependence indicators. We propose a class of adaptive (multi-threshold) test
statistics, which form the basis for permutation tests. These tests empirically
outperform some of the established tests in average and worst-case statistical
sensitivity across a range of univariate and multivariate relationships, offer
useful insights to the data and may deserve further exploration."
"['stat.ML', 'stat.CO', 'stat.ME']",An adaptive transport framework for joint and conditional density estimation,"We propose a general framework to robustly characterize joint and conditional
probability distributions via transport maps. Transport maps or ""flows""
deterministically couple two distributions via an expressive monotone
transformation. Yet, learning the parameters of such transformations in high
dimensions is challenging given few samples from the unknown target
distribution, and structural choices for these transformations can have a
significant impact on performance. Here we formulate a systematic framework for
representing and learning monotone maps, via invertible transformations of
smooth functions, and demonstrate that the associated minimization problem has
a unique global optimum. Given a hierarchical basis for the appropriate
function space, we propose a sample-efficient adaptive algorithm that estimates
a sparse approximation for the map. We demonstrate how this framework can learn
densities with stable generalization performance across a wide range of sample
sizes on real-world datasets."
"['stat.ME', 'stat.ML']",Roundtrip: A Deep Generative Neural Density Estimator,"Density estimation is a fundamental problem in both statistics and machine
learning. In this study, we proposed Roundtrip as a general-purpose neural
density estimator based on deep generative models. Roundtrip retains the
generative power of generative adversarial networks (GANs) but also provides
estimates of density values. Unlike previous neural density estimators that put
stringent conditions on the transformation from the latent space to the data
space, Roundtrip enables the use of much more general mappings. In a series of
experiments, Roundtrip achieves state-of-the-art performance in a diverse range
of density estimation tasks."
"['stat.ML', 'stat.TH']",Probability Link Models with Symmetric Information Divergence,"This paper introduces link functions for transforming one probability
distribution to another such that the Kullback-Leibler and R\'enyi divergences
between the two distributions are symmetric. Two general classes of link models
are proposed. The first model links two survival functions and is applicable to
models such as the proportional odds and change point, which are used in
survival analysis and reliability modeling. A prototype application involving
the proportional odds model demonstrates advantages of symmetric divergence
measures over asymmetric measures for assessing the efficacy of features and
for model averaging purposes. The advantages include providing unique ranks for
models and unique information weights for model averaging with one-half as much
computation requirement of asymmetric divergences. The second model links two
cumulative probability distribution functions. This model produces a
generalized location model which are continuous counterparts of the binary
probability models such as probit and logit models. Examples include the
generalized probit and logit models which have appeared in the survival
analysis literature, and a generalized Laplace model and a generalized
Student-$t$ model, which are survival time models corresponding to the
respective binary probability models. Lastly, extensions to symmetric
divergence between survival functions and conditions for copula dependence
information are presented."
"['stat.ML', 'stat.CO']",Bayesian learning of orthogonal embeddings for multi-fidelity Gaussian Processes,"We present a Bayesian approach to identify optimal transformations that map
model input points to low dimensional latent variables. The ""projection""
mapping consists of an orthonormal matrix that is considered a priori unknown
and needs to be inferred jointly with the GP parameters, conditioned on the
available training data. The proposed Bayesian inference scheme relies on a
two-step iterative algorithm that samples from the marginal posteriors of the
GP parameters and the projection matrix respectively, both using Markov Chain
Monte Carlo (MCMC) sampling. In order to take into account the orthogonality
constraints imposed on the orthonormal projection matrix, a Geodesic Monte
Carlo sampling algorithm is employed, that is suitable for exploiting
probability measures on manifolds. We extend the proposed framework to
multi-fidelity models using GPs including the scenarios of training multiple
outputs together. We validate our framework on three synthetic problems with a
known lower-dimensional subspace. The benefits of our proposed framework, are
illustrated on the computationally challenging three-dimensional aerodynamic
optimization of a last-stage blade for an industrial gas turbine, where we
study the effect of an 85-dimensional airfoil shape parameterization on two
output quantities of interest, specifically on the aerodynamic efficiency and
the degree of reaction."
"['stat.ML', 'stat.CO']","Marginal Densities, Factor Graph Duality, and High-Temperature Series Expansions","We prove that the marginal densities of a global probability mass function in
a primal normal factor graph and the corresponding marginal densities in the
dual normal factor graph are related via local mappings. The mapping depends on
the Fourier transform of the local factors of the models. Details of the
mapping, including its fixed points, are derived for the Ising model, and then
extended to the Potts model. By employing the mapping, we can transform
simultaneously all the estimated marginal densities from one domain to the
other, which is advantageous if estimating the marginals can be carried out
more efficiently in the dual domain. An example of particular significance is
the ferromagnetic Ising model in a positive external field, for which there is
a rapidly mixing Markov chain (called the subgraphs-world process) to generate
configurations in the dual normal factor graph of the model. Our numerical
experiments illustrate that the proposed procedure can provide more accurate
estimates of marginal densities in various settings."
"['stat.CO', 'stat.ME', 'stat.ML']",Involutive MCMC: a Unifying Framework,"Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental
problems such as inference, integration, optimization, and simulation. The
field has developed a broad spectrum of algorithms, varying in the way they are
motivated, the way they are applied and how efficiently they sample. Despite
all the differences, many of them share the same core principle, which we unify
as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a
wide range of MCMC algorithms in terms of iMCMC, and formulate a number of
""tricks"" which one can use as design principles for developing new MCMC
algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms,
which facilitates the derivation of powerful extensions. We demonstrate the
latter with two examples where we transform known reversible MCMC algorithms
into more efficient irreversible ones."
"['stat.ML', 'stat.TH']",Beyond UCB: Optimal and Efficient Contextual Bandits with Regression Oracles,"A fundamental challenge in contextual bandits is to develop flexible,
general-purpose algorithms with computational requirements no worse than
classical supervised learning tasks such as classification and regression.
Algorithms based on regression have shown promising empirical success, but
theoretical guarantees have remained elusive except in special cases. We
provide the first universal and optimal reduction from contextual bandits to
online regression. We show how to transform any oracle for online regression
with a given value function class into an algorithm for contextual bandits with
the induced policy class, with no overhead in runtime or memory requirements.
We characterize the minimax rates for contextual bandits with general,
potentially nonparametric function classes, and show that our algorithm is
minimax optimal whenever the oracle obtains the optimal rate for regression.
Compared to previous results, our algorithm requires no distributional
assumptions beyond realizability, and works even when contexts are chosen
adversarially."
"['stat.ML', 'stat.TH']",Estimates on Learning Rates for Multi-Penalty Distribution Regression,"This paper is concerned with functional learning by utilizing two-stage
sampled distribution regression. We study a multi-penalty regularization
algorithm for distribution regression under the framework of learning theory.
The algorithm aims at regressing to real valued outputs from probability
measures. The theoretical analysis on distribution regression is far from
maturity and quite challenging, since only second stage samples are observable
in practical setting. In the algorithm, to transform information from samples,
we embed the distributions to a reproducing kernel Hilbert space
$\mathcal{H}_K$ associated with Mercer kernel $K$ via mean embedding technique.
The main contribution of the paper is to present a novel multi-penalty
regularization algorithm to capture more features of distribution regression
and derive optimal learning rates for the algorithm. The work also derives
learning rates for distribution regression in the nonstandard setting
$f_{\rho}\notin\mathcal{H}_K$, which is not explored in existing literature.
Moreover, we propose a distribution regression-based distributed learning
algorithm to face large-scale data or information challenge. The optimal
learning rates are derived for the distributed learning algorithm. By providing
new algorithms and showing their learning rates, we improve the existing work
in different aspects in the literature."
"['stat.ML', 'stat.ME']",Optimal Training of Fair Predictive Models,"Recently there has been sustained interest in modifying prediction algorithms
to satisfy fairness constraints. These constraints are typically complex
nonlinear functionals of the observed data distribution. Focusing on the causal
constraints proposed by Nabi and Shpitser (2018), we introduce new theoretical
results and optimization techniques to make model training easier and more
accurate. Specifically, we show how to reparameterize the observed data
likelihood such that fairness constraints correspond directly to parameters
that appear in the likelihood, transforming a complex constrained optimization
objective into a simple optimization problem with box constraints. We also
exploit methods from empirical likelihood theory in statistics to improve
predictive performance, without requiring parametric models for
high-dimensional feature vectors."
"['stat.ML', 'stat.CO']",Forecasting with time series imaging,"Feature-based time series representations have attracted substantial
attention in a wide range of time series analysis methods. Recently, the use of
time series features for forecast model averaging has been an emerging research
focus in the forecasting community. Nonetheless, most of the existing
approaches depend on the manual choice of an appropriate set of features.
Exploiting machine learning methods to extract features from time series
automatically becomes crucial in state-of-the-art time series analysis. In this
paper, we introduce an automated approach to extract time series features based
on time series imaging. We first transform time series into recurrence plots,
from which local features can be extracted using computer vision algorithms.
The extracted features are used for forecast model averaging. Our experiments
show that forecasting based on automatically extracted features, with less
human intervention and a more comprehensive view of the raw time series data,
yields highly comparable performances with the best methods in the largest
forecasting competition dataset (M4) and outperforms the top methods in the
Tourism forecasting competition dataset."
"['stat.ML', 'stat.ME']",Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models,"Models which estimate main effects of individual variables alongside
interaction effects have an identifiability challenge: effects can be freely
moved between main effects and interaction effects without changing the model
prediction. This is a critical problem for interpretability because it permits
""contradictory"" models to represent the same function. To solve this problem,
we propose pure interaction effects: variance in the outcome which cannot be
represented by any smaller subset of features. This definition has an
equivalence with the Functional ANOVA decomposition. To compute this
decomposition, we present a fast, exact algorithm that transforms any
piecewise-constant function (such as a tree-based model) into a purified,
canonical representation. We apply this algorithm to Generalized Additive
Models with interactions trained on several datasets and show large disparity,
including contradictions, between the effects before and after purification.
These results underscore the need to specify data distributions and ensure
identifiability before interpreting model parameters."
"['stat.ML', 'stat.ME', 'stat.TH']",Efficient Clustering for Stretched Mixtures: Landscape and Optimality,"This paper considers a canonical clustering problem where one receives
unlabeled samples drawn from a balanced mixture of two elliptical distributions
and aims for a classifier to estimate the labels. Many popular methods
including PCA and k-means require individual components of the mixture to be
somewhat spherical, and perform poorly when they are stretched. To overcome
this issue, we propose a non-convex program seeking for an affine transform to
turn the data into a one-dimensional point cloud concentrating around -1 and 1,
after which clustering becomes easy. Our theoretical contributions are
two-fold: (1) we show that the non-convex loss function exhibits desirable
landscape properties as long as the sample size exceeds some constant multiple
of the dimension, and (2) we leverage this to prove that an efficient
first-order algorithm achieves near-optimal statistical precision even without
good initialization. We also propose a general methodology for multi-class
clustering tasks with flexible choices of feature transforms and loss
objectives."
"['stat.ML', 'stat.ME']",MAP Clustering under the Gaussian Mixture Model via Mixed Integer Nonlinear Optimization,"We present a global optimization approach for solving the maximum
a-posteriori (MAP) clustering problem under the Gaussian mixture model.Our
approach can accommodate side constraints and it preserves the combinatorial
structure of the MAP clustering problem by formulating it asa mixed-integer
nonlinear optimization problem (MINLP). We approximate the MINLP through a
mixed-integer quadratic program (MIQP) transformation that improves
computational aspects while guaranteeing $\epsilon$-global optimality. An
important benefit of our approach is the explicit quantification of the degree
of suboptimality, via the optimality gap, en route to finding the globally
optimal MAP clustering. Numerical experiments comparing our method to other
approaches show that our method finds a better solution than standard
clustering methods. Finally, we cluster a real breast cancer gene expression
data set incorporating intrinsic subtype information; the induced constraints
substantially improve the computational performance and produce more coherent
and bio-logically meaningful clusters."
"['stat.ML', 'stat.ME']",Improved Calibration of Numerical Integration Error in Sigma-Point Filters,"The sigma-point filters, such as the UKF, which exploit numerical quadrature
to obtain an additional order of accuracy in the moment transformation step,
are popular alternatives to the ubiquitous EKF. The classical quadrature rules
used in the sigma-point filters are motivated via polynomial approximation of
the integrand, however in the applied context these assumptions cannot always
be justified. As a result, quadrature error can introduce bias into estimated
moments, for which there is no compensatory mechanism in the classical
sigma-point filters. This can lead in turn to estimates and predictions that
are poorly calibrated. In this article, we investigate the Bayes-Sard
quadrature method in the context of sigma-point filters, which enables
uncertainty due to quadrature error to be formalised within a probabilistic
model. Our first contribution is to derive the well-known classical quadratures
as special cases of the Bayes-Sard quadrature method. Then a general-purpose
moment transform is developed and utilised in the design of novel sigma-point
filters, so that uncertainty due to quadrature error is explicitly quantified.
Numerical experiments on a challenging tracking example with misspecified
initial conditions show that the additional uncertainty quantification built
into our method leads to better-calibrated state estimates with improved RMSE."
"['stat.ML', 'stat.TH']",Data Transformation Insights in Self-supervision with Clustering Tasks,"Self-supervision is key to extending use of deep learning for label scarce
domains. For most of self-supervised approaches data transformations play an
important role. However, up until now the impact of transformations have not
been studied. Furthermore, different transformations may have different impact
on the system. We provide novel insights into the use of data transformation in
self-supervised tasks, specially pertaining to clustering. We show
theoretically and empirically that certain set of transformations are helpful
in convergence of self-supervised clustering. We also show the cases when the
transformations are not helpful or in some cases even harmful. We show faster
convergence rate with valid transformations for convex as well as certain
family of non-convex objectives along with the proof of convergence to the
original set of optima. We have synthetic as well as real world data
experiments. Empirically our results conform with the theoretical insights
provided."
"['stat.ML', 'stat.TH']",Histogram Transform Ensembles for Large-scale Regression,"We propose a novel algorithm for large-scale regression problems named
histogram transform ensembles (HTE), composed of random rotations, stretchings,
and translations. First of all, we investigate the theoretical properties of
HTE when the regression function lies in the H\""{o}lder space $C^{k,\alpha}$,
$k \in \mathbb{N}_0$, $\alpha \in (0,1]$. In the case that $k=0, 1$, we adopt
the constant regressors and develop the na\""{i}ve histogram transforms (NHT).
Within the space $C^{0,\alpha}$, although almost optimal convergence rates can
be derived for both single and ensemble NHT, we fail to show the benefits of
ensembles over single estimators theoretically. In contrast, in the subspace
$C^{1,\alpha}$, we prove that if $d \geq 2(1+\alpha)/\alpha$, the lower bound
of the convergence rates for single NHT turns out to be worse than the upper
bound of the convergence rates for ensemble NHT. In the other case when $k \geq
2$, the NHT may no longer be appropriate in predicting smoother regression
functions. Instead, we apply kernel histogram transforms (KHT) equipped with
smoother regressors such as support vector machines (SVMs), and it turns out
that both single and ensemble KHT enjoy almost optimal convergence rates. Then
we validate the above theoretical results by numerical experiments. On the one
hand, simulations are conducted to elucidate that ensemble NHT outperform
single NHT. On the other hand, the effects of bin sizes on accuracy of both NHT
and KHT also accord with theoretical analysis. Last but not least, in the
real-data experiments, comparisons between the ensemble KHT, equipped with
adaptive histogram transforms, and other state-of-the-art large-scale
regression estimators verify the effectiveness and accuracy of our algorithm."
"['stat.CO', 'stat.ML']",Differentiable Algorithm for Marginalising Changepoints,"We present an algorithm for marginalising changepoints in time-series models
that assume a fixed number of unknown changepoints. Our algorithm is
differentiable with respect to its inputs, which are the values of latent
random variables other than changepoints. Also, it runs in time O(mn) where n
is the number of time steps and m the number of changepoints, an improvement
over a naive marginalisation method with O(n^m) time complexity. We derive the
algorithm by identifying quantities related to this marginalisation problem,
showing that these quantities satisfy recursive relationships, and transforming
the relationships to an algorithm via dynamic programming. Since our algorithm
is differentiable, it can be applied to convert a model non-differentiable due
to changepoints to a differentiable one, so that the resulting models can be
analysed using gradient-based inference or learning techniques. We empirically
show the effectiveness of our algorithm in this application by tackling the
posterior inference problem on synthetic and real-world data."
"['stat.ML', 'stat.CO']",Invariance and identifiability issues for word embeddings,"Word embeddings are commonly obtained as optimizers of a criterion function
$f$ of a text corpus, but assessed on word-task performance using a different
evaluation function $g$ of the test data. We contend that a possible source of
disparity in performance on tasks is the incompatibility between classes of
transformations that leave $f$ and $g$ invariant. In particular, word
embeddings defined by $f$ are not unique; they are defined only up to a class
of transformations to which $f$ is invariant, and this class is larger than the
class to which $g$ is invariant. One implication of this is that the apparent
superiority of one word embedding over another, as measured by word task
performance, may largely be a consequence of the arbitrary elements selected
from the respective solution sets. We provide a formal treatment of the above
identifiability issue, present some numerical examples, and discuss possible
resolutions."
['stat.TH'],A Novel Intrinsic Measure of Data Separability,"In machine learning, the performance of a classifier depends on both the
classifier model and the separability/complexity of datasets. To quantitatively
measure the separability of datasets, we create an intrinsic measure -- the
Distance-based Separability Index (DSI), which is independent of the classifier
model. We consider the situation in which different classes of data are mixed
in the same distribution to be the most difficult for classifiers to separate.
We then formally show that the DSI can indicate whether the distributions of
datasets are identical for any dimensionality. And we verify the DSI to be an
effective separability measure by comparing to several state-of-the-art
separability/complexity measures using synthetic and real datasets. Having
demonstrated the DSI's ability to compare distributions of samples, we also
discuss some of its other promising applications, such as measuring the
performance of generative adversarial networks (GANs) and evaluating the
results of clustering methods."
"['stat.ML', 'stat.TH']",An error analysis of generative adversarial networks for learning distributions,"This paper studies how well generative adversarial networks (GANs) learn
probability distributions from finite samples. Our main results establish the
convergence rates of GANs under a collection of integral probability metrics
defined through H\""older classes, including the Wasserstein distance as a
special case. We also show that GANs are able to adaptively learn data
distributions with low-dimensional structures or have H\""older densities, when
the network architectures are chosen properly. In particular, for distributions
concentrated around a low-dimensional set, we show that the learning rates of
GANs do not depend on the high ambient dimension, but on the lower intrinsic
dimension. Our analysis is based on a new oracle inequality decomposing the
estimation error into the generator and discriminator approximation error and
the statistical error, which may be of independent interest."
"['stat.ML', 'stat.CO']",Exploiting Chain Rule and Bayes' Theorem to Compare Probability Distributions,"To measure the difference between two probability distributions, referred to
as the source and target, respectively, we exploit both the chain rule and
Bayes' theorem to construct conditional transport (CT), which is constituted by
both a forward component and a backward one. The forward CT is the expected
cost of moving a source data point to a target one, with their joint
distribution defined by the product of the source probability density function
(PDF) and a source-dependent conditional distribution, which is related to the
target PDF via Bayes' theorem. The backward CT is defined by reversing the
direction. The CT cost can be approximated by replacing the source and target
PDFs with their discrete empirical distributions supported on mini-batches,
making it amenable to implicit distributions and stochastic gradient
descent-based optimization. When applied to train a generative model, CT is
shown to strike a good balance between mode-covering and mode-seeking behaviors
and strongly resist mode collapse. On a wide variety of benchmark datasets for
generative modeling, substituting the default statistical distance of an
existing generative adversarial network with CT is shown to consistently
improve the performance. PyTorch-style code is provided."
"['stat.ML', 'stat.CO']",IGANI: Iterative Generative Adversarial Networks for Imputation with Application to Traffic Data,"Increasing use of sensor data in intelligent transportation systems calls for
accurate imputation algorithms that can enable reliable traffic management in
the occasional absence of data. As one of the effective imputation approaches,
generative adversarial networks (GANs) are implicit generative models that can
be used for data imputation, which is formulated as an unsupervised learning
problem. This work introduces a novel iterative GAN architecture, called
Iterative Generative Adversarial Networks for Imputation (IGANI), for data
imputation. IGANI imputes data in two steps and maintains the invertibility of
the generative imputer, which will be shown to be a sufficient condition for
the convergence of the proposed GAN-based imputation. The performance of our
proposed method is evaluated on (1) the imputation of traffic speed data
collected in the city of Guangzhou in China, and the training of short-term
traffic prediction models using imputed data, and (2) the imputation of
multi-variable traffic data of highways in Portland-Vancouver metropolitan
region which includes volume, occupancy, and speed with different missing rates
for each of them. It is shown that our proposed algorithm mostly produces more
accurate results compared to those of previous GAN-based imputation
architectures."
"['stat.ML', 'stat.CO']",Convergence dynamics of Generative Adversarial Networks: the dual metric flows,"Fitting neural networks often resorts to stochastic (or similar) gradient
descent which is a noise-tolerant (and efficient) resolution of a gradient
descent dynamics. It outputs a sequence of networks parameters, which sequence
evolves during the training steps. The gradient descent is the limit, when the
learning rate is small and the batch size is infinite, of this set of
increasingly optimal network parameters obtained during training. In this
contribution, we investigate instead the convergence in the Generative
Adversarial Networks used in machine learning. We study the limit of small
learning rate, and show that, similar to single network training, the GAN
learning dynamics tend, for vanishing learning rate to some limit dynamics.
This leads us to consider evolution equations in metric spaces (which is the
natural framework for evolving probability laws) that we call dual flows. We
give formal definitions of solutions and prove the convergence. The theory is
then applied to specific instances of GANs and we discuss how this insight
helps understand and mitigate the mode collapse.
  Keywords: GAN; metric flow; generative network"
"['stat.ML', 'stat.CO']",Conditional Sampling With Monotone GANs,"We present a new approach for sampling conditional probability measures,
enabling consistent uncertainty quantification in supervised learning tasks. We
construct a mapping that transforms a reference measure to the measure of the
output conditioned on new inputs. The mapping is trained via a modification of
generative adversarial networks (GANs), called monotone GANs, that imposes
monotonicity and a block triangular structure. We present theoretical
guarantees for the consistency of our proposed method, as well as numerical
experiments demonstrating the ability of our method to accurately sample
conditional measures in applications ranging from inverse problems to image
in-painting."
"['stat.ME', 'stat.ML']",Time Series (re)sampling using Generative Adversarial Networks,"We propose a novel bootstrap procedure for dependent data based on Generative
Adversarial networks (GANs). We show that the dynamics of common stationary
time series processes can be learned by GANs and demonstrate that GANs trained
on a single sample path can be used to generate additional samples from the
process. We find that temporal convolutional neural networks provide a suitable
design for the generator and discriminator, and that convincing samples can be
generated on the basis of a vector of iid normal noise. We demonstrate the
finite sample properties of GAN sampling and the suggested bootstrap using
simulations where we compare the performance to circular block bootstrapping in
the case of resampling an AR(1) time series processes. We find that resampling
using the GAN can outperform circular block bootstrapping in terms of empirical
coverage."
['stat.TH'],A Convenient Infinite Dimensional Framework for Generative Adversarial Learning,"In recent years, generative adversarial networks (GANs) have demonstrated
impressive experimental results while there are only a few works that foster
statistical learning theory for GANs. In this work, we propose an infinite
dimensional theoretical framework for generative adversarial learning. Assuming
the class of uniformly bounded $k$-times $\alpha$-H\""older differentiable and
uniformly positive densities, we show that the Rosenblatt transformation
induces an optimal generator, which is realizable in the hypothesis space of
$\alpha$-H\""older differentiable generators. With a consistent definition of
the hypothesis space of discriminators, we further show that in our framework
the Jensen-Shannon divergence between the distribution induced by the generator
from the adversarial learning procedure and the data generating distribution
converges to zero. Under sufficiently strict regularity assumptions on the
density of the data generating process, we also provide rates of convergence
based on concentration and chaining."
"['stat.ML', 'stat.ME']",Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,"We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier."
"['stat.ML', 'stat.TH']",Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization,"We consider the optimization problem of minimizing a functional defined over
a family of probability distributions, where the objective functional is
assumed to possess a variational form. Such a distributional optimization
problem arises widely in machine learning and statistics, with Monte-Carlo
sampling, variational inference, policy optimization, and generative
adversarial network as examples. For this problem, we propose a novel
particle-based algorithm, dubbed as variational transport, which approximately
performs Wasserstein gradient descent over the manifold of probability
distributions via iteratively pushing a set of particles. Specifically, we
prove that moving along the geodesic in the direction of functional gradient
with respect to the second-order Wasserstein distance is equivalent to applying
a pushforward mapping to a probability distribution, which can be approximated
accurately by pushing a set of particles. Specifically, in each iteration of
variational transport, we first solve the variational problem associated with
the objective functional using the particles, whose solution yields the
Wasserstein gradient direction. Then we update the current distribution by
pushing each particle along the direction specified by such a solution. By
characterizing both the statistical error incurred in estimating the
Wasserstein gradient and the progress of the optimization algorithm, we prove
that when the objective function satisfies a functional version of the
Polyak-\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,
variational transport converges linearly to the global minimum of the objective
functional up to a certain statistical error, which decays to zero sublinearly
as the number of particles goes to infinity."
['stat.TH'],Convergence and Sample Complexity of SGD in GANs,"We provide theoretical convergence guarantees on training Generative
Adversarial Networks (GANs) via SGD. We consider learning a target distribution
modeled by a 1-layer Generator network with a non-linear activation function
$\phi(\cdot)$ parametrized by a $d \times d$ weight matrix $\mathbf W_*$, i.e.,
$f_*(\mathbf x) = \phi(\mathbf W_* \mathbf x)$.
  Our main result is that by training the Generator together with a
Discriminator according to the Stochastic Gradient Descent-Ascent iteration
proposed by Goodfellow et al. yields a Generator distribution that approaches
the target distribution of $f_*$. Specifically, we can learn the target
distribution within total-variation distance $\epsilon$ using $\tilde
O(d^2/\epsilon^2)$ samples which is (near-)information theoretically optimal.
  Our results apply to a broad class of non-linear activation functions $\phi$,
including ReLUs and is enabled by a connection with truncated statistics and an
appropriate design of the Discriminator network. Our approach relies on a
bilevel optimization framework to show that vanilla SGDA works."
"['stat.ME', 'stat.ML']",Roundtrip: A Deep Generative Neural Density Estimator,"Density estimation is a fundamental problem in both statistics and machine
learning. In this study, we proposed Roundtrip as a general-purpose neural
density estimator based on deep generative models. Roundtrip retains the
generative power of generative adversarial networks (GANs) but also provides
estimates of density values. Unlike previous neural density estimators that put
stringent conditions on the transformation from the latent space to the data
space, Roundtrip enables the use of much more general mappings. In a series of
experiments, Roundtrip achieves state-of-the-art performance in a diverse range
of density estimation tasks."
"['stat.ML', 'stat.TH']",Classification Logit Two-sample Testing by Neural Networks,"The recent success of generative adversarial networks and variational
learning suggests training a classifier network may work well in addressing the
classical two-sample problem. Network-based tests have the computational
advantage that the algorithm scales to large samples. This paper proposes a
two-sample statistic which is the difference of the logit function, provided by
a trained classification neural network, evaluated on the testing set split of
the two datasets. Theoretically, we prove the testing power to differentiate
two sub-exponential densities given that the network is sufficiently
parametrized. When the two densities lie on or near to low-dimensional
manifolds embedded in possibly high-dimensional space, the needed network
complexity is reduced to only scale with the intrinsic dimensionality. Both the
approximation and estimation error analysis are based on a new result of
near-manifold integral approximation. In experiments, the proposed method
demonstrates better performance than previous network-based tests using
classification accuracy as the two-sample statistic, and compares favorably to
certain kernel maximum mean discrepancy tests on synthetic datasets and
hand-written digit datasets."
"['stat.ML', 'stat.ME']",Prescribed Generative Adversarial Networks,"Generative adversarial networks (GANs) are a powerful approach to
unsupervised learning. They have achieved state-of-the-art performance in the
image domain. However, GANs are limited in two ways. They often learn
distributions with low support---a phenomenon known as mode collapse---and they
do not guarantee the existence of a probability density, which makes evaluating
generalization using predictive log-likelihood impossible. In this paper, we
develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs
add noise to the output of a density network and optimize an
entropy-regularized adversarial loss. The added noise renders tractable
approximations of the predictive log-likelihood and stabilizes the training
procedure. The entropy regularizer encourages PresGANs to capture all the modes
of the data distribution. Fitting PresGANs involves computing the intractable
gradients of the entropy regularization term; PresGANs sidestep this
intractability using unbiased stochastic estimates. We evaluate PresGANs on
several datasets and found they mitigate mode collapse and generate samples
with high perceptual quality. We further found that PresGANs reduce the gap in
performance in terms of predictive log-likelihood between traditional GANs and
variational autoencoders (VAEs)."
"['stat.ML', 'stat.CO']",Introducing a Generative Adversarial Network Model for Lagrangian Trajectory Simulation,"We introduce a generative adversarial network (GAN) model to simulate the
3-dimensional Lagrangian motion of particles trapped in the recirculation zone
of a buoyancy-opposed flame. The GAN model comprises a stochastic recurrent
neural network, serving as a generator, and a convoluted neural network,
serving as a discriminator. Adversarial training was performed to the point
where the best-trained discriminator failed to distinguish the ground truth
from the trajectory produced by the best-trained generator. The model
performance was then benchmarked against a statistical analysis performed on
both the simulated trajectories and the ground truth, with regard to the
accuracy and generalization criteria."
"['stat.ML', 'stat.TH']",How Well Can Generative Adversarial Networks Learn Densities: A Nonparametric View,"We study in this paper the rate of convergence for learning densities under
the Generative Adversarial Networks (GAN) framework, borrowing insights from
nonparametric statistics. We introduce an improved GAN estimator that achieves
a faster rate, through simultaneously leveraging the level of smoothness in the
target density and the evaluation metric, which in theory remedies the mode
collapse problem reported in the literature. A minimax lower bound is
constructed to show that when the dimension is large, the exponent in the rate
for the new GAN estimator is near optimal. One can view our results as
answering in a quantitative way how well GAN learns a wide range of densities
with different smoothness properties, under a hierarchy of evaluation metrics.
As a byproduct, we also obtain improved generalization bounds for GAN with
deeper ReLU discriminator network."
"['stat.ML', 'stat.CO', 'stat.ME']",Hierarchical Implicit Models and Likelihood-Free Variational Inference,"Implicit probabilistic models are a flexible class of models defined by a
simulation process for data. They form the basis for theories which encompass
our understanding of the physical world. Despite this fundamental nature, the
use of implicit models remains limited due to challenges in specifying complex
latent structure in them, and in performing inferences in such models with
large data sets. In this paper, we first introduce hierarchical implicit models
(HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian
modeling, thereby defining models via simulators of data with rich hidden
structure. Next, we develop likelihood-free variational inference (LFVI), a
scalable variational inference algorithm for HIMs. Key to LFVI is specifying a
variational family that is also implicit. This matches the model's flexibility
and allows for accurate approximation of the posterior. We demonstrate diverse
applications: a large-scale physical simulator for predator-prey populations in
ecology; a Bayesian generative adversarial network for discrete data; and a
deep implicit model for text generation."
"['stat.ML', 'stat.CO']",Deep Probabilistic Programming,"We propose Edward, a Turing-complete probabilistic programming language.
Edward defines two compositional representations---random variables and
inference. By treating inference as a first class citizen, on a par with
modeling, we show that probabilistic programming can be as flexible and
computationally efficient as traditional deep learning. For flexibility, Edward
makes it easy to fit the same model using a variety of composable inference
methods, ranging from point estimation to variational inference to MCMC. In
addition, Edward can reuse the modeling representation as part of inference,
facilitating the design of rich variational models and generative adversarial
networks. For efficiency, Edward is integrated into TensorFlow, providing
significant speedups over existing probabilistic systems. For example, we show
on a benchmark logistic regression task that Edward is at least 35x faster than
Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it
is as fast as handwritten TensorFlow."
"['stat.ML', 'stat.CO']",Learning in Implicit Generative Models,"Generative adversarial networks (GANs) provide an algorithmic framework for
constructing generative models with several appealing properties: they do not
require a likelihood function to be specified, only a generating procedure;
they provide samples that are sharp and compelling; and they allow us to
harness our knowledge of building highly accurate neural network classifiers.
Here, we develop our understanding of GANs with the aim of forming a rich view
of this growing area of machine learning---to build connections to the diverse
set of statistical thinking on this topic, of which much can be gained by a
mutual exchange of ideas. We frame GANs within the wider landscape of
algorithms for learning in implicit generative models--models that only specify
a stochastic procedure with which to generate data--and relate these ideas to
modelling problems in related fields, such as econometrics and approximate
Bayesian computation. We develop likelihood-free inference methods and
highlight hypothesis testing as a principle for learning in implicit generative
models, using which we are able to derive the objective function used by GANs,
and many other related objectives. The testing viewpoint directs our focus to
the general problem of density ratio estimation. There are four approaches for
density ratio estimation, one of which is a solution using classifiers to
distinguish real from generated data. Other approaches such as divergence
minimisation and moment matching have also been explored in the GAN literature,
and we synthesise these views to form an understanding in terms of the
relationships between them and the wider literature, highlighting avenues for
future exploration and cross-pollination."
"['stat.ML', 'stat.CO']",Non-stationary Gaussian process discriminant analysis with variable selection for high-dimensional functional data,"High-dimensional classification and feature selection tasks are ubiquitous
with the recent advancement in data acquisition technology. In several
application areas such as biology, genomics and proteomics, the data are often
functional in their nature and exhibit a degree of roughness and
non-stationarity. These structures pose additional challenges to commonly used
methods that rely mainly on a two-stage approach performing variable selection
and classification separately. We propose in this work a novel Gaussian process
discriminant analysis (GPDA) that combines these steps in a unified framework.
Our model is a two-layer non-stationary Gaussian process coupled with an Ising
prior to identify differentially-distributed locations. Scalable inference is
achieved via developing a variational scheme that exploits advances in the use
of sparse inverse covariance matrices. We demonstrate the performance of our
methodology on simulated datasets and two proteomics datasets: breast cancer
and SARS-CoV-2. Our approach distinguishes itself by offering explainability as
well as uncertainty quantification in addition to low computational cost, which
are crucial to increase trust and social acceptance of data-driven tools."
"['stat.ME', 'stat.ML']",Inferring feature importance with uncertainties in high-dimensional data,"Estimating feature importance is a significant aspect of explaining
data-based models. Besides explaining the model itself, an equally relevant
question is which features are important in the underlying data generating
process. We present a Shapley value based framework for inferring the
importance of individual features, including uncertainty in the estimator. We
build upon the recently published feature importance measure of SAGE (Shapley
additive global importance) and introduce sub-SAGE which can be estimated
without resampling for tree-based models. We argue that the uncertainties can
be estimated from bootstrapping and demonstrate the approach for tree ensemble
methods. The framework is exemplified on synthetic data as well as
high-dimensional genomics data."
"['stat.ML', 'stat.ME']",Neural Networks for Latent Budget Analysis of Compositional Data,"Compositional data are non-negative data collected in a rectangular matrix
with a constant row sum. Due to the non-negativity the focus is on conditional
proportions that add up to 1 for each row. A row of conditional proportions is
called an observed budget. Latent budget analysis (LBA) assumes a mixture of
latent budgets that explains the observed budgets. LBA is usually fitted to a
contingency table, where the rows are levels of one or more explanatory
variables and the columns the levels of a response variable. In prospective
studies, there is only knowledge about the explanatory variables of individuals
and interest goes out to predicting the response variable. Thus, a form of LBA
is needed that has the functionality of prediction. Previous studies proposed a
constrained neural network (NN) extension of LBA that was hampered by an
unsatisfying prediction ability. Here we propose LBA-NN, a feed forward NN
model that yields a similar interpretation to LBA but equips LBA with a better
ability of prediction. A stable and plausible interpretation of LBA-NN is
obtained through the use of importance plots and table, that show the relative
importance of all explanatory variables on the response variable. An LBA-NN-K-
means approach that applies K-means clustering on the importance table is used
to produce K clusters that are comparable to K latent budgets in LBA. Here we
provide different experiments where LBA-NN is implemented and compared with
LBA. In our analysis, LBA-NN outperforms LBA in prediction in terms of
accuracy, specificity, recall and mean square error. We provide open-source
software at GitHub."
"['stat.ML', 'stat.CO', 'stat.ME']",A New Basis for Sparse Principal Component Analysis,"Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids ""deflation"" and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data."
['stat.ME'],Challenges for cognitive decoding using deep learning methods,"In cognitive decoding, researchers aim to characterize a brain region's
representations by identifying the cognitive states (e.g., accepting/rejecting
a gamble) that can be identified from the region's activity. Deep learning (DL)
methods are highly promising for cognitive decoding, with their unmatched
ability to learn versatile representations of complex data. Yet, their
widespread application in cognitive decoding is hindered by their general lack
of interpretability as well as difficulties in applying them to small datasets
and in ensuring their reproducibility and robustness. We propose to approach
these challenges by leveraging recent advances in explainable artificial
intelligence and transfer learning, while also providing specific
recommendations on how to improve the reproducibility and robustness of DL
modeling results."
"['stat.ML', 'stat.ME', 'stat.OT']",Modeling longitudinal data using matrix completion,"In clinical practice and biomedical research, measurements are often
collected sparsely and irregularly in time while the data acquisition is
expensive and inconvenient. Examples include measurements of spine bone mineral
density, cancer growth through mammography or biopsy, a progression of
defective vision, or assessment of gait in patients with neurological
disorders. Since the data collection is often costly and inconvenient,
estimation of progression from sparse observations is of great interest for
practitioners.
  From the statistical standpoint, such data is often analyzed in the context
of a mixed-effect model where time is treated as both a fixed-effect
(population progression curve) and a random-effect (individual variability).
Alternatively, researchers analyze Gaussian processes or functional data where
observations are assumed to be drawn from a certain distribution of processes.
These models are flexible but rely on probabilistic assumptions, require very
careful implementation, specific to the given problem, and tend to be slow in
practice.
  In this study, we propose an alternative elementary framework for analyzing
longitudinal data, relying on matrix completion. Our method yields estimates of
progression curves by iterative application of the Singular Value
Decomposition. Our framework covers multivariate longitudinal data, regression,
and can be easily extended to other settings. As it relies on existing tools
for matrix algebra it is efficient and easy to implement.
  We apply our methods to understand trends of progression of motor impairment
in children with Cerebral Palsy. Our model approximates individual progression
curves and explains 30% of the variability. Low-rank representation of
progression trends enables identification of different progression trends in
subtypes of Cerebral Palsy."
"['stat.CO', 'stat.ML']",Generalized Matrix Factorization,"Unmeasured or latent variables are often the cause of correlations between
multivariate measurements and are studied in a variety of fields such as
psychology, ecology, and medicine. For Gaussian measurements, there are
classical tools such as factor analysis or principal component analysis with a
well-established theory and fast algorithms. Generalized Linear Latent Variable
models (GLLVM) generalize such factor models to non-Gaussian responses.
However, current algorithms for estimating model parameters in GLLVMs require
intensive computation and do not scale to large datasets with thousands of
observational units or responses. In this article, we propose a new approach
for fitting GLLVMs to such high-volume, high-dimensional datasets. We
approximate the likelihood using penalized quasi-likelihood and use a Newton
method and Fisher scoring to learn the model parameters. Our method greatly
reduces the computation time and can be easily parallelized, enabling
factorization at unprecedented scale using commodity hardware. We illustrate
application of our method on a dataset of 48,000 observational units with over
2,000 observed species in each unit, finding that most of the variability can
be explained with a handful of factors."
['stat.CO'],A Unified View of Stochastic Hamiltonian Sampling,"In this work, we revisit the theoretical properties of Hamiltonian stochastic
differential equations (SDEs) for Bayesian posterior sampling, and we study the
two types of errors that arise from numerical SDE simulation: the
discretization error and the error due to noisy gradient estimates in the
context of data subsampling. We consider overlooked results describing the
ergodic convergence rates of numerical integration schemes, and we produce a
novel analysis for the effect of mini-batches through the lens of differential
operator splitting. In our analysis, the stochastic component of the proposed
Hamiltonian SDE is decoupled from the gradient noise, for which we make no
normality assumptions. This allows us to derive interesting connections among
different sampling schemes, including the original Hamiltonian Monte Carlo
(HMC) algorithm, and explain their performance. We show that for a careful
selection of numerical integrators, both errors vanish at a rate
$\mathcal{O}(\eta^2)$, where $\eta$ is the integrator step size. Our
theoretical results are supported by an empirical study on a variety of
regression and classification tasks for Bayesian neural networks."
"['stat.CO', 'stat.ME', 'stat.ML']",On Locality of Local Explanation Models,"Shapley values provide model agnostic feature attributions for model outcome
at a particular instance by simulating feature absence under a global
population distribution. The use of a global population can lead to potentially
misleading results when local model behaviour is of interest. Hence we consider
the formulation of neighbourhood reference distributions that improve the local
interpretability of Shapley values. By doing so, we find that the
Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as
a self-normalised importance sampling estimator. Empirically, we observe that
Neighbourhood Shapley values identify meaningful sparse feature relevance
attributions that provide insight into local model behaviour, complimenting
conventional Shapley analysis. They also increase on-manifold explainability
and robustness to the construction of adversarial classifiers."
['stat.CO'],Learned Interpretable Residual Extragradient ISTA for Sparse Coding,"Recently, the study on learned iterative shrinkage thresholding algorithm
(LISTA) has attracted increasing attentions. A large number of experiments as
well as some theories have proved the high efficiency of LISTA for solving
sparse coding problems. However, existing LISTA methods are all serial
connection. To address this issue, we propose a novel extragradient based LISTA
(ELISTA), which has a residual structure and theoretical guarantees. In
particular, our algorithm can also provide the interpretability for Res-Net to
a certain extent. From a theoretical perspective, we prove that our method
attains linear convergence. In practice, extensive empirical results verify the
advantages of our method."
"['stat.ML', 'stat.TH']","Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting","We consider interpolation learning in high-dimensional linear regression with
Gaussian data, and prove a generic uniform convergence guarantee on the
generalization error of interpolators in an arbitrary hypothesis class in terms
of the class's Gaussian width. Applying the generic bound to Euclidean norm
balls recovers the consistency result of Bartlett et al. (2020) for
minimum-norm interpolators, and confirms a prediction of Zhou et al. (2020) for
near-minimal-norm interpolators in the special case of Gaussian data. We
demonstrate the generality of the bound by applying it to the simplex,
obtaining a novel consistency result for minimum l1-norm interpolators (basis
pursuit). Our results show how norm-based generalization bounds can explain and
be used to analyze benign overfitting, at least in some settings."
"['stat.ML', 'stat.TH']",Unsuitability of NOTEARS for Causal Graph Discovery,"Causal Discovery methods aim to identify a DAG structure that represents
causal relationships from observational data. In this article, we stress that
it is important to test such methods for robustness in practical settings. As
our main example, we analyze the NOTEARS method, for which we demonstrate a
lack of scale-invariance. We show that NOTEARS is a method that aims to
identify a parsimonious DAG from the data that explains the residual variance.
We conclude that NOTEARS is not suitable for identifying truly causal
relationships from the data."
"['stat.ML', 'stat.ME']",Significance tests of feature relevance for a blackbox learner,"An exciting recent development is the uptake of deep learning in many
scientific fields, where the objective is seeking novel scientific insights and
discoveries. To interpret a learning outcome, researchers perform hypothesis
testing for explainable features to advance scientific domain knowledge. In
such a situation, testing for a blackbox learner poses a severe challenge
because of intractable models, unknown limiting distributions of parameter
estimates, and high computational constraints. In this article, we derive two
consistent tests for the feature relevance of a blackbox learner. The first one
evaluates a loss difference with perturbation on an inference sample, which is
independent of an estimation sample used for parameter estimation in model
fitting. The second further splits the inference sample into two but does not
require data perturbation. Also, we develop their combined versions by
aggregating the order statistics of the $p$-values based on repeated sample
splitting. To estimate the splitting ratio and the perturbation size, we
develop adaptive splitting schemes for suitably controlling the Type \rom{1}
error subject to computational constraints. By deflating the
\textit{bias-sd-ratio}, we establish asymptotic null distributions of the test
statistics and their consistency in terms of statistical power. Our theoretical
power analysis and simulations indicate that the one-split test is more
powerful than the two-split test, though the latter is easier to apply for
large datasets. Moreover, the combined tests are more stable while compensating
for a power loss by repeated sample splitting. Numerically, we demonstrate the
utility of the proposed tests on two benchmark examples. Accompanying this
paper is our Python library {\tt dnn-inference}
https://dnn-inference.readthedocs.io/en/latest/ that implements the proposed
tests."
['stat.TH'],Why does CTC result in peaky behavior?,"The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence."
"['stat.ML', 'stat.CO']",GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions,"The lack of interpretability is an inevitable problem when using neural
network models in real applications. In this paper, an explainable neural
network based on generalized additive models with structured interactions
(GAMI-Net) is proposed to pursue a good balance between prediction accuracy and
model interpretability. GAMI-Net is a disentangled feedforward network with
multiple additive subnetworks; each subnetwork consists of multiple hidden
layers and is designed for capturing one main effect or one pairwise
interaction. Three interpretability aspects are further considered, including
a) sparsity, to select the most significant effects for parsimonious
representations; b) heredity, a pairwise interaction could only be included
when at least one of its parent main effects exists; and c) marginal clarity,
to make main effects and pairwise interactions mutually distinguishable. An
adaptive training algorithm is developed, where main effects are first trained
and then pairwise interactions are fitted to the residuals. Numerical
experiments on both synthetic functions and real-world datasets show that the
proposed model enjoys superior interpretability and it maintains competitive
prediction accuracy in comparison to the explainable boosting machine and other
classic machine learning models."
['stat.TH'],Statistical optimality conditions for compressive ensembles,"We present a framework for the theoretical analysis of ensembles of
low-complexity empirical risk minimisers trained on independent random
compressions of high-dimensional data. First we introduce a general
distribution-dependent upper-bound on the excess risk, framed in terms of a
natural notion of compressibility. This bound is independent of the dimension
of the original data representation, and explains the in-built regularisation
effect of the compressive approach. We then instantiate this general bound to
classification and regression tasks, considering Johnson-Lindenstrauss mappings
as the compression scheme. For each of these tasks, our strategy is to develop
a tight upper bound on the compressibility function, and by doing so we
discover distributional conditions of geometric nature under which the
compressive algorithm attains minimax-optimal rates up to at most
poly-logarithmic factors. In the case of compressive classification, this is
achieved with a mild geometric margin condition along with a flexible moment
condition that is significantly more general than the assumption of bounded
domain. In the case of regression with strongly convex smooth loss functions we
find that compressive regression is capable of exploiting spectral decay with
near-optimal guarantees. In addition, a key ingredient for our central upper
bound is a high probability uniform upper bound on the integrated deviation of
dependent empirical processes, which may be of independent interest."
['stat.TH'],Demystification of Few-shot and One-shot Learning,"Few-shot and one-shot learning have been the subject of active and intensive
research in recent years, with mounting evidence pointing to successful
implementation and exploitation of few-shot learning algorithms in practice.
Classical statistical learning theories do not fully explain why few- or
one-shot learning is at all possible since traditional generalisation bounds
normally require large training and testing samples to be meaningful. This
sharply contrasts with numerous examples of successful one- and few-shot
learning systems and applications.
  In this work we present mathematical foundations for a theory of one-shot and
few-shot learning and reveal conditions specifying when such learning schemes
are likely to succeed. Our theory is based on intrinsic properties of
high-dimensional spaces. We show that if the ambient or latent decision space
of a learning machine is sufficiently high-dimensional than a large class of
objects in this space can indeed be easily learned from few examples provided
that certain data non-concentration conditions are met."
"['stat.ML', 'stat.TH']",Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data,"This study investigates the theoretical foundations of t-distributed
stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction
and data visualization method. A novel theoretical framework for the analysis
of t-SNE based on the gradient descent approach is presented. For the early
exaggeration stage of t-SNE, we show its asymptotic equivalence to a power
iteration based on the underlying graph Laplacian, characterize its limiting
behavior, and uncover its deep connection to Laplacian spectral clustering, and
fundamental principles including early stopping as implicit regularization. The
results explain the intrinsic mechanism and the empirical benefits of such a
computational strategy. For the embedding stage of t-SNE, we characterize the
kinematics of the low-dimensional map throughout the iterations, and identify
an amplification phase, featuring the intercluster repulsion and the expansive
behavior of the low-dimensional map. The general theory explains the fast
convergence rate and the exceptional empirical performance of t-SNE for
visualizing clustered data, brings forth the interpretations of the t-SNE
output, and provides theoretical guidance for selecting tuning parameters in
various applications."
"['stat.ML', 'stat.ME']",Increasing the efficiency of randomized trial estimates via linear adjustment for a prognostic score,"Estimating causal effects from randomized experiments is central to clinical
research. Reducing the statistical uncertainty in these analyses is an
important objective for statisticians. Registries, prior trials, and health
records constitute a growing compendium of historical data on patients under
standard-of-care conditions that may be exploitable to this end. However, most
methods for historical borrowing achieve reductions in variance by sacrificing
strict type-I error rate control. Here, we propose a use of historical data
that exploits linear covariate adjustment to improve the efficiency of trial
analyses without incurring bias. Specifically, we train a prognostic model on
the historical data, then estimate the treatment effect using a linear
regression while adjusting for the trial subjects' predicted outcomes (their
prognostic scores). We prove that, under certain conditions, this prognostic
covariate adjustment procedure attains the minimum variance possible among a
large class of estimators. When those conditions are not met, prognostic
covariate adjustment is still more efficient than raw covariate adjustment and
the gain in efficiency is proportional to a measure of the predictive accuracy
of the prognostic model. We demonstrate the approach using simulations and a
reanalysis of an Alzheimer's Disease clinical trial and observe meaningful
reductions in mean-squared error and the estimated variance. Lastly, we provide
a simplified formula for asymptotic variance that enables power and sample size
calculations that account for the gains from the prognostic model for clinical
trial design. Sample size reductions between 10% and 30% are attainable when
using prognostic models that explain a clinically realistic percentage of the
outcome variance."
"['stat.ML', 'stat.TH']",Fast Global Convergence for Low-rank Matrix Recovery via Riemannian Gradient Descent with Random Initialization,"In this paper, we propose a new global analysis framework for a class of
low-rank matrix recovery problems on the Riemannian manifold. We analyze the
global behavior for the Riemannian optimization with random initialization. We
use the Riemannian gradient descent algorithm to minimize a least squares loss
function, and study the asymptotic behavior as well as the exact convergence
rate. We reveal a previously unknown geometric property of the low-rank matrix
manifold, which is the existence of spurious critical points for the simple
least squares function on the manifold. We show that under some assumptions,
the Riemannian gradient descent starting from a random initialization with high
probability avoids these spurious critical points and only converges to the
ground truth in nearly linear convergence rate, i.e.
$\mathcal{O}(\text{log}(\frac{1}{\epsilon})+ \text{log}(n))$ iterations to
reach an $\epsilon$-accurate solution. We use two applications as examples for
our global analysis. The first one is a rank-1 matrix recovery problem. The
second one is a generalization of the Gaussian phase retrieval problem. It only
satisfies the weak isometry property, but has behavior similar to that of the
first one except for an extra saddle set. Our convergence guarantee is nearly
optimal and almost dimension-free, which fully explains the numerical
observations. The global analysis can be potentially extended to other data
problems with random measurement structures and empirical least squares loss
functions."
"['stat.ML', 'stat.ME']",sJIVE: Supervised Joint and Individual Variation Explained,"Analyzing multi-source data, which are multiple views of data on the same
subjects, has become increasingly common in molecular biomedical research.
Recent methods have sought to uncover underlying structure and relationships
within and/or between the data sources, and other methods have sought to build
a predictive model for an outcome using all sources. However, existing methods
that do both are presently limited because they either (1) only consider data
structure shared by all datasets while ignoring structures unique to each
source, or (2) they extract underlying structures first without consideration
to the outcome. We propose a method called supervised joint and individual
variation explained (sJIVE) that can simultaneously (1) identify shared (joint)
and source-specific (individual) underlying structure and (2) build a linear
prediction model for an outcome using these structures. These two components
are weighted to compromise between explaining variation in the multi-source
data and in the outcome. Simulations show sJIVE to outperform existing methods
when large amounts of noise are present in the multi-source data. An
application to data from the COPDGene study reveals gene expression and
proteomic patterns that are predictive of lung function. Functions to perform
sJIVE are included in the R.JIVE package, available online at
http://github.com/lockEF/r.jive ."
"['stat.ML', 'stat.TH']",Dimension Free Generalization Bounds for Non Linear Metric Learning,"In this work we study generalization guarantees for the metric learning
problem, where the metric is induced by a neural network type embedding of the
data. Specifically, we provide uniform generalization bounds for two regimes --
the sparse regime, and a non-sparse regime which we term \emph{bounded
amplification}. The sparse regime bounds correspond to situations where
$\ell_1$-type norms of the parameters are small. Similarly to the situation in
classification, solutions satisfying such bounds can be obtained by an
appropriate regularization of the problem. On the other hand, unregularized SGD
optimization of a metric learning loss typically does not produce sparse
solutions. We show that despite this lack of sparsity, by relying on a
different, new property of the solutions, it is still possible to provide
dimension free generalization guarantees. Consequently, these bounds can
explain generalization in non sparse real experimental situations. We
illustrate the studied phenomena on the MNIST and 20newsgroups datasets."
"['stat.ML', 'stat.TH']",Fixing an error in Caponnetto and de Vito (2007),"The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal
rates for kernel ridge regression in a very general setting. Its proof,
however, contains an error in its bound on the effective dimensionality. In
this note, we explain the mistake, provide a correct bound, and show that the
main theorem remains true."
"['stat.ML', 'stat.ME']",Learning Deep Kernels for Non-Parametric Two-Sample Tests,"We propose a class of kernel-based two-sample tests, which aim to determine
whether two sets of samples are drawn from the same distribution. Our tests are
constructed from kernels parameterized by deep neural nets, trained to maximize
test power. These tests adapt to variations in distribution smoothness and
shape over space, and are especially suited to high dimensions and complex
data. By contrast, the simpler kernels used in prior kernel testing work are
spatially homogeneous, and adaptive only in lengthscale. We explain how this
scheme includes popular classifier-based two-sample tests as a special case,
but improves on them in general. We provide the first proof of consistency for
the proposed adaptation method, which applies both to kernels on deep features
and to simpler radial basis kernels or multiple kernel learning. In
experiments, we establish the superior performance of our deep kernels in
hypothesis testing on benchmark and real-world data. The code of our
deep-kernel-based two sample tests is available at
https://github.com/fengliu90/DK-for-TST."
"['stat.ML', 'stat.TH']",Mathematical Models of Overparameterized Neural Networks,"Deep learning has received considerable empirical successes in recent years.
However, while many ad hoc tricks have been discovered by practitioners, until
recently, there has been a lack of theoretical understanding for tricks
invented in the deep learning literature. Known by practitioners that
overparameterized neural networks are easy to learn, in the past few years
there have been important theoretical developments in the analysis of
overparameterized neural networks. In particular, it was shown that such
systems behave like convex systems under various restricted settings, such as
for two-layer NNs, and when learning is restricted locally in the so-called
neural tangent kernel space around specialized initializations. This paper
discusses some of these recent progresses leading to significant better
understanding of neural networks. We will focus on the analysis of two-layer
neural networks, and explain the key mathematical models, with their
algorithmic implications. We will then discuss challenges in understanding deep
neural networks and some current research directions."
['stat.OT'],A Survey on the Visual Perceptions of Gaussian Noise Filtering on Photography,"Statisticians, as well as machine learning and computer vision experts, have
been studying image reconstitution through denoising different domains of
photography, such as textual documentation, tomographic, astronomical, and
low-light photography. In this paper, we apply common inferential kernel
filters in the R and python languages, as well as Adobe Lightroom's denoise
filter, and compare their effectiveness in removing noise from JPEG images. We
ran standard benchmark tests to evaluate each method's effectiveness for
removing noise. In doing so, we also surveyed students at Elon University about
their opinion of a single filtered photo from a collection of photos processed
by the various filter methods. Many scientists believe that noise filters cause
blurring and image quality loss so we analyzed whether or not people felt as
though denoising causes any quality loss as compared to their noiseless images.
Individuals assigned scores indicating the image quality of a denoised photo
compared to its noiseless counterpart on a 1 to 10 scale. Survey scores are
compared across filters to evaluate whether there were significant differences
in image quality scores received. Benchmark scores were compared to the visual
perception scores. Then, an analysis of covariance test was run to identify
whether or not survey training scores explained any unplanned variation in
visual scores assigned by students across the filter methods."
"['stat.ML', 'stat.CO', 'stat.ME']",Categorical exploratory data analysis on goodness-of-fit issues,"If the aphorism ""All models are wrong""- George Box, continues to be true in
data analysis, particularly when analyzing real-world data, then we should
annotate this wisdom with visible and explainable data-driven patterns. Such
annotations can critically shed invaluable light on validity as well as
limitations of statistical modeling as a data analysis approach. In an effort
to avoid holding our real data to potentially unattainable or even unrealistic
theoretical structures, we propose to utilize the data analysis paradigm called
Categorical Exploratory Data Analysis (CEDA). We illustrate the merits of this
proposal with two real-world data sets from the perspective of goodness-of-fit.
In both data sets, the Normal distribution's bell shape seemingly fits rather
well by first glance. We apply CEDA to bring out where and how each data fits
or deviates from the model shape via several important distributional aspects.
We also demonstrate that CEDA affords a version of tree-based p-value, and
compare it with p-values based on traditional statistical approaches. Along our
data analysis, we invest computational efforts in making graphic display to
illuminate the advantages of using CEDA as one primary way of data analysis in
Data Science education."
"['stat.ML', 'stat.TH']",A Theory of Universal Learning,"How quickly can a given class of concepts be learned from examples? It is
common to measure the performance of a supervised machine learning algorithm by
plotting its ""learning curve"", that is, the decay of the error rate as a
function of the number of training examples. However, the classical theoretical
framework for understanding learnability, the PAC model of Vapnik-Chervonenkis
and Valiant, does not explain the behavior of learning curves: the
distribution-free PAC model of learning can only bound the upper envelope of
the learning curves over all possible data distributions. This does not match
the practice of machine learning, where the data source is typically fixed in
any given scenario, while the learner may choose the number of training
examples on the basis of factors such as computational resources and desired
accuracy.
  In this paper, we study an alternative learning model that better captures
such practical aspects of machine learning, but still gives rise to a complete
theory of the learnable in the spirit of the PAC model. More precisely, we
consider the problem of universal learning, which aims to understand the
performance of learning algorithms on every data distribution, but without
requiring uniformity over the distribution. The main result of this paper is a
remarkable trichotomy: there are only three possible rates of universal
learning. More precisely, we show that the learning curves of any given concept
class decay either at an exponential, linear, or arbitrarily slow rates.
Moreover, each of these cases is completely characterized by appropriate
combinatorial parameters, and we exhibit optimal learning algorithms that
achieve the best possible rate in each case.
  For concreteness, we consider in this paper only the realizable case, though
analogous results are expected to extend to more general learning scenarios."
"['stat.ML', 'stat.TH']",A Group-Theoretic Framework for Data Augmentation,"Data augmentation is a widely used trick when training deep neural networks:
in addition to the original data, properly transformed data are also added to
the training set. However, to the best of our knowledge, a clear mathematical
framework to explain the performance benefits of data augmentation is not
available. In this paper, we develop such a theoretical framework. We show data
augmentation is equivalent to an averaging operation over the orbits of a
certain group that keeps the data distribution approximately invariant. We
prove that it leads to variance reduction. We study empirical risk
minimization, and the examples of exponential families, linear regression, and
certain two-layer neural networks. We also discuss how data augmentation could
be used in problems with symmetry where other approaches are prevalent, such as
in cryo-electron microscopy (cryo-EM)."
"['stat.CO', 'stat.ME', 'stat.ML']",Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial Sampling,"The fate of scientific hypotheses often relies on the ability of a
computational model to explain the data, quantified in modern statistical
approaches by the likelihood function. The log-likelihood is the key element
for parameter estimation and model evaluation. However, the log-likelihood of
complex models in fields such as computational biology and neuroscience is
often intractable to compute analytically or numerically. In those cases,
researchers can often only estimate the log-likelihood by comparing observed
data with synthetic observations generated by model simulations. Standard
techniques to approximate the likelihood via simulation either use summary
statistics of the data or are at risk of producing severe biases in the
estimate. Here, we explore another method, inverse binomial sampling (IBS),
which can estimate the log-likelihood of an entire data set efficiently and
without bias. For each observation, IBS draws samples from the simulator model
until one matches the observation. The log-likelihood estimate is then a
function of the number of samples drawn. The variance of this estimator is
uniformly bounded, achieves the minimum variance for an unbiased estimator, and
we can compute calibrated estimates of the variance. We provide theoretical
arguments in favor of IBS and an empirical assessment of the method for
maximum-likelihood estimation with simulation-based models. As case studies, we
take three model-fitting problems of increasing complexity from computational
and cognitive neuroscience. In all problems, IBS generally produces lower error
in the estimated parameters and maximum log-likelihood values than alternative
sampling methods with the same average number of samples. Our results
demonstrate the potential of IBS as a practical, robust, and easy to implement
method for log-likelihood evaluation when exact techniques are not available."
"['stat.ML', 'stat.TH']",Precise Statistical Analysis of Classification Accuracies for Adversarial Training,"Despite the wide empirical success of modern machine learning algorithms and
models in a multitude of applications, they are known to be highly susceptible
to seemingly small indiscernible perturbations to the input data known as
adversarial attacks. A variety of recent adversarial training procedures have
been proposed to remedy this issue. Despite the success of such procedures at
increasing accuracy on adversarially perturbed inputs or robust accuracy, these
techniques often reduce accuracy on natural unperturbed inputs or standard
accuracy. Complicating matters further the effect and trend of adversarial
training procedures on standard and robust accuracy is rather counter intuitive
and radically dependent on a variety of factors including the perceived form of
the perturbation during training, size/quality of data, model
overparameterization, etc. In this paper we focus on binary classification
problems where the data is generated according to the mixture of two Gaussians
with general anisotropic covariance matrices and derive a precise
characterization of the standard and robust accuracy for a class of minimax
adversarially trained models. We consider a general norm-based adversarial
model, where the adversary can add perturbations of bounded $\ell_p$ norm to
each input data, for an arbitrary $p\ge 1$. Our comprehensive analysis allows
us to theoretically explain several intriguing empirical phenomena and provide
a precise understanding of the role of different problem parameters on standard
and robust accuracies."
"['stat.ML', 'stat.TH']",Consistent Feature Selection for Analytic Deep Neural Networks,"One of the most important steps toward interpretability and explainability of
neural network models is feature selection, which aims to identify the subset
of relevant features. Theoretical results in the field have mostly focused on
the prediction aspect of the problem with virtually no work on feature
selection consistency for deep neural networks due to the model's severe
nonlinearity and unidentifiability. This lack of theoretical foundation casts
doubt on the applicability of deep learning to contexts where correct
interpretations of the features play a central role.
  In this work, we investigate the problem of feature selection for analytic
deep networks. We prove that for a wide class of networks, including deep
feed-forward neural networks, convolutional neural networks, and a major
sub-class of residual neural networks, the Adaptive Group Lasso selection
procedure with Group Lasso as the base estimator is selection-consistent. The
work provides further evidence that Group Lasso might be inefficient for
feature selection with neural networks and advocates the use of Adaptive Group
Lasso over the popular Group Lasso."
"['stat.ML', 'stat.ME']","Transparency, Auditability and eXplainability of Machine Learning Models in Credit Scoring","A major requirement for credit scoring models is to provide a maximally
accurate risk prediction. Additionally, regulators demand these models to be
transparent and auditable. Thus, in credit scoring, very simple predictive
models such as logistic regression or decision trees are still widely used and
the superior predictive power of modern machine learning algorithms cannot be
fully leveraged. Significant potential is therefore missed, leading to higher
reserves or more credit defaults. This paper works out different dimensions
that have to be considered for making credit scoring models understandable and
presents a framework for making ``black box'' machine learning models
transparent, auditable and explainable. Following this framework, we present an
overview of techniques, demonstrate how they can be applied in credit scoring
and how results compare to the interpretability of score cards. A real world
case study shows that a comparable degree of interpretability can be achieved
while machine learning techniques keep their ability to improve predictive
power."
"['stat.ML', 'stat.TH']",Predictive and Causal Implications of using Shapley Value for Model Interpretation,"Shapley value is a concept from game theory. Recently, it has been used for
explaining complex models produced by machine learning techniques. Although the
mathematical definition of Shapley value is straight-forward, the implication
of using it as a model interpretation tool is yet to be described. In the
current paper, we analyzed Shapley value in the Bayesian network framework. We
established the relationship between Shapley value and conditional
independence, a key concept in both predictive and causal modeling. Our results
indicate that, eliminating a variable with high Shapley value from a model do
not necessarily impair predictive performance, whereas eliminating a variable
with low Shapley value from a model could impair performance. Therefore, using
Shapley value for feature selection do not result in the most parsimonious and
predictively optimal model in the general case. More importantly, Shapley value
of a variable do not reflect their causal relationship with the target of
interest."
"['stat.ML', 'stat.TH']",Variational Bayes under Model Misspecification,"Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo
(MCMC) for Bayesian posterior inference. Though popular, VB comes with few
theoretical guarantees, most of which focus on well-specified models. However,
models are rarely well-specified in practice. In this work, we study VB under
model misspecification. We prove the VB posterior is asymptotically normal and
centers at the value that minimizes the Kullback-Leibler (KL) divergence to the
true data-generating distribution. Moreover, the VB posterior mean centers at
the same value and is also asymptotically normal. These results generalize the
variational Bernstein--von Mises theorem [29] to misspecified models. As a
consequence of these results, we find that the model misspecification error
dominates the variational approximation error in VB posterior predictive
distributions. It explains the widely observed phenomenon that VB achieves
comparable predictive accuracy with MCMC even though VB uses an approximating
family. As illustrations, we study VB under three forms of model
misspecification, ranging from model over-/under-dispersion to latent
dimensionality misspecification. We conduct two simulation studies that
demonstrate the theoretical results."
"['stat.ME', 'stat.ML']",Variable Selection with Copula Entropy,"Variable selection is of significant importance for classification and
regression tasks in machine learning and statistical applications where both
predictability and explainability are needed. In this paper, a Copula Entropy
(CE) based method for variable selection which use CE based ranks to select
variables is proposed. The method is both model-free and tuning-free.
Comparison experiments between the proposed method and traditional variable
selection methods, such as Distance Correlation, Hilbert-Schmidt Independence
Criterion, Stepwise Selection, regularized generalized linear models and
Adaptive LASSO, were conducted on the UCI heart disease data. Experimental
results show that CE based method can select the `right' variables out more
effectively and derive better interpretable results than traditional methods do
without sacrificing accuracy performance. It is believed that CE based variable
selection can help to build more explainable models."
"['stat.CO', 'stat.ML']",SBI -- A toolkit for simulation-based inference,"Scientists and engineers employ stochastic numerical simulators to model
empirically observed phenomena. In contrast to purely statistical models,
simulators express scientific principles that provide powerful inductive
biases, improve generalization to new data or scenarios and allow for fewer,
more interpretable and domain-relevant parameters. Despite these advantages,
tuning a simulator's parameters so that its outputs match data is challenging.
Simulation-based inference (SBI) seeks to identify parameter sets that a) are
compatible with prior knowledge and b) match empirical observations.
Importantly, SBI does not seek to recover a single 'best' data-compatible
parameter set, but rather to identify all high probability regions of parameter
space that explain observed data, and thereby to quantify parameter
uncertainty. In Bayesian terminology, SBI aims to retrieve the posterior
distribution over the parameters of interest. In contrast to conventional
Bayesian inference, SBI is also applicable when one can run model simulations,
but no formula or algorithm exists for evaluating the probability of data given
parameters, i.e. the likelihood. We present $\texttt{sbi}$, a PyTorch-based
package that implements SBI algorithms based on neural networks. $\texttt{sbi}$
facilitates inference on black-box simulators for practising scientists and
engineers by providing a unified interface to state-of-the-art algorithms
together with documentation and tutorials."
"['stat.ML', 'stat.CO']",How Good is the Bayes Posterior in Deep Neural Networks Really?,"During the past five years the Bayesian deep learning community has developed
increasingly accurate and efficient approximate inference procedures that allow
for Bayesian inference in deep neural networks. However, despite this
algorithmic progress and the promise of improved uncertainty quantification and
sample efficiency there are---as of early 2020---no publicized deployments of
Bayesian neural networks in industrial practice. In this work we cast doubt on
the current understanding of Bayes posteriors in popular deep neural networks:
we demonstrate through careful MCMC sampling that the posterior predictive
induced by the Bayes posterior yields systematically worse predictions compared
to simpler methods including point estimates obtained from SGD. Furthermore, we
demonstrate that predictive performance is improved significantly through the
use of a ""cold posterior"" that overcounts evidence. Such cold posteriors
sharply deviate from the Bayesian paradigm but are commonly used as heuristic
in Bayesian deep learning papers. We put forward several hypotheses that could
explain cold posteriors and evaluate the hypotheses through experiments. Our
work questions the goal of accurate posterior approximations in Bayesian deep
learning: If the true Bayes posterior is poor, what is the use of more accurate
approximations? Instead, we argue that it is timely to focus on understanding
the origin of the improved performance of cold posteriors."
"['stat.ML', 'stat.TH']",Consistency of Anchor-based Spectral Clustering,"Anchor-based techniques reduce the computational complexity of spectral
clustering algorithms. Although empirical tests have shown promising results,
there is currently a lack of theoretical support for the anchoring approach. We
define a specific anchor-based algorithm and show that it is amenable to
rigorous analysis, as well as being effective in practice. We establish the
theoretical consistency of the method in an asymptotic setting where data is
sampled from an underlying continuous probability distribution. In particular,
we provide sharp asymptotic conditions for the algorithm parameters which
ensure that the anchor-based method can recover with high probability disjoint
clusters that are mutually separated by a positive distance. We illustrate the
performance of the algorithm on synthetic data and explain how the theoretical
convergence analysis can be used to inform the practical choice of parameter
scalings. We also test the accuracy and efficiency of the algorithm on two
large scale real data sets. We find that the algorithm offers clear advantages
over standard spectral clustering. We also find that it is competitive with the
state-of-the-art LSC method of Chen and Cai (Twenty-Fifth AAAI Conference on
Artificial Intelligence, 2011), while having the added benefit of a consistency
guarantee."
"['stat.ME', 'stat.ML']",Counterfactual Explanations of Concept Drift,"The notion of concept drift refers to the phenomenon that the distribution,
which is underlying the observed data, changes over time; as a consequence
machine learning models may become inaccurate and need adjustment. While there
do exist methods to detect concept drift or to adjust models in the presence of
observed drift, the question of explaining drift has hardly been considered so
far. This problem is of importance, since it enables an inspection of the most
prominent features where drift manifests itself; hence it enables human
understanding of the necessity of change and it increases acceptance of
life-long learning models. In this paper we present a novel technology, which
characterizes concept drift in terms of the characteristic change of spatial
features represented by typical examples based on counterfactual explanations.
We establish a formal definition of this problem, derive an efficient
algorithmic solution based on counterfactual explanations, and demonstrate its
usefulness in several examples."
"['stat.ML', 'stat.CO']",From unbiased MDI Feature Importance to Explainable AI for Trees,"We attempt to give a unifying view of the various recent attempts to (i)
improve the interpretability of tree-based models and (ii) debias the the
default variable-importance measure in random Forests, Gini importance. In
particular, we demonstrate a common thread among the out-of-bag based bias
correction methods and their connection to local explanation for trees. In
addition, we point out a bias caused by the inclusion of inbag data in the
newly developed explainable AI for trees algorithms."
"['stat.ML', 'stat.ME']",Explaining individual predictions when features are dependent: More accurate approximations to Shapley values,"Explaining complex or seemingly simple machine learning models is an
important practical problem. We want to explain individual predictions from a
complex machine learning model by learning simple, interpretable explanations.
Shapley values is a game theoretic concept that can be used for this purpose.
The Shapley value framework has a series of desirable theoretical properties,
and can in principle handle any predictive model. Kernel SHAP is a
computationally efficient approximation to Shapley values in higher dimensions.
Like several other existing methods, this approach assumes that the features
are independent, which may give very wrong explanations. This is the case even
if a simple linear model is used for predictions. In this paper, we extend the
Kernel SHAP method to handle dependent features. We provide several examples of
linear and non-linear models with various degrees of feature dependence, where
our method gives more accurate approximations to the true Shapley values. We
also propose a method for aggregating individual Shapley values, such that the
prediction can be explained by groups of dependent variables."
"['stat.ML', 'stat.TH']",Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model,"Gradient-based algorithms are effective for many machine learning tasks, but
despite ample recent effort and some progress, it often remains unclear why
they work in practice in optimising high-dimensional non-convex functions and
why they find good minima instead of being trapped in spurious ones.
  Here we present a quantitative theory explaining this behaviour in a spiked
matrix-tensor model.
  Our framework is based on the Kac-Rice analysis of stationary points and a
closed-form analysis of gradient-flow originating from statistical physics. We
show that there is a well defined region of parameters where the gradient-flow
algorithm finds a good global minimum despite the presence of exponentially
many spurious local minima.
  We show that this is achieved by surfing on saddles that have strong negative
direction towards the global minima, a phenomenon that is connected to a
BBP-type threshold in the Hessian describing the critical points of the
landscapes."
"['stat.ML', 'stat.ME', 'stat.TH']",A Statistical Learning Approach to Modal Regression,"This paper studies the nonparametric modal regression problem systematically
from a statistical learning view. Originally motivated by pursuing a
theoretical understanding of the maximum correntropy criterion based regression
(MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is
essentially modal regression. We show that nonparametric modal regression
problem can be approached via the classical empirical risk minimization. Some
efforts are then made to develop a framework for analyzing and implementing
modal regression. For instance, the modal regression function is described, the
modal regression risk is defined explicitly and its \textit{Bayes} rule is
characterized; for the sake of computational tractability, the surrogate modal
regression risk, which is termed as the generalization risk in our study, is
introduced. On the theoretical side, the excess modal regression risk, the
excess generalization risk, the function estimation error, and the relations
among the above three quantities are studied rigorously. It turns out that
under mild conditions, function estimation consistency and convergence may be
pursued in modal regression as in vanilla regression protocols, such as mean
regression, median regression, and quantile regression. However, it outperforms
these regression models in terms of robustness as shown in our study from a
re-descending M-estimation view. This coincides with and in return explains the
merits of MCCR on robustness. On the practical side, the implementation issues
of modal regression including the computational algorithm and the tuning
parameters selection are discussed. Numerical assessments on modal regression
are also conducted to verify our findings empirically."
"['stat.ME', 'stat.ML']",Robust Gaussian Process Regression with a Bias Model,"This paper presents a new approach to a robust Gaussian process (GP)
regression. Most existing approaches replace an outlier-prone Gaussian
likelihood with a non-Gaussian likelihood induced from a heavy tail
distribution, such as the Laplace distribution and Student-t distribution.
However, the use of a non-Gaussian likelihood would incur the need for a
computationally expensive Bayesian approximate computation in the posterior
inferences. The proposed approach models an outlier as a noisy and biased
observation of an unknown regression function, and accordingly, the likelihood
contains bias terms to explain the degree of deviations from the regression
function. We entail how the biases can be estimated accurately with other
hyperparameters by a regularized maximum likelihood estimation. Conditioned on
the bias estimates, the robust GP regression can be reduced to a standard GP
regression problem with analytical forms of the predictive mean and variance
estimates. Therefore, the proposed approach is simple and very computationally
attractive. It also gives a very robust and accurate GP estimate for many
tested scenarios. For the numerical evaluation, we perform a comprehensive
simulation study to evaluate the proposed approach with the comparison to the
existing robust GP approaches under various simulated scenarios of different
outlier proportions and different noise levels. The approach is applied to data
from two measurement systems, where the predictors are based on robust
environmental parameter measurements and the response variables utilize more
complex chemical sensing methods that contain a certain percentage of outliers.
The utility of the measurement systems and value of the environmental data are
improved through the computationally efficient GP regression and bias model."
"['stat.ML', 'stat.ME']",GP-ALPS: Automatic Latent Process Selection for Multi-Output Gaussian Process Models,"A simple and widely adopted approach to extend Gaussian processes (GPs) to
multiple outputs is to model each output as a linear combination of a
collection of shared, unobserved latent GPs. An issue with this approach is
choosing the number of latent processes and their kernels. These choices are
typically done manually, which can be time consuming and prone to human biases.
We propose Gaussian Process Automatic Latent Process Selection (GP-ALPS), which
automatically chooses the latent processes by turning off those that do not
meaningfully contribute to explaining the data. We develop a variational
inference scheme, assess the quality of the variational posterior by comparing
it against the gold standard MCMC, and demonstrate the suitability of GP-ALPS
in a set of preliminary experiments."
"['stat.ML', 'stat.CO', 'stat.OT']",Item Response Theory based Ensemble in Machine Learning,"In this article, we propose a novel probabilistic framework to improve the
accuracy of a weighted majority voting algorithm. In order to assign higher
weights to the classifiers which can correctly classify hard-to-classify
instances, we introduce the Item Response Theory (IRT) framework to evaluate
the samples' difficulty and classifiers' ability simultaneously. Three models
are created with different assumptions suitable for different cases. When
making an inference, we keep a balance between the accuracy and complexity. In
our experiment, all the base models are constructed by single trees via
bootstrap. To explain the models, we illustrate how the IRT ensemble model
constructs the classifying boundary. We also compare their performance with
other widely used methods and show that our model performs well on 19 datasets."
"['stat.ML', 'stat.ME']",Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise,"We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our proposed general noise model
allows to perform ICA in settings where other noisy ICA procedures fail.
Additionally, it can be used for applications with grouped data by adjusting
for different stationary noise within each group. Our proposed noise model has
a natural relation to causality and we explain how it can be applied in the
context of causal inference. In addition to our theoretical framework, we
provide an efficient estimation procedure and prove identifiability of the
unmixing matrix under mild assumptions. Finally, we illustrate the performance
and robustness of our method on simulated data, provide audible and visual
examples, and demonstrate the applicability to real-world scenarios by
experiments on publicly available Antarctic ice core data as well as two EEG
data sets. We provide a scikit-learn compatible pip-installable Python package
coroICA as well as R and Matlab implementations accompanied by a documentation
at https://sweichwald.de/coroICA/"
"['stat.ML', 'stat.ME']",NLS: an accurate and yet easy-to-interpret regression method,"An important feature of successful supervised machine learning applications
is to be able to explain the predictions given by the regression or
classification model being used. However, most state-of-the-art models that
have good predictive power lead to predictions that are hard to interpret.
Thus, several model-agnostic interpreters have been developed recently as a way
of explaining black-box classifiers. In practice, using these methods is a slow
process because a novel fitting is required for each new testing instance, and
several non-trivial choices must be made. We develop NLS (neural local
smoother), a method that is complex enough to give good predictions, and yet
gives solutions that are easy to be interpreted without the need of using a
separate interpreter. The key idea is to use a neural network that imposes a
local linear shape to the output layer. We show that NLS leads to predictive
power that is comparable to state-of-the-art machine learning models, and yet
is easier to interpret."
"['stat.ML', 'stat.TH']",Noisy Matrix Completion: Understanding Statistical Guarantees for Convex Relaxation via Nonconvex Optimization,"This paper studies noisy low-rank matrix completion: given partial and noisy
entries of a large low-rank matrix, the goal is to estimate the underlying
matrix faithfully and efficiently. Arguably one of the most popular paradigms
to tackle this problem is convex relaxation, which achieves remarkable efficacy
in practice. However, the theoretical support of this approach is still far
from optimal in the noisy setting, falling short of explaining its empirical
success.
  We make progress towards demystifying the practical efficacy of convex
relaxation vis-\`a-vis random noise. When the rank and the condition number of
the unknown matrix are bounded by a constant, we demonstrate that the convex
programming approach achieves near-optimal estimation errors --- in terms of
the Euclidean loss, the entrywise loss, and the spectral norm loss --- for a
wide range of noise levels. All of this is enabled by bridging convex
relaxation with the nonconvex Burer-Monteiro approach, a seemingly distinct
algorithmic paradigm that is provably robust against noise. More specifically,
we show that an approximate critical point of the nonconvex formulation serves
as an extremely tight approximation of the convex solution, thus allowing us to
transfer the desired statistical guarantees of the nonconvex approach to its
convex counterpart."
"['stat.ML', 'stat.CO']",On the Insufficiency of the Large Margins Theory in Explaining the Performance of Ensemble Methods,"Boosting and other ensemble methods combine a large number of weak
classifiers through weighted voting to produce stronger predictive models. To
explain the successful performance of boosting algorithms, Schapire et al.
(1998) showed that AdaBoost is especially effective at increasing the margins
of the training data. Schapire et al. (1998) also developed an upper bound on
the generalization error of any ensemble based on the margins of the training
data, from which it was concluded that larger margins should lead to lower
generalization error, everything else being equal (sometimes referred to as the
``large margins theory''). Tighter bounds have been derived and have reinforced
the large margins theory hypothesis. For instance, Wang et al. (2011) suggest
that specific margin instances, such as the equilibrium margin, can better
summarize the margins distribution. These results have led many researchers to
consider direct optimization of the margins to improve ensemble generalization
error with mixed results. We show that the large margins theory is not
sufficient for explaining the performance of voting classifiers. We do this by
illustrating how it is possible to improve upon the margin distribution of an
ensemble solution, while keeping the complexity fixed, yet not improve the test
set performance."
"['stat.ML', 'stat.CO']",On the Current State of Research in Explaining Ensemble Performance Using Margins,"Empirical evidence shows that ensembles, such as bagging, boosting, random
and rotation forests, generally perform better in terms of their generalization
error than individual classifiers. To explain this performance, Schapire et al.
(1998) developed an upper bound on the generalization error of an ensemble
based on the margins of the training data, from which it was concluded that
larger margins should lead to lower generalization error, everything else being
equal. Many other researchers have backed this assumption and presented tighter
bounds on the generalization error based on either the margins or functions of
the margins. For instance, Shen and Li (2010) provide evidence suggesting that
the generalization error of a voting classifier might be reduced by increasing
the mean and decreasing the variance of the margins. In this article we propose
several techniques and empirically test whether the current state of research
in explaining ensemble performance holds. We evaluate the proposed methods
through experiments with real and simulated data sets."
"['stat.ML', 'stat.ME']",TiK-means: $K$-means clustering for skewed groups,"The $K$-means algorithm is extended to allow for partitioning of skewed
groups. Our algorithm is called TiK-Means and contributes a $K$-means type
algorithm that assigns observations to groups while estimating their
skewness-transformation parameters. The resulting groups and transformation
reveal general-structured clusters that can be explained by inverting the
estimated transformation. Further, a modification of the jump statistic chooses
the number of groups. Our algorithm is evaluated on simulated and real-life
datasets and then applied to a long-standing astronomical dispute regarding the
distinct kinds of gamma ray bursts."
"['stat.ML', 'stat.ME', 'stat.TH']",A Selective Overview of Deep Learning,"Deep learning has arguably achieved tremendous success in recent years. In
simple words, deep learning uses the composition of many nonlinear functions to
model the complex dependency between input features and labels. While neural
networks have a long history, recent advances have greatly improved their
performance in computer vision, natural language processing, etc. From the
statistical and scientific perspective, it is natural to ask: What is deep
learning? What are the new characteristics of deep learning, compared with
classical methods? What are the theoretical foundations of deep learning? To
answer these questions, we introduce common neural network models (e.g.,
convolutional neural nets, recurrent neural nets, generative adversarial nets)
and training techniques (e.g., stochastic gradient descent, dropout, batch
normalization) from a statistical point of view. Along the way, we highlight
new characteristics of deep learning (including depth and over-parametrization)
and explain their practical and theoretical benefits. We also sample recent
results on theories of deep learning, many of which are only suggestive. While
a complete understanding of deep learning remains elusive, we hope that our
perspectives and discussions serve as a stimulus for new statistical research."
"['stat.ML', 'stat.CO']",Elements of Sequential Monte Carlo,"A core problem in statistics and probabilistic machine learning is to compute
probability distributions and expectations. This is the fundamental problem of
Bayesian statistics and machine learning, which frames all inference as
expectations with respect to the posterior distribution. The key challenge is
to approximate these intractable expectations. In this tutorial, we review
sequential Monte Carlo (SMC), a random-sampling-based class of methods for
approximate inference. First, we explain the basics of SMC, discuss practical
issues, and review theoretical results. We then examine two of the main user
design choices: the proposal distributions and the so called intermediate
target distributions. We review recent results on how variational inference and
amortization can be used to learn efficient proposals and target distributions.
Next, we discuss the SMC estimate of the normalizing constant, how this can be
used for pseudo-marginal inference and inference evaluation. Throughout the
tutorial we illustrate the use of SMC on various models commonly used in
machine learning, such as stochastic recurrent neural networks, probabilistic
graphical models, and probabilistic programs."
"['stat.ML', 'stat.TH']",Understanding training and generalization in deep learning by Fourier analysis,"Background: It is still an open research area to theoretically understand why
Deep Neural Networks (DNNs)---equipped with many more parameters than training
data and trained by (stochastic) gradient-based methods---often achieve
remarkably low generalization error. Contribution: We study DNN training by
Fourier analysis. Our theoretical framework explains: i) DNN with (stochastic)
gradient-based methods often endows low-frequency components of the target
function with a higher priority during the training; ii) Small initialization
leads to good generalization ability of DNN while preserving the DNN's ability
to fit any function. These results are further confirmed by experiments of DNNs
fitting the following datasets, that is, natural images, one-dimensional
functions and MNIST dataset."
"['stat.ML', 'stat.CO']",Coordinate Descent for MCP/SCAD Penalized Least Squares Converges Linearly,"Recovering sparse signals from observed data is an important topic in
signal/imaging processing, statistics and machine learning. Nonconvex penalized
least squares have been attracted a lot of attentions since they enjoy nice
statistical properties. Computationally, coordinate descent (CD) is a workhorse
for minimizing the nonconvex penalized least squares criterion due to its
simplicity and scalability. In this work, we prove the linear convergence rate
to CD for solving MCP/SCAD penalized least squares problems."
"['stat.CO', 'stat.ML']",A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning,"This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided."
"['stat.ML', 'stat.ME']",Clustering acoustic emission data streams with sequentially appearing clusters using mixture models,"The interpretation of unlabeled acoustic emission (AE) data classically
relies on general-purpose clustering methods. While several external criteria
have been used in the past to select the hyperparameters of those algorithms,
few studies have paid attention to the development of dedicated objective
functions in clustering methods able to cope with the specificities of AE data.
We investigate how to explicitly represent clusters onsets in mixture models in
general, and in Gaussian Mixture Models (GMM) in particular. By modifying the
internal criterion of such models, we propose the first clustering method able
to provide, through parameters estimated by an expectation-maximization
procedure, information about when clusters occur (onsets), how they grow
(kinetics) and their level of activation through time. This new objective
function accommodates continuous timestamps of AE signals and, thus, their
order of occurrence. The method, called GMMSEQ, is experimentally validated to
characterize the loosening phenomenon in bolted structure under vibrations. A
comparison with three standard clustering methods on raw streaming data from
five experimental campaigns shows that GMMSEQ not only provides useful
qualitative information about the timeline of clusters, but also shows better
performance in terms of cluster characterization. In view of developing an open
acoustic emission initiative and according to the FAIR principles, the datasets
and the codes are made available to reproduce the research of this paper."
"['stat.ML', 'stat.TH']",Nonasymptotic one-and two-sample tests in high dimension with unknown covariance structure,"Let $\mathbf{X} = (X_i)_{1\leq i \leq n}$ be an i.i.d. sample of
square-integrable variables in $\mathbb{R}^d$, with common expectation $\mu$
and covariance matrix $\Sigma$, both unknown. We consider the problem of
testing if $\mu$ is $\eta$-close to zero, i.e. $\|\mu\| \leq \eta $ against
$\|\mu\| \geq (\eta + \delta)$; we also tackle the more general two-sample mean
closeness testing problem. The aim of this paper is to obtain nonasymptotic
upper and lower bounds on the minimal separation distance $\delta$ such that we
can control both the Type I and Type II errors at a given level. The main
technical tools are concentration inequalities, first for a suitable estimator
of $\|\mu\|^2$ used a test statistic, and secondly for estimating the operator
and Frobenius norms of $\Sigma$ coming into the quantiles of said test
statistic. These properties are obtained for Gaussian and bounded
distributions. A particular attention is given to the dependence in the
pseudo-dimension $d_*$ of the distribution, defined as $d_* :=
\|\Sigma\|_2^2/\|\Sigma\|_\infty^2$. In particular, for $\eta=0$, the minimum
separation distance is ${\Theta}(d_*^{\frac{1}{4}}\sqrt{\|\Sigma\|_\infty/n})$,
in contrast with the minimax estimation distance for $\mu$, which is
${\Theta}(d_e^{\frac{1}{2}}\sqrt{\|\Sigma\|_\infty/n})$ (where
$d_e:=\|\Sigma\|_1/\|\Sigma\|_\infty$). This generalizes a phenomenon spelled
out in particular by Baraud (2002)."
['stat.TH'],Provable Tensor-Train Format Tensor Completion by Riemannian Optimization,"The tensor train (TT) format enjoys appealing advantages in handling
structural high-order tensors. The recent decade has witnessed the wide
applications of TT-format tensors from diverse disciplines, among which tensor
completion has drawn considerable attention. Numerous fast algorithms,
including the Riemannian gradient descent (RGrad) algorithm, have been proposed
for the TT-format tensor completion. However, the theoretical guarantees of
these algorithms are largely missing or sub-optimal, partly due to the
complicated and recursive algebraic operations in TT-format decomposition.
Moreover, existing results established for the tensors of other formats, for
example, Tucker and CP, are inapplicable because the algorithms treating
TT-format tensors are substantially different and more involved. In this paper,
we provide, to our best knowledge, the first theoretical guarantees of the
convergence of RGrad algorithm for TT-format tensor completion, under a nearly
optimal sample size condition. The RGrad algorithm converges linearly with a
constant contraction rate that is free of tensor condition number without the
necessity of re-conditioning. We also propose a novel approach, referred to as
the sequential second-order moment method, to attain a warm initialization
under a similar sample size requirement. As a byproduct, our result even
significantly refines the prior investigation of RGrad algorithm for matrix
completion. Numerical experiments confirm our theoretical discovery and
showcase the computational speedup gained by the TT-format decomposition."
"['stat.ME', 'stat.ML']",Pitfalls in Machine Learning Research: Reexamining the Development Cycle,"Machine learning has the potential to fuel further advances in data science,
but it is greatly hindered by an ad hoc design process, poor data hygiene, and
a lack of statistical rigor in model evaluation. Recently, these issues have
begun to attract more attention as they have caused public and embarrassing
issues in research and development. Drawing from our experience as machine
learning researchers, we follow the machine learning process from algorithm
design to data collection to model evaluation, drawing attention to common
pitfalls and providing practical recommendations for improvements. At each
step, case studies are introduced to highlight how these pitfalls occur in
practice, and where things could be improved."
['stat.ME'],High dimensional Bayesian Optimization Algorithm for Complex System in Time Series,"At present, high-dimensional global optimization problems with time-series
models have received much attention from engineering fields. Since it was
proposed, Bayesian optimization has quickly become a popular and promising
approach for solving global optimization problems. However, the standard
Bayesian optimization algorithm is insufficient to solving the global optimal
solution when the model is high-dimensional. Hence, this paper presents a novel
high dimensional Bayesian optimization algorithm by considering dimension
reduction and different dimension fill-in strategies. Most existing literature
about Bayesian optimization algorithms did not discuss the sampling strategies
to optimize the acquisition function. This study proposed a new sampling method
based on both the multi-armed bandit and random search methods while optimizing
the acquisition function. Besides, based on the time-dependent or
dimension-dependent characteristics of the model, the proposed algorithm can
reduce the dimension evenly. Then, five different dimension fill-in strategies
were discussed and compared in this study. Finally, to increase the final
accuracy of the optimal solution, the proposed algorithm adds a local search
based on a series of Adam-based steps at the final stage. Our computational
experiments demonstrated that the proposed Bayesian optimization algorithm
could achieve reasonable solutions with excellent performances for high
dimensional global optimization problems with a time-series optimal control
model."
['stat.CO'],Learned Interpretable Residual Extragradient ISTA for Sparse Coding,"Recently, the study on learned iterative shrinkage thresholding algorithm
(LISTA) has attracted increasing attentions. A large number of experiments as
well as some theories have proved the high efficiency of LISTA for solving
sparse coding problems. However, existing LISTA methods are all serial
connection. To address this issue, we propose a novel extragradient based LISTA
(ELISTA), which has a residual structure and theoretical guarantees. In
particular, our algorithm can also provide the interpretability for Res-Net to
a certain extent. From a theoretical perspective, we prove that our method
attains linear convergence. In practice, extensive empirical results verify the
advantages of our method."
"['stat.ML', 'stat.ME', 'stat.TH']",BELT: Block-wise Missing Embedding Learning Transformer,"Matrix completion has attracted attention in many fields, including
statistics, applied mathematics, and electrical engineering. Most of the works
focus on the independent sampling models under which the observed entries are
sampled independently. Motivated by applications in the integration of multiple
Electronic Health Record (EHR) datasets, we propose the method {\bf B}lock-wise
missing {\bf E}mbedding {\bf L}earning {\bf T}ransformer (BELT) to treat
row-wise/column-wise missingness. Specifically, BELT can recover block-wise
missing matrices efficiently when every pair of matrices has an overlap. Our
idea is to exploit the orthogonal Procrustes problem to align the eigenspace of
the two sub-matrices using their overlap, then complete the missing blocks by
the inner product of the two low-rank components. Besides, we prove the
statistical rate for the eigenspace of the underlying matrix, which is
comparable to the rate under the independently missing assumption. Simulation
studies show that the method performs well under a variety of configurations.
In the real data analysis, the method is applied to two tasks: (i) the
integrating of several point-wise mutual information matrices built by English
EHR and Chinese medical text data, and (ii) the machine translation between
English and Chinese medical concepts. Our method shows an advantage over
existing methods."
"['stat.CO', 'stat.ML']",Matrix games with bandit feedback,"We study a version of the classical zero-sum matrix game with unknown payoff
matrix and bandit feedback, where the players only observe each others actions
and a noisy payoff. This generalizes the usual matrix game, where the payoff
matrix is known to the players. Despite numerous applications, this problem has
received relatively little attention. Although adversarial bandit algorithms
achieve low regret, they do not exploit the matrix structure and perform poorly
relative to the new algorithms. The main contributions are regret analyses of
variants of UCB and K-learning that hold for any opponent, e.g., even when the
opponent adversarially plays the best-response to the learner's mixed strategy.
Along the way, we show that Thompson fails catastrophically in this setting and
provide empirical comparison to existing algorithms."
"['stat.ML', 'stat.TH']",Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize,"This paper provides a non-asymptotic analysis of linear stochastic
approximation (LSA) algorithms with fixed stepsize. This family of methods
arises in many machine learning tasks and is used to obtain approximate
solutions of a linear system $\bar{A}\theta = \bar{b}$ for which $\bar{A}$ and
$\bar{b}$ can only be accessed through random estimates $\{({\bf A}_n, {\bf
b}_n): n \in \mathbb{N}^*\}$. Our analysis is based on new results regarding
moments and high probability bounds for products of matrices which are shown to
be tight. We derive high probability bounds on the performance of LSA under
weaker conditions on the sequence $\{({\bf A}_n, {\bf b}_n): n \in
\mathbb{N}^*\}$ than previous works. However, in contrast, we establish
polynomial concentration bounds with order depending on the stepsize. We show
that our conclusions cannot be improved without additional assumptions on the
sequence of random matrices $\{{\bf A}_n: n \in \mathbb{N}^*\}$, and in
particular that no Gaussian or exponential high probability bounds can hold.
Finally, we pay a particular attention to establishing bounds with sharp order
with respect to the number of iterations and the stepsize and whose leading
terms contain the covariance matrices appearing in the central limit theorems."
"['stat.ML', 'stat.ME', 'stat.TH']",The Chi-Square Test of Distance Correlation,"Distance correlation has gained much recent attention in the data science
community: the sample statistic is straightforward to compute and
asymptotically equals zero if and only if independence, making it an ideal
choice to discover any type of dependency structure given sufficient sample
size. One major bottleneck is the testing process: because the null
distribution of distance correlation depends on the underlying random variables
and metric choice, it typically requires a permutation test to estimate the
null and compute the p-value, which is very costly for large amount of data. To
overcome the difficulty, in this paper we propose a chi-square test for
distance correlation. Method-wise, the chi-square test is non-parametric,
extremely fast, and applicable to bias-corrected distance correlation using any
strong negative type metric or characteristic kernel. The test exhibits a
similar testing power as the standard permutation test, and can be utilized for
K-sample and partial testing. Theory-wise, we show that the underlying
chi-square distribution well approximates and dominates the limiting null
distribution in upper tail, prove the chi-square test can be valid and
universally consistent for testing independence, and establish a testing power
inequality with respect to the permutation test."
"['stat.ML', 'stat.ME']",High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach,"Undirected graphical models have been widely used to model the conditional
independence structure of high-dimensional random vector data for years. In
many modern applications such as EEG and fMRI data, the observations are
multivariate random functions rather than scalars. To model the conditional
independence of this type of data, functional graphical models are proposed and
have attracted an increasing attention in recent years. In this paper, we
propose a neighborhood selection approach to estimate Gaussian functional
graphical models. We first estimate the neighborhood of all nodes via
function-on-function regression, and then we can recover the whole graph
structure based on the neighborhood information. By estimating conditional
structure directly, we can circumvent the need of a well-defined precision
operator which generally does not exist. Besides, we can better explore the
effect of the choice of function basis for dimension reduction. We give a
criterion for choosing the best function basis and motivate two practically
useful choices, which we justified by both theory and experiments and show that
they are better than expanding each function onto its own FPCA basis as in
previous literature. In addition, the neighborhood selection approach is
computationally more efficient than fglasso as it is more easy to do parallel
computing. The statistical consistency of our proposed methods in
high-dimensional setting are supported by both theory and experiment."
"['stat.ML', 'stat.TH']",Prediction in latent factor regression: Adaptive PCR and beyond,"This work is devoted to the finite sample prediction risk analysis of a class
of linear predictors of a response $Y\in \mathbb{R}$ from a high-dimensional
random vector $X\in \mathbb{R}^p$ when $(X,Y)$ follows a latent factor
regression model generated by a unobservable latent vector $Z$ of dimension
less than $p$. Our primary contribution is in establishing finite sample risk
bounds for prediction with the ubiquitous Principal Component Regression (PCR)
method, under the factor regression model, with the number of principal
components adaptively selected from the data -- a form of theoretical guarantee
that is surprisingly lacking from the PCR literature. To accomplish this, we
prove a master theorem that establishes a risk bound for a large class of
predictors, including the PCR predictor as a special case. This approach has
the benefit of providing a unified framework for the analysis of a wide range
of linear prediction methods, under the factor regression setting. In
particular, we use our main theorem to recover known risk bounds for the
minimum-norm interpolating predictor, which has received renewed attention in
the past two years, and a prediction method tailored to a subclass of factor
regression models with identifiable parameters. This model-tailored method can
be interpreted as prediction via clusters with latent centers.
  To address the problem of selecting among a set of candidate predictors, we
analyze a simple model selection procedure based on data-splitting, providing
an oracle inequality under the factor model to prove that the performance of
the selected predictor is close to the optimal candidate. We conclude with a
detailed simulation study to support and complement our theoretical results."
"['stat.ML', 'stat.CO']",Machine Learning Assisted Orthonormal Basis Selection for Functional Data Analysis,"In implementations of the functional data methods, the effect of the initial
choice of an orthonormal basis has not gained much attention in the past.
Typically, several standard bases such as Fourier, wavelets, splines, etc. are
considered to transform observed functional data and a choice is made without
any formal criteria indicating which of the bases is preferable for the initial
transformation of the data into functions. In an attempt to address this issue,
we propose a strictly data-driven method of orthogonal basis selection. The
method uses recently introduced orthogonal spline bases called the splinets
obtained by efficient orthogonalization of the B-splines. The algorithm learns
from the data in the machine learning style to efficiently place knots. The
optimality criterion is based on the average (per functional data point) mean
square error and is utilized both in the learning algorithms and in comparison
studies. The latter indicates efficiency that is particularly evident for the
sparse functional data and to a lesser degree in analyses of responses to
complex physical systems."
"['stat.ML', 'stat.ME']",Computing Valid p-value for Optimal Changepoint by Selective Inference using Dynamic Programming,"There is a vast body of literature related to methods for detecting
changepoints (CP). However, less attention has been paid to assessing the
statistical reliability of the detected CPs. In this paper, we introduce a
novel method to perform statistical inference on the significance of the CPs,
estimated by a Dynamic Programming (DP)-based optimal CP detection algorithm.
Based on the selective inference (SI) framework, we propose an exact
(non-asymptotic) approach to compute valid p-values for testing the
significance of the CPs. Although it is well-known that SI has low statistical
power because of over-conditioning, we address this disadvantage by introducing
parametric programming techniques. Then, we propose an efficient method to
conduct SI with the minimum amount of conditioning, leading to high statistical
power. We conduct experiments on both synthetic and real-world datasets,
through which we offer evidence that our proposed method is more powerful than
existing methods, has decent performance in terms of computational efficiency,
and provides good results in many practical applications."
"['stat.ML', 'stat.ME']",Identifiability of Hierarchical Latent Attribute Models,"Hierarchical Latent Attribute Models (HLAMs) are a family of discrete latent
variable models that are attracting increasing attention in educational,
psychological, and behavioral sciences. The key ingredients of an HLAM include
a binary structural matrix and a directed acyclic graph specifying hierarchical
constraints on the configurations of latent attributes. These components encode
practitioners' design information and carry important scientific meanings.
Despite the popularity of HLAMs, the fundamental identifiability issue remains
unaddressed. The existence of the attribute hierarchy graph leads to degenerate
parameter space, and the potentially unknown structural matrix further
complicates the identifiability problem. This paper addresses this issue of
identifying the latent structure and model parameters underlying an HLAM. We
develop sufficient and necessary identifiability conditions. These results
directly and sharply characterize the different impacts on identifiability cast
by different attribute types in the graph. The proposed conditions not only
provide insights into diagnostic test designs under the attribute hierarchy,
but also serve as tools to assess the validity of an estimated HLAM."
"['stat.ML', 'stat.TH']",Regularized spectral methods for clustering signed networks,"We study the problem of $k$-way clustering in signed graphs. Considerable
attention in recent years has been devoted to analyzing and modeling signed
graphs, where the affinity measure between nodes takes either positive or
negative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral
method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem),
which casts the clustering task as a generalized eigenvalue problem optimizing
a suitably defined objective function. This approach is motivated by social
balance theory, where the clustering task aims to decompose a given network
into disjoint groups, such that individuals within the same group are connected
by as many positive edges as possible, while individuals from different groups
are mainly connected by negative edges. Through extensive numerical
simulations, SPONGE was shown to achieve state-of-the-art empirical
performance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the
popular Signed Laplacian method under the setting of a Signed Stochastic Block
Model (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is
moderately dense.
  In this work, we build on the results in [CDGT 2019] on two fronts for the
normalized versions of SPONGE and the Signed Laplacian. Firstly, for both
algorithms, we extend the theoretical analysis in [CDGT 2019] to the general
setting of $k \geq 2$ unequal-sized clusters in the moderately dense regime.
Secondly, we introduce regularized versions of both methods to handle sparse
graphs -- a regime where standard spectral methods underperform -- and provide
theoretical guarantees under the same SSBM model. To the best of our knowledge,
regularized spectral methods have so far not been considered in the setting of
clustering signed graphs. We complement our theoretical results with an
extensive set of numerical experiments on synthetic data."
['stat.TH'],Exact Asymptotics for Linear Quadratic Adaptive Control,"Recent progress in reinforcement learning has led to remarkable performance
in a range of applications, but its deployment in high-stakes settings remains
quite rare. One reason is a limited understanding of the behavior of
reinforcement algorithms, both in terms of their regret and their ability to
learn the underlying system dynamics---existing work is focused almost
exclusively on characterizing rates, with little attention paid to the
constants multiplying those rates that can be critically important in practice.
To start to address this challenge, we study perhaps the simplest non-bandit
reinforcement learning problem: linear quadratic adaptive control (LQAC). By
carefully combining recent finite-sample performance bounds for the LQAC
problem with a particular (less-recent) martingale central limit theorem, we
are able to derive asymptotically-exact expressions for the regret, estimation
error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm.
In simulations on both stable and unstable systems, we find that our asymptotic
theory also describes the algorithm's finite-sample behavior remarkably well."
['stat.CO'],"The Pursuit of Algorithmic Fairness: On ""Correcting"" Algorithmic Unfairness in a Child Welfare Reunification Success Classifier","The algorithmic fairness of predictive analytic tools in the public sector
has increasingly become a topic of rigorous exploration. While instruments
pertaining to criminal recidivism and academic admissions, for example, have
garnered much attention, the predictive instruments of Child Welfare
jurisdictions have received considerably less attention. This is in part
because comparatively few such instruments exist and because even fewer have
been scrutinized through the lens of algorithmic fairness. In this work, we
seek to address both of these gaps. To this end, a novel classification
algorithm for predicting reunification success within Oregon Child Welfare is
presented, including all of the relevant details associated with building such
an instrument. The purpose of this tool is to maximize the number of stable
reunifications and identify potentially unstable reunifications which may
require additional resources and scrutiny. Additionally, because the
algorithmic fairness of the resulting tool, if left unaltered, is
unquestionably lacking, the utilized procedure for mitigating such unfairness
is presented, along with the rationale behind each difficult and unavoidable
choice. This procedure, though similar to other post-processing group-specific
thresholding methods, is novel in its use of a penalized optimizer and
contextually requisite subsampling. These novel methodological components yield
a rich and informative empirical understanding of the trade-off continuum
between fairness and accuracy. As the developed procedure is generalizable
across a variety of group-level definitions of algorithmic fairness, as well as
across an arbitrary number of protected attribute levels and risk thresholds,
the approach is broadly applicable both within and beyond Child Welfare."
"['stat.ML', 'stat.TH']",Multivariate Gaussian Variational Inference by Natural Gradient Descent,"This short note reviews so-called Natural Gradient Descent (NGD) for
multivariate Gaussians. The Fisher Information Matrix (FIM) is derived for
several different parameterizations of Gaussians. Careful attention is paid to
the symmetric nature of the covariance matrix when calculating derivatives. We
show that there are some advantages to choosing a parameterization comprising
the mean and inverse covariance matrix and provide a simple NGD update that
accounts for the symmetric (and sparse) nature of the inverse covariance
matrix."
"['stat.ML', 'stat.TH']",Tensor Regression Using Low-rank and Sparse Tucker Decompositions,"This paper studies a tensor-structured linear regression model with a scalar
response variable and tensor-structured predictors, such that the regression
parameters form a tensor of order $d$ (i.e., a $d$-fold multiway array) in
$\mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$. It focuses on the task
of estimating the regression tensor from $m$ realizations of the response
variable and the predictors where $m\ll n = \prod \nolimits_{i} n_i$. Despite
the seeming ill-posedness of this problem, it can still be solved if the
parameter tensor belongs to the space of sparse, low Tucker-rank tensors.
Accordingly, the estimation procedure is posed as a non-convex optimization
program over the space of sparse, low Tucker-rank tensors, and a tensor variant
of projected gradient descent is proposed to solve the resulting non-convex
problem. In addition, mathematical guarantees are provided that establish the
proposed method linearly converges to an appropriate solution under a certain
set of conditions. Further, an upper bound on sample complexity of tensor
parameter estimation for the model under consideration is characterized for the
special case when the individual (scalar) predictors independently draw values
from a sub-Gaussian distribution. The sample complexity bound is shown to have
a polylogarithmic dependence on $\bar{n} = \max \big\{n_i: i\in \{1,2,\ldots,d
\} \big\}$ and, orderwise, it matches the bound one can obtain from a heuristic
parameter counting argument. Finally, numerical experiments demonstrate the
efficacy of the proposed tensor model and estimation method on a synthetic
dataset and a collection of neuroimaging datasets pertaining to attention
deficit hyperactivity disorder. Specifically, the proposed method exhibits
better sample complexities on both synthetic and real datasets, demonstrating
the usefulness of the model and the method in settings where $n \gg m$."
"['stat.ML', 'stat.TH']",Optimal Rates of Distributed Regression with Imperfect Kernels,"Distributed machine learning systems have been receiving increasing
attentions for their efficiency to process large scale data. Many distributed
frameworks have been proposed for different machine learning tasks. In this
paper, we study the distributed kernel regression via the divide and conquer
approach. This approach has been proved asymptotically minimax optimal if the
kernel is perfectly selected so that the true regression function lies in the
associated reproducing kernel Hilbert space. However, this is usually, if not
always, impractical because kernels that can only be selected via prior
knowledge or a tuning process are hardly perfect. Instead it is more common
that the kernel is good enough but imperfect in the sense that the true
regression can be well approximated by but does not lie exactly in the kernel
space. We show distributed kernel regression can still achieves capacity
independent optimal rate in this case. To this end, we first establish a
general framework that allows to analyze distributed regression with response
weighted base algorithms by bounding the error of such algorithms on a single
data set, provided that the error bounds has factored the impact of the
unexplained variance of the response variable. Then we perform a leave one out
analysis of the kernel ridge regression and bias corrected kernel ridge
regression, which in combination with the aforementioned framework allows us to
derive sharp error bounds and capacity independent optimal rates for the
associated distributed kernel regression algorithms. As a byproduct of the
thorough analysis, we also prove the kernel ridge regression can achieve rates
faster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting
which, to our best knowledge, are first observed and novel in regression
learning."
"['stat.ML', 'stat.CO']",Forecasting with time series imaging,"Feature-based time series representations have attracted substantial
attention in a wide range of time series analysis methods. Recently, the use of
time series features for forecast model averaging has been an emerging research
focus in the forecasting community. Nonetheless, most of the existing
approaches depend on the manual choice of an appropriate set of features.
Exploiting machine learning methods to extract features from time series
automatically becomes crucial in state-of-the-art time series analysis. In this
paper, we introduce an automated approach to extract time series features based
on time series imaging. We first transform time series into recurrence plots,
from which local features can be extracted using computer vision algorithms.
The extracted features are used for forecast model averaging. Our experiments
show that forecasting based on automatically extracted features, with less
human intervention and a more comprehensive view of the raw time series data,
yields highly comparable performances with the best methods in the largest
forecasting competition dataset (M4) and outperforms the top methods in the
Tourism forecasting competition dataset."
"['stat.ML', 'stat.ME']",An Evaluation of Change Point Detection Algorithms,"Change point detection is an important part of time series analysis, as the
presence of a change point indicates an abrupt and significant change in the
data generating process. While many algorithms for change point detection
exist, little attention has been paid to evaluating their performance on
real-world time series. Algorithms are typically evaluated on simulated data
and a small number of commonly-used series with unreliable ground truth.
Clearly this does not provide sufficient insight into the comparative
performance of these algorithms. Therefore, instead of developing yet another
change point detection method, we consider it vastly more important to properly
evaluate existing algorithms on real-world data. To achieve this, we present
the first data set specifically designed for the evaluation of change point
detection algorithms, consisting of 37 time series from various domains. Each
time series was annotated by five expert human annotators to provide ground
truth on the presence and location of change points. We analyze the consistency
of the human annotators, and describe evaluation metrics that can be used to
measure algorithm performance in the presence of multiple ground truth
annotations. Subsequently, we present a benchmark study where 14 existing
algorithms are evaluated on each of the time series in the data set. This study
shows that binary segmentation (Scott and Knott, 1974) and Bayesian online
change point detection (Adams and MacKay, 2007) are among the best performing
methods. Our aim is that this data set will serve as a proving ground in the
development of novel change point detection algorithms."
"['stat.ME', 'stat.ML']",Deep Neural Network in Cusp Catastrophe Model,"Catastrophe theory was originally proposed to study dynamical systems that
exhibit sudden shifts in behavior arising from small changes in input. These
models can generate reasonable explanation behind abrupt jumps in nonlinear
dynamic models. Among the different catastrophe models, the Cusp Catastrophe
model attracted the most attention due to it's relatively simpler dynamics and
rich domain of application. Due to the complex behavior of the response, the
parameter space becomes highly non-convex and hence it becomes very hard to
optimize to figure out the generating parameters. Instead of solving for these
generating parameters, we demonstrated how a Machine learning model can be
trained to learn the dynamics of the Cusp catastrophe models, without ever
really solving for the generating model parameters. Simulation studies and
application on a few famous datasets are used to validate our approach. To our
knowledge, this is the first paper of such kind where a neural network based
approach has been applied in Cusp Catastrophe model."
"['stat.ML', 'stat.TH']",Transport Gaussian Processes for Regression,"Gaussian process (GP) priors are non-parametric generative models with
appealing modelling properties for Bayesian inference: they can model
non-linear relationships through noisy observations, have closed-form
expressions for training and inference, and are governed by interpretable
hyperparameters. However, GP models rely on Gaussianity, an assumption that
does not hold in several real-world scenarios, e.g., when observations are
bounded or have extreme-value dependencies, a natural phenomenon in physics,
finance and social sciences. Although beyond-Gaussian stochastic processes have
caught the attention of the GP community, a principled definition and rigorous
treatment is still lacking. In this regard, we propose a methodology to
construct stochastic processes, which include GPs, warped GPs, Student-t
processes and several others under a single unified approach. We also provide
formulas and algorithms for training and inference of the proposed models in
the regression problem. Our approach is inspired by layers-based models, where
each proposed layer changes a specific property over the generated stochastic
process. That, in turn, allows us to push-forward a standard Gaussian white
noise prior towards other more expressive stochastic processes, for which
marginals and copulas need not be Gaussian, while retaining the appealing
properties of GPs. We validate the proposed model through experiments with
real-world data."
"['stat.ME', 'stat.ML']",Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality,"Granger causality is a widely-used criterion for analyzing interactions in
large-scale networks. As most physical interactions are inherently nonlinear,
we consider the problem of inferring the existence of pairwise Granger
causality between nonlinearly interacting stochastic processes from their time
series measurements. Our proposed approach relies on modeling the embedded
nonlinearities in the measurements using a component-wise time series
prediction model based on Statistical Recurrent Units (SRUs). We make a case
that the network topology of Granger causal relations is directly inferrable
from a structured sparse estimate of the internal parameters of the SRU
networks trained to predict the processes$'$ time series measurements. We
propose a variant of SRU, called economy-SRU, which, by design has considerably
fewer trainable parameters, and therefore less prone to overfitting. The
economy-SRU computes a low-dimensional sketch of its high-dimensional hidden
state in the form of random projections to generate the feedback for its
recurrent processing. Additionally, the internal weight parameters of the
economy-SRU are strategically regularized in a group-wise manner to facilitate
the proposed network in extracting meaningful predictive features that are
highly time-localized to mimic real-world causal events. Extensive experiments
are carried out to demonstrate that the proposed economy-SRU based time series
prediction model outperforms the MLP, LSTM and attention-gated CNN-based time
series models considered previously for inferring Granger causality."
"['stat.ML', 'stat.TH']",Regression from Dependent Observations,"The standard linear and logistic regression models assume that the response
variables are independent, but share the same linear relationship to their
corresponding vectors of covariates. The assumption that the response variables
are independent is, however, too strong. In many applications, these responses
are collected on nodes of a network, or some spatial or temporal domain, and
are dependent. Examples abound in financial and meteorological applications,
and dependencies naturally arise in social networks through peer effects.
Regression with dependent responses has thus received a lot of attention in the
Statistics and Economics literature, but there are no strong consistency
results unless multiple independent samples of the vectors of dependent
responses can be collected from these models. We present computationally and
statistically efficient methods for linear and logistic regression models when
the response variables are dependent on a network. Given one sample from a
networked linear or logistic regression model and under mild assumptions, we
prove strong consistency results for recovering the vector of coefficients and
the strength of the dependencies, recovering the rates of standard regression
under independent observations. We use projected gradient descent on the
negative log-likelihood, or negative log-pseudolikelihood, and establish their
strong convexity and consistency using concentration of measure for dependent
random variables."
"['stat.ML', 'stat.TH']",Inference for multiple object tracking: A Bayesian nonparametric approach,"In recent years, multi object tracking (MOT) problem has drawn attention to
it and has been studied in various research areas. However, some of the
challenging problems including time dependent cardinality, unordered
measurement set, and object labeling remain unclear. In this paper, we propose
robust nonparametric methods to model the state prior for MOT problem. These
models are shown to be more flexible and robust compared to existing methods.
In particular, the overall approach estimates time dependent object
cardinality, provides object labeling, and identifies object associated
measurements. Moreover, our proposed framework dynamically contends with the
birth/death and survival of the objects through dependent nonparametric
processes. We present Inference algorithms that demonstrate the utility of the
dependent nonparametric models for tracking. We employ Monte Carlo sampling
methods to demonstrate the proposed algorithms efficiently learn the trajectory
of objects from noisy measurements. The computational results display the
performance of the proposed algorithms and comparison not only between one
another, but also between proposed algorithms and labeled multi Bernoulli
tracker."
"['stat.ML', 'stat.CO']",A stochastic alternating minimizing method for sparse phase retrieval,"Sparse phase retrieval plays an important role in many fields of applied
science and thus attracts lots of attention. In this paper, we propose a
\underline{sto}chastic alte\underline{r}nating \underline{m}inimizing method
for \underline{sp}arse ph\underline{a}se \underline{r}etrieval
(\textit{StormSpar}) algorithm which {emprically} is able to recover
$n$-dimensional $s$-sparse signals from only $O(s\,\mathrm{log}\, n)$ number of
measurements without a desired initial value required by many existing methods.
In \textit{StormSpar}, the hard-thresholding pursuit (HTP) algorithm is
employed to solve the sparse constraint least square sub-problems. The main
competitive feature of \textit{StormSpar} is that it converges globally
requiring optimal order of number of samples with random initialization.
Extensive numerical experiments are given to validate the proposed algorithm."
"['stat.ML', 'stat.ME']",Benchmarking Minimax Linkage,"Minimax linkage was first introduced by Ao et al. [3] in 2004, as an
alternative to standard linkage methods used in hierarchical clustering.
Minimax linkage relies on distances to a prototype for each cluster; this
prototype can be thought of as a representative object in the cluster, hence
improving the interpretability of clustering results. Bien and Tibshirani
analyzed properties of this method in 2011 [2], popularizing the method within
the statistics community. Additionally, they performed comparisons of minimax
linkage to standard linkage methods, making use of five data sets and two
different evaluation metrics (distance to prototype and misclassification
rate). In an effort to expand upon their work and evaluate minimax linkage more
comprehensively, our benchmark study focuses on thorough method evaluation via
multiple performance metrics on several well-described data sets. We also make
all code and data publicly available through an R package, for full
reproducibility. Similarly to [2], we find that minimax linkage often produces
the smallest maximum minimax radius of all linkage methods, meaning that
minimax linkage produces clusters where objects in a cluster are tightly
clustered around their prototype. This is true across a range of values for the
total number of clusters (k). However, this is not always the case, and special
attention should be paid to the case when k is the true known value. For true
k, minimax linkage does not always perform the best in terms of all the
evaluation metrics studied, including maximum minimax radius. This paper was
motivated by the IFCS Cluster Benchmarking Task Force's call for clustering
benchmark studies and the white paper [5], which put forth guidelines and
principles for comprehensive benchmarking in clustering. Our work is designed
to be a neutral benchmark study of minimax linkage."
"['stat.ML', 'stat.CO', 'stat.ME']",Combining Model and Parameter Uncertainty in Bayesian Neural Networks,"Bayesian neural networks (BNNs) have recently regained a significant amount
of attention in the deep learning community due to the development of scalable
approximate Bayesian inference techniques. There are several advantages of
using Bayesian approach: Parameter and prediction uncertainty become easily
available, facilitating rigid statistical analysis. Furthermore, prior
knowledge can be incorporated. However so far there have been no scalable
techniques capable of combining both model (structural) and parameter
uncertainty. In this paper we introduce the concept of model uncertainty in
BNNs and hence make inference in the joint space of models and parameters.
Moreover, we suggest an adaptation of a scalable variational inference approach
with reparametrization of marginal inclusion probabilities to incorporate the
model space constraints. Finally, we show that incorporating model uncertainty
via Bayesian model averaging and Bayesian model selection allows to drastically
sparsify the structure of BNNs."
"['stat.ML', 'stat.CO', 'stat.ME']",Variational approximations using Fisher divergence,"Modern applications of Bayesian inference involve models that are
sufficiently complex that the corresponding posterior distributions are
intractable and must be approximated. The most common approximation is based on
Markov chain Monte Carlo, but these can be expensive when the data set is large
and/or the model is complex, so more efficient variational approximations have
recently received considerable attention. The traditional variational methods,
that seek to minimize the Kullback--Leibler divergence between the posterior
and a relatively simple parametric family, provide accurate and efficient
estimation of the posterior mean, but often does not capture other moments, and
have limitations in terms of the models to which they can be applied. Here we
propose the construction of variational approximations based on minimizing the
Fisher divergence, and develop an efficient computational algorithm that can be
applied to a wide range of models without conjugacy or potentially unrealistic
mean-field assumptions. We demonstrate the superior performance of the proposed
method for the benchmark case of logistic regression."
"['stat.ML', 'stat.ME']",A Method for Robust Online Classification using Dictionary Learning: Development and Assessment for Monitoring Manual Material Handling Activities Using Wearable Sensors,"Classification methods based on sparse estimation have drawn much attention
recently, due to their effectiveness in processing high-dimensional data such
as images. In this paper, a method to improve the performance of a sparse
representation classification (SRC) approach is proposed; it is then applied to
the problem of online process monitoring of human workers, specifically manual
material handling (MMH) operations monitored using wearable sensors (involving
111 sensor channels). Our proposed method optimizes the design matrix (aka
dictionary) in the linear model used for SRC, minimizing its ill-posedness to
achieve a sparse solution. This procedure is based on the idea of dictionary
learning (DL): we optimize the design matrix formed by training datasets to
minimize both redundancy and coherency as well as reducing the size of these
datasets. Use of such optimized training data can subsequently improve
classification accuracy and help decrease the computational time needed for the
SRC; it is thus more applicable for online process monitoring. Performance of
the proposed methodology is demonstrated using wearable sensor data obtained
from manual material handling experiments, and is found to be superior to those
of benchmark methods in terms of accuracy, while also requiring computational
time appropriate for MMH online monitoring."
['stat.TH'],Statistical Learning Guarantees for Compressive Clustering and Compressive Mixture Modeling,"We provide statistical learning guarantees for two unsupervised learning
tasks in the context of compressive statistical learning, a general framework
for resource-efficient large-scale learning that we introduced in a companion
paper.The principle of compressive statistical learning is to compress a
training collection, in one pass, into a low-dimensional sketch (a vector of
random empirical generalized moments) that captures the information relevant to
the considered learning task. We explicitly describe and analyze random feature
functions which empirical averages preserve the needed information for
compressive clustering and compressive Gaussian mixture modeling with fixed
known variance, and establish sufficient sketch sizes given the problem
dimensions."
"['stat.ML', 'stat.CO']",IGANI: Iterative Generative Adversarial Networks for Imputation with Application to Traffic Data,"Increasing use of sensor data in intelligent transportation systems calls for
accurate imputation algorithms that can enable reliable traffic management in
the occasional absence of data. As one of the effective imputation approaches,
generative adversarial networks (GANs) are implicit generative models that can
be used for data imputation, which is formulated as an unsupervised learning
problem. This work introduces a novel iterative GAN architecture, called
Iterative Generative Adversarial Networks for Imputation (IGANI), for data
imputation. IGANI imputes data in two steps and maintains the invertibility of
the generative imputer, which will be shown to be a sufficient condition for
the convergence of the proposed GAN-based imputation. The performance of our
proposed method is evaluated on (1) the imputation of traffic speed data
collected in the city of Guangzhou in China, and the training of short-term
traffic prediction models using imputed data, and (2) the imputation of
multi-variable traffic data of highways in Portland-Vancouver metropolitan
region which includes volume, occupancy, and speed with different missing rates
for each of them. It is shown that our proposed algorithm mostly produces more
accurate results compared to those of previous GAN-based imputation
architectures."
['stat.ME'],Capturing patterns of variation unique to a specific dataset,"Capturing patterns of variation present in a dataset is important in
exploratory data analysis and unsupervised learning. Contrastive dimension
reduction methods, such as contrastive principal component analysis (cPCA),
find patterns unique to a target dataset of interest by contrasting with a
carefully chosen background dataset representing unwanted or uninteresting
variation. However, such methods typically require a tuning parameter that
governs the level of contrast, and it is unclear how to choose this parameter
objectively. Furthermore, it is frequently of interest to contrast against
multiple backgrounds, which is difficult to accomplish with existing methods.
We propose unique component analysis (UCA), a tuning-free method that
identifies low-dimensional representations of a target dataset relative to one
or more comparison datasets. It is computationally efficient even with large
numbers of features. We show in several experiments that UCA with a single
background dataset achieves similar results compared to cPCA with various
tuning parameters, and that UCA with multiple individual background datasets is
superior to both cPCA with any single background data and cPCA with a pooled
background dataset."
"['stat.ML', 'stat.TH']",Analysis of feature learning in weight-tied autoencoders via the mean field lens,"Autoencoders are among the earliest introduced nonlinear models for
unsupervised learning. Although they are widely adopted beyond research, it has
been a longstanding open problem to understand mathematically the feature
extraction mechanism that trained nonlinear autoencoders provide.
  In this work, we make progress in this problem by analyzing a class of
two-layer weight-tied nonlinear autoencoders in the mean field framework. Upon
a suitable scaling, in the regime of a large number of neurons, the models
trained with stochastic gradient descent are shown to admit a mean field
limiting dynamics. This limiting description reveals an asymptotically precise
picture of feature learning by these models: their training dynamics exhibit
different phases that correspond to the learning of different principal
subspaces of the data, with varying degrees of nonlinear shrinkage dependent on
the $\ell_{2}$-regularization and stopping time. While we prove these results
under an idealized assumption of (correlated) Gaussian data, experiments on
real-life data demonstrate an interesting match with the theory.
  The autoencoder setup of interests poses a nontrivial mathematical challenge
to proving these results. In this setup, the ""Lipschitz"" constants of the
models grow with the data dimension $d$. Consequently an adaptation of previous
analyses requires a number of neurons $N$ that is at least exponential in $d$.
Our main technical contribution is a new argument which proves that the
required $N$ is only polynomial in $d$. We conjecture that $N\gg d$ is
sufficient and that $N$ is necessarily larger than a data-dependent intrinsic
dimension, a behavior that is fundamentally different from previously studied
setups."
"['stat.ML', 'stat.ME']",Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy,"We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier."
['stat.TH'],Non-parametric Models for Non-negative Functions,"Linear models have shown great effectiveness and flexibility in many fields
such as machine learning, signal processing and statistics. They can represent
rich spaces of functions while preserving the convexity of the optimization
problems where they are used, and are simple to evaluate, differentiate and
integrate. However, for modeling non-negative functions, which are crucial for
unsupervised learning, density estimation, or non-parametric Bayesian methods,
linear models are not applicable directly. Moreover, current state-of-the-art
models like generalized linear models either lead to non-convex optimization
problems, or cannot be easily integrated. In this paper we provide the first
model for non-negative functions which benefits from the same good properties
of linear models. In particular, we prove that it admits a representer theorem
and provide an efficient dual formulation for convex problems. We study its
representation power, showing that the resulting space of functions is strictly
richer than that of generalized linear models. Finally we extend the model and
the theoretical results to functions with outputs in convex cones. The paper is
complemented by an experimental evaluation of the model showing its
effectiveness in terms of formulation, algorithmic derivation and practical
results on the problems of density estimation, regression with heteroscedastic
errors, and multiple quantile regression."
"['stat.ML', 'stat.TH']",Structures of Spurious Local Minima in $k$-means,"$k$-means clustering is a fundamental problem in unsupervised learning. The
problem concerns finding a partition of the data points into $k$ clusters such
that the within-cluster variation is minimized. Despite its importance and wide
applicability, a theoretical understanding of the $k$-means problem has not
been completely satisfactory. Existing algorithms with theoretical performance
guarantees often rely on sophisticated (sometimes artificial) algorithmic
techniques and restricted assumptions on the data. The main challenge lies in
the non-convex nature of the problem; in particular, there exist additional
local solutions other than the global optimum. Moreover, the simplest and most
popular algorithm for $k$-means, namely Lloyd's algorithm, generally converges
to such spurious local solutions both in theory and in practice.
  In this paper, we approach the $k$-means problem from a new perspective, by
investigating the structures of these spurious local solutions under a
probabilistic generative model with $k$ ground truth clusters. As soon as
$k=3$, spurious local minima provably exist, even for well-separated and
balanced clusters. One such local minimum puts two centers at one true cluster,
and the third center in the middle of the other two true clusters. For general
$k$, one local minimum puts multiple centers at a true cluster, and one center
in the middle of multiple true clusters. Perhaps surprisingly, we prove that
this is essentially the only type of spurious local minima under a separation
condition. Our results pertain to the $k$-means formulation for mixtures of
Gaussians or bounded distributions. Our theoretical results corroborate
existing empirical observations and provide justification for several improved
algorithms for $k$-means clustering."
"['stat.ML', 'stat.TH']",A Unified Framework for Tuning Hyperparameters in Clustering Problems,"Selecting hyperparameters for unsupervised learning problems is challenging
in general due to the lack of ground truth for validation. Despite the
prevalence of this issue in statistics and machine learning, especially in
clustering problems, there are not many methods for tuning these
hyperparameters with theoretical guarantees. In this paper, we provide a
framework with provable guarantees for selecting hyperparameters in a number of
distinct models. We consider both the subgaussian mixture model and network
models to serve as examples of i.i.d. and non-i.i.d. data. We demonstrate that
the same framework can be used to choose the Lagrange multipliers of penalty
terms in semi-definite programming (SDP) relaxations for community detection,
and the bandwidth parameter for constructing kernel similarity matrices for
spectral clustering. By incorporating a cross-validation procedure, we show the
framework can also do consistent model selection for network models. Using a
variety of simulated and real data examples, we show that our framework
outperforms other widely used tuning procedures in a broad range of parameter
settings."
"['stat.ML', 'stat.ME']",Prescribed Generative Adversarial Networks,"Generative adversarial networks (GANs) are a powerful approach to
unsupervised learning. They have achieved state-of-the-art performance in the
image domain. However, GANs are limited in two ways. They often learn
distributions with low support---a phenomenon known as mode collapse---and they
do not guarantee the existence of a probability density, which makes evaluating
generalization using predictive log-likelihood impossible. In this paper, we
develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs
add noise to the output of a density network and optimize an
entropy-regularized adversarial loss. The added noise renders tractable
approximations of the predictive log-likelihood and stabilizes the training
procedure. The entropy regularizer encourages PresGANs to capture all the modes
of the data distribution. Fitting PresGANs involves computing the intractable
gradients of the entropy regularization term; PresGANs sidestep this
intractability using unbiased stochastic estimates. We evaluate PresGANs on
several datasets and found they mitigate mode collapse and generate samples
with high perceptual quality. We further found that PresGANs reduce the gap in
performance in terms of predictive log-likelihood between traditional GANs and
variational autoencoders (VAEs)."
"['stat.ML', 'stat.TH']",Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection,"Nearest-neighbor (NN) procedures are well studied and widely used in both
supervised and unsupervised learning problems. In this paper we are concerned
with investigating the performance of NN-based methods for anomaly detection.
We first show through extensive simulations that NN methods compare favorably
to some of the other state-of-the-art algorithms for anomaly detection based on
a set of benchmark synthetic datasets. We further consider the performance of
NN methods on real datasets, and relate it to the dimensionality of the
problem. Next, we analyze the theoretical properties of NN-methods for anomaly
detection by studying a more general quantity called distance-to-measure (DTM),
originally developed in the literature on robust geometric and topological
inference. We provide finite-sample uniform guarantees for the empirical DTM
and use them to derive misclassification rates for anomalous observations under
various settings. In our analysis we rely on Huber's contamination model and
formulate mild geometric regularity assumptions on the underlying distribution
of the data."
"['stat.ML', 'stat.CO']",Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications,"Density Estimation is one of the central areas of statistics whose purpose is
to estimate the probability density function underlying the observed data. It
serves as a building block for many tasks in statistical inference,
visualization, and machine learning. Density Estimation is widely adopted in
the domain of unsupervised learning especially for the application of
clustering. As big data become pervasive in almost every area of data sciences,
analyzing high-dimensional data that have many features and variables appears
to be a major focus in both academia and industry. High-dimensional data pose
challenges not only from the theoretical aspects of statistical inference, but
also from the algorithmic/computational considerations of machine learning and
data analytics. This paper reviews a collection of selected nonparametric
density estimation algorithms for high-dimensional data, some of them are
recently published and provide interesting mathematical insights. The important
application domain of nonparametric density estimation, such as { modal
clustering}, are also included in this paper. Several research directions
related to density estimation and high-dimensional data analysis are suggested
by the authors."
"['stat.ME', 'stat.ML']",A Bayesian Perspective of Statistical Machine Learning for Big Data,"Statistical Machine Learning (SML) refers to a body of algorithms and methods
by which computers are allowed to discover important features of input data
sets which are often very large in size. The very task of feature discovery
from data is essentially the meaning of the keyword `learning' in SML.
Theoretical justifications for the effectiveness of the SML algorithms are
underpinned by sound principles from different disciplines, such as Computer
Science and Statistics. The theoretical underpinnings particularly justified by
statistical inference methods are together termed as statistical learning
theory.
  This paper provides a review of SML from a Bayesian decision theoretic point
of view -- where we argue that many SML techniques are closely connected to
making inference by using the so called Bayesian paradigm. We discuss many
important SML techniques such as supervised and unsupervised learning, deep
learning, online learning and Gaussian processes especially in the context of
very large data sets where these are often employed. We present a dictionary
which maps the key concepts of SML from Computer Science and Statistics. We
illustrate the SML techniques with three moderately large data sets where we
also discuss many practical implementation issues. Thus the review is
especially targeted at statisticians and computer scientists who are aspiring
to understand and apply SML for moderately large to big data sets."
"['stat.ML', 'stat.ME']",Provable Estimation of the Number of Blocks in Block Models,"Community detection is a fundamental unsupervised learning problem for
unlabeled networks which has a broad range of applications. Many community
detection algorithms assume that the number of clusters $r$ is known apriori.
In this paper, we propose an approach based on semi-definite relaxations, which
does not require prior knowledge of model parameters like many existing convex
relaxation methods and recovers the number of clusters and the clustering
matrix exactly under a broad parameter regime, with probability tending to one.
On a variety of simulated and real data experiments, we show that the proposed
method often outperforms state-of-the-art techniques for estimating the number
of clusters."
"['stat.ML', 'stat.TH']",Simultaneous Clustering and Estimation of Heterogeneous Graphical Models,"We consider joint estimation of multiple graphical models arising from
heterogeneous and high-dimensional observations. Unlike most previous
approaches which assume that the cluster structure is given in advance, an
appealing feature of our method is to learn cluster structure while estimating
heterogeneous graphical models. This is achieved via a high dimensional version
of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).
A joint graphical lasso penalty is imposed on the conditional maximization step
to extract both homogeneity and heterogeneity components across all clusters.
Our algorithm is computationally efficient due to fast sparse learning routines
and can be implemented without unsupervised learning knowledge. The superior
performance of our method is demonstrated by extensive experiments and its
application to a Glioblastoma cancer dataset reveals some new insights in
understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound
is established for the output directly from our high dimensional ECM algorithm,
and it consists of two quantities: statistical error (statistical accuracy) and
optimization error (computational complexity). Such a result gives a
theoretical guideline in terminating our ECM iterations."
"['stat.ML', 'stat.CO']",The Variational Gaussian Process,"Variational inference is a powerful tool for approximate inference, and it
has been recently applied for representation learning with deep generative
models. We develop the variational Gaussian process (VGP), a Bayesian
nonparametric variational family, which adapts its shape to match complex
posterior distributions. The VGP generates approximate posterior samples by
generating latent inputs and warping them through random non-linear mappings;
the distribution over random mappings is learned during inference, enabling the
transformed outputs to adapt to varying complexity. We prove a universal
approximation theorem for the VGP, demonstrating its representative power for
learning any model. For inference we present a variational objective inspired
by auto-encoders and perform black box inference over a wide class of models.
The VGP achieves new state-of-the-art results for unsupervised learning,
inferring models such as the deep latent Gaussian model and the recently
proposed DRAW."
"['stat.ML', 'stat.ME']",SparseCodePicking: feature extraction in mass spectrometry using sparse coding algorithms,"Mass spectrometry (MS) is an important technique for chemical profiling which
calculates for a sample a high dimensional histogram-like spectrum. A crucial
step of MS data processing is the peak picking which selects peaks containing
information about molecules with high concentrations which are of interest in
an MS investigation. We present a new procedure of the peak picking based on a
sparse coding algorithm. Given a set of spectra of different classes, i.e. with
different positions and heights of the peaks, this procedure can extract peaks
by means of unsupervised learning. Instead of an $l_1$-regularization penalty
term used in the original sparse coding algorithm we propose using an
elastic-net penalty term for better regularization. The evaluation is done by
means of simulation. We show that for a large region of parameters the proposed
peak picking method based on the sparse coding features outperforms a mean
spectrum-based method. Moreover, we demonstrate the procedure applying it to
two real-life datasets."
"['stat.ML', 'stat.TH']",Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation,"In the past decade the mathematical theory of machine learning has lagged far
behind the triumphs of deep neural networks on practical challenges. However,
the gap between theory and practice is gradually starting to close. In this
paper I will attempt to assemble some pieces of the remarkable and still
incomplete mathematical mosaic emerging from the efforts to understand the
foundations of deep learning. The two key themes will be interpolation, and its
sibling, over-parameterization. Interpolation corresponds to fitting data, even
noisy data, exactly. Over-parameterization enables interpolation and provides
flexibility to select a right interpolating model.
  As we will see, just as a physical prism separates colors mixed within a ray
of light, the figurative prism of interpolation helps to disentangle
generalization and optimization properties within the complex picture of modern
Machine Learning. This article is written with belief and hope that clearer
understanding of these issues brings us a step closer toward a general theory
of deep learning and machine learning."
"['stat.ML', 'stat.TH']",Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices,"We consider two closely related problems: planted clustering and submatrix
localization. The planted clustering problem assumes that a random graph is
generated based on some underlying clusters of the nodes; the task is to
recover these clusters given the graph. The submatrix localization problem
concerns locating hidden submatrices with elevated means inside a large
real-valued random matrix. Of particular interest is the setting where the
number of clusters/submatrices is allowed to grow unbounded with the problem
size. These formulations cover several classical models such as planted clique,
planted densest subgraph, planted partition, planted coloring, and stochastic
block model, which are widely used for studying community detection and
clustering/bi-clustering.
  For both problems, we show that the space of the model parameters
(cluster/submatrix size, cluster density, and submatrix mean) can be
partitioned into four disjoint regions corresponding to decreasing statistical
and computational complexities: (1) the \emph{impossible} regime, where all
algorithms fail; (2) the \emph{hard} regime, where the computationally
expensive Maximum Likelihood Estimator (MLE) succeeds; (3) the \emph{easy}
regime, where the polynomial-time convexified MLE succeeds; (4) the
\emph{simple} regime, where a simple counting/thresholding procedure succeeds.
Moreover, we show that each of these algorithms provably fails in the previous
harder regimes.
  Our theorems establish the minimax recovery limit, which are tight up to
constants and hold with a growing number of clusters/submatrices, and provide a
stronger performance guarantee than previously known for polynomial-time
algorithms. Our study demonstrates the tradeoffs between statistical and
computational considerations, and suggests that the minimax recovery limit may
not be achievable by polynomial-time algorithms."
"['stat.ML', 'stat.TH']",Optimal classification in sparse Gaussian graphic model,"Consider a two-class classification problem where the number of features is
much larger than the sample size. The features are masked by Gaussian noise
with mean zero and covariance matrix $\Sigma$, where the precision matrix
$\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,
also unknown, are sparse and each contributes weakly (i.e., rare and weak) to
the classification decision. By obtaining a reasonably good estimate of
$\Omega$, we formulate the setting as a linear regression model. We propose a
two-stage classification method where we first select features by the method of
Innovated Thresholding (IT), and then use the retained features and Fisher's
LDA for classification. In this approach, a crucial problem is how to set the
threshold of IT. We approach this problem by adapting the recent innovation of
Higher Criticism Thresholding (HCT). We find that when useful features are rare
and weak, the limiting behavior of HCT is essentially just as good as the
limiting behavior of ideal threshold, the threshold one would choose if the
underlying distribution of the signals is known (if only). Somewhat
surprisingly, when $\Omega$ is sufficiently sparse, its off-diagonal
coordinates usually do not have a major influence over the classification
decision. Compared to recent work in the case where $\Omega$ is the identity
matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.
Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current
setting is much more general, which needs a new approach and much more
sophisticated analysis. One key component of the analysis is the intimate
relationship between HCT and Fisher's separation. Another key component is the
tight large-deviation bounds for empirical processes for data with
unconventional correlation structures, where graph theory on vertex coloring
plays an important role."
['stat.CO'],SoRC -- Evaluation of Computational Molecular Co-Localization Analysis in Mass Spectrometry Images,"The computational analysis of Mass Spectrometry Imaging (MSI) data aims at
the identification of interesting mass co-localizations and the visualization
of their lateral distribution in the sample, usually a tissue cross section.
But as the morphological structure of tissues and the different kinds of mass
co-localization naturally show a huge diversity, the selection and tuning of
the computational method is a time-consuming effort. In this work we address
the special problem of computationally grouping mass channel images according
to their similarities in their lateral distribution patterns. Such an analysis
is driven by the idea, that groups of molecules that feature a similar
distribution pattern may have a functional relation. But the selection of the
similarity function and other parameters is often done by a time-consuming and
unsatsifactory trial and error. We propose a new flexible workflow scheme
called SoRC (sum of ranked cluster indices) for automating this tuning step and
making it much more efficient. We test SoRC using three different data sets
acquired from the lab for three different kinds of samples (barley seed, mouse
bladder tissue, human PXE skin). We show, that SORC can be applied to score and
visualize the results obtained with the applied methods in short time without
too much effort. In our application example, the SoRC results for the three
data sets reveal that a) some well-known similarity functions are suited to
achieve good results for all three data sets and b) for the MSI data featuring
a higher degree of irregularity improved results can be achieved by applying
non-standard similarity functions. The SoRC scores computed with our approach
indicate that an automated testing and scoring of different methods for mass
channel image grouping can improve the final outcome of a study by finally
selecting the methods of the highest scores."
"['stat.ML', 'stat.ME']",SparseCodePicking: feature extraction in mass spectrometry using sparse coding algorithms,"Mass spectrometry (MS) is an important technique for chemical profiling which
calculates for a sample a high dimensional histogram-like spectrum. A crucial
step of MS data processing is the peak picking which selects peaks containing
information about molecules with high concentrations which are of interest in
an MS investigation. We present a new procedure of the peak picking based on a
sparse coding algorithm. Given a set of spectra of different classes, i.e. with
different positions and heights of the peaks, this procedure can extract peaks
by means of unsupervised learning. Instead of an $l_1$-regularization penalty
term used in the original sparse coding algorithm we propose using an
elastic-net penalty term for better regularization. The evaluation is done by
means of simulation. We show that for a large region of parameters the proposed
peak picking method based on the sparse coding features outperforms a mean
spectrum-based method. Moreover, we demonstrate the procedure applying it to
two real-life datasets."
['stat.TH'],Preserved central model for faster bidirectional compression in distributed settings,"We develop a new approach to tackle communication constraints in a
distributed learning problem with a central server. We propose and analyze a
new algorithm that performs bidirectional compression and achieves the same
convergence rate as algorithms using only uplink (from the local workers to the
central server) compression. To obtain this improvement, we design MCM, an
algorithm such that the downlink compression only impacts local models, while
the global model is preserved. As a result, and contrary to previous works, the
gradients on local servers are computed on perturbed models. Consequently,
convergence proofs are more challenging and require a precise control of this
perturbation. To ensure it, MCM additionally combines model compression with a
memory mechanism. This analysis opens new doors, e.g. incorporating worker
dependent randomized-models and partial participation."
"['stat.ME', 'stat.ML']","Distribution-Free, Risk-Controlling Prediction Sets","While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning."
"['stat.ML', 'stat.TH']",Computing Valid p-values for Image Segmentation by Selective Inference,"Image segmentation is one of the most fundamental tasks of computer vision.
In many practical applications, it is essential to properly evaluate the
reliability of individual segmentation results. In this study, we propose a
novel framework to provide the statistical significance of segmentation results
in the form of p-values. Specifically, we consider a statistical hypothesis
test for determining the difference between the object and the background
regions. This problem is challenging because the difference can be deceptively
large (called segmentation bias) due to the adaptation of the segmentation
algorithm to the data. To overcome this difficulty, we introduce a statistical
approach called selective inference, and develop a framework to compute valid
p-values in which the segmentation bias is properly accounted for. Although the
proposed framework is potentially applicable to various segmentation
algorithms, we focus in this paper on graph cut-based and threshold-based
segmentation algorithms, and develop two specific methods to compute valid
p-values for the segmentation results obtained by these algorithms. We prove
the theoretical validity of these two methods and demonstrate their
practicality by applying them to segmentation problems for medical images."
['stat.CO'],A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using Adaptive Plant Propagation Algorithm,"One of the most straightforward, direct and efficient approaches to Image
Segmentation is Image Thresholding. Multi-level Image Thresholding is an
essential viewpoint in many image processing and Pattern Recognition based
real-time applications which can effectively and efficiently classify the
pixels into various groups denoting multiple regions in an Image. Thresholding
based Image Segmentation using fuzzy entropy combined with intelligent
optimization approaches are commonly used direct methods to properly identify
the thresholds so that they can be used to segment an Image accurately. In this
paper a novel approach for multi-level image thresholding is proposed using
Type II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).
Obtaining the optimal thresholds for an image by maximizing the entropy is
extremely tedious and time consuming with increase in the number of thresholds.
Hence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based
on plant intelligence, is used for fast and efficient selection of optimal
thresholds. This fact is reasonably justified by comparing the accuracy of the
outcomes and computational time consumed by other modern state-of-the-art
algorithms such as Particle Swarm Optimization (PSO), Gravitational Search
Algorithm (GSA) and Genetic Algorithm (GA)."
"['stat.ML', 'stat.ME']",Improving Gibbs Sampler Scan Quality with DoGS,"The pairwise influence matrix of Dobrushin has long been used as an
analytical tool to bound the rate of convergence of Gibbs sampling. In this
work, we use Dobrushin influence as the basis of a practical tool to certify
and efficiently improve the quality of a discrete Gibbs sampler. Our
Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection
orders for a given sampling budget and variable subset of interest, explicit
bounds on total variation distance to stationarity, and certifiable
improvements over the standard systematic and uniform random scan Gibbs
samplers. In our experiments with joint image segmentation and object
recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising
model inference, DoGS consistently deliver higher-quality inferences with
significantly smaller sampling budgets than standard Gibbs samplers."
"['stat.ML', 'stat.ME']",A deep-structured fully-connected random field model for structured inference,"There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation."
['stat.CO'],Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means,"A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm
is proposed with iterative reweighted least squares (IRLS) algorithm, and
penalty constraint and kernelization extensions of MFCM algorithms are also
developed. Introducing penalty information to the object functions of MFCM
algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to
penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean
distance with kernel method, the MFCM and pMFCM algorithms are extended to
kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.
The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in
three tasks: pattern recognition on 10 standard data sets from UCI Machine
Learning databases, noise image segmentation performances on a synthetic image,
a magnetic resonance brain image (MRI), and image segmentation of a standard
images from Berkeley Segmentation Dataset and Benchmark. The experimental
results demonstrate the effectiveness of our proposed algorithms in pattern
recognition and image segmentation."
"['stat.ML', 'stat.TH']",Plugin procedure in segmentation and application to hyperspectral image segmentation,"In this article we give our contribution to the problem of segmentation with
plug-in procedures. We give general sufficient conditions under which plug in
procedure are efficient. We also give an algorithm that satisfy these
conditions. We give an application of the used algorithm to hyperspectral
images segmentation. Hyperspectral images are images that have both spatial and
spectral coherence with thousands of spectral bands on each pixel. In the
proposed procedure we combine a reduction dimension technique and a spatial
regularisation technique. This regularisation is based on the mixlet
modelisation of Kolaczyck and Al."
"['stat.ML', 'stat.TH']",Approximating Continuous Functions by ReLU Nets of Minimal Width,"This article concerns the expressive power of depth in deep feed-forward
neural nets with ReLU activations. Specifically, we answer the following
question: for a fixed $d_{in}\geq 1,$ what is the minimal width $w$ so that
neural nets with ReLU activations, input dimension $d_{in}$, hidden layer
widths at most $w,$ and arbitrary depth can approximate any continuous,
real-valued function of $d_{in}$ variables arbitrarily well? It turns out that
this minimal width is exactly equal to $d_{in}+1.$ That is, if all the hidden
layer widths are bounded by $d_{in}$, then even in the infinite depth limit,
ReLU nets can only express a very limited class of functions, and, on the other
hand, any continuous function on the $d_{in}$-dimensional unit cube can be
approximated to arbitrary precision by ReLU nets in which all hidden layers
have width exactly $d_{in}+1.$ Our construction in fact shows that any
continuous function $f:[0,1]^{d_{in}}\to\mathbb R^{d_{out}}$ can be
approximated by a net of width $d_{in}+d_{out}$. We obtain quantitative depth
estimates for such an approximation in terms of the modulus of continuity of
$f$."
"['stat.ML', 'stat.TH']",Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations,"This article concerns the expressive power of depth in neural nets with ReLU
activations and bounded width. We are particularly interested in the following
questions: what is the minimal width $w_{\text{min}}(d)$ so that ReLU nets of
width $w_{\text{min}}(d)$ (and arbitrary depth) can approximate any continuous
function on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this
minimal width, what can one say about the depth necessary to approximate a
given function? Our approach to this paper is based on the observation that,
due to the convexity of the ReLU activation, ReLU nets are particularly
well-suited for representing convex functions. In particular, we prove that
ReLU nets with width $d+1$ can approximate any continuous convex function of
$d$ variables arbitrarily well. These results then give quantitative depth
estimates for the rate of approximation of any continuous scalar function on
the $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$"
"['stat.ML', 'stat.TH']",Algorithms for ridge estimation with convergence guarantees,"The extraction of filamentary structure from a point cloud is discussed. The
filaments are modeled as ridge lines or higher dimensional ridges of an
underlying density. We propose two novel algorithms, and provide theoretical
guarantees for their convergences. We consider the new algorithms as
alternatives to the Subspace Constraint Mean Shift (SCMS) algorithm that do not
suffer from a shortcoming of the SCMS that is also revealed in this paper."
"['stat.ML', 'stat.ME', 'stat.TH']",Efficient Clustering for Stretched Mixtures: Landscape and Optimality,"This paper considers a canonical clustering problem where one receives
unlabeled samples drawn from a balanced mixture of two elliptical distributions
and aims for a classifier to estimate the labels. Many popular methods
including PCA and k-means require individual components of the mixture to be
somewhat spherical, and perform poorly when they are stretched. To overcome
this issue, we propose a non-convex program seeking for an affine transform to
turn the data into a one-dimensional point cloud concentrating around -1 and 1,
after which clustering becomes easy. Our theoretical contributions are
two-fold: (1) we show that the non-convex loss function exhibits desirable
landscape properties as long as the sample size exceeds some constant multiple
of the dimension, and (2) we leverage this to prove that an efficient
first-order algorithm achieves near-optimal statistical precision even without
good initialization. We also propose a general methodology for multi-class
clustering tasks with flexible choices of feature transforms and loss
objectives."
"['stat.ML', 'stat.TH']",Denoising Linear Models with Permuted Data,"The multivariate linear regression model with shuffled data and additive
Gaussian noise arises in various correspondence estimation and matching
problems. Focusing on the denoising aspect of this problem, we provide a
characterization the minimax error rate that is sharp up to logarithmic
factors. We also analyze the performance of two versions of a computationally
efficient estimator, and establish their consistency for a large range of input
parameters. Finally, we provide an exact algorithm for the noiseless problem
and demonstrate its performance on an image point-cloud matching task. Our
analysis also extends to datasets with outliers."
"['stat.ML', 'stat.TH']","Relax, no need to round: integrality of clustering formulations","We study exact recovery conditions for convex relaxations of point cloud
clustering problems, focusing on two of the most common optimization problems
for unsupervised clustering: $k$-means and $k$-median clustering. Motivations
for focusing on convex relaxations are: (a) they come with a certificate of
optimality, and (b) they are generic tools which are relatively parameter-free,
not tailored to specific assumptions over the input. More precisely, we
consider the distributional setting where there are $k$ clusters in
$\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a
symmetric distribution within a ball of unit radius. We ask: what is the
minimal separation distance between cluster centers needed for convex
relaxations to exactly recover these $k$ clusters as the optimal integral
solution? For the $k$-median linear programming relaxation we show a tight
bound: exact recovery is obtained given arbitrarily small pairwise separation
$\epsilon > 0$ between the balls. In other words, the pairwise center
separation is $\Delta > 2+\epsilon$. Under the same distributional model, the
$k$-means LP relaxation fails to recover such clusters at separation as large
as $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get
exact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$.
In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means
algorithm) can fail to recover clusters in this setting; even with arbitrarily
large cluster separation, k-means++ with overseeding by any constant factor
fails with high probability at exact cluster recovery. To complement the
theoretical analysis, we provide an experimental study of the recovery
guarantees for these various methods, and discuss several open problems which
these experiments suggest."
"['stat.ML', 'stat.TH']",Consistency of Cheeger and Ratio Graph Cuts,"This paper establishes the consistency of a family of graph-cut-based
algorithms for clustering of data clouds. We consider point clouds obtained as
samples of a ground-truth measure. We investigate approaches to clustering
based on minimizing objective functionals defined on proximity graphs of the
given sample. Our focus is on functionals based on graph cuts like the Cheeger
and ratio cuts. We show that minimizers of the these cuts converge as the
sample size increases to a minimizer of a corresponding continuum cut (which
partitions the ground truth measure). Moreover, we obtain sharp conditions on
how the connectivity radius can be scaled with respect to the number of sample
points for the consistency to hold. We provide results for two-way and for
multiway cuts. Furthermore we provide numerical experiments that illustrate the
results and explore the optimality of scaling in dimension two."
"['stat.ML', 'stat.CO']",Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models,"Describing the relationship between the variables in a study domain and
modelling the data generating mechanism is a fundamental problem in many
empirical sciences. Probabilistic graphical models are one common approach to
tackle the problem. Learning the graphical structure is computationally
challenging and a fervent area of current research with a plethora of
algorithms being developed. To facilitate the benchmarking of different
methods, we present a novel automated workflow, called benchpress for producing
scalable, reproducible, and platform-independent benchmarks of structure
learning algorithms for probabilistic graphical models. Benchpress is
interfaced via a simple JSON-file, which makes it accessible for all users,
while the code is designed in a fully modular fashion to enable researchers to
contribute additional methodologies. Benchpress currently provides an interface
to a large number of state-of-the-art algorithms from libraries such as
BDgraph, BiDAG, bnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and
trilearn as well as a variety of methods for data generating models and
performance evaluation. Alongside user-defined models and randomly generated
datasets, the software tool also includes a number of standard datasets and
graphical models from the literature, which may be included in a benchmarking
workflow. We demonstrate the applicability of this workflow for learning
Bayesian networks in four typical data scenarios. The source code and
documentation is publicly available from
http://github.com/felixleopoldo/benchpress."
"['stat.ML', 'stat.ME', 'stat.TH']",Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation,"We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Specially, under both
Gaussian and sub-Gaussian models, we study the sample size conditions for the
well-known Chow-Liu algorithm to exactly recover the equivalence class of the
polytree, which is uniquely represented by a CPDAG. We also study the error
rate for the estimation of the inverse correlation matrix under such models.
Our theoretical findings are illustrated by comprehensive numerical
simulations, and experiments on benchmark data also demonstrate the robustness
of the method when the ground truth graphical structure can only be
approximated by a polytree."
"['stat.ML', 'stat.TH']",Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions,"We develop simple methods for constructing parameter priors for model choice
among Directed Acyclic Graphical (DAG) models. In particular, we introduce
several assumptions that permit the construction of parameter priors for a
large number of DAG models from a small set of assessments. We then present a
method for directly computing the marginal likelihood of every DAG model given
a random sample with no missing observations. We apply this methodology to
Gaussian DAG models which consist of a recursive set of linear regression
models. We show that the only parameter prior for complete Gaussian DAG models
that satisfies our assumptions is the normal-Wishart distribution. Our analysis
is based on the following new characterization of the Wishart distribution: let
$W$ be an $n \times n$, $n \ge 3$, positive-definite symmetric matrix of random
variables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if
and only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of
$\{W_{12},W_{22}\}$ for every block partitioning $W_{11},W_{12}, W'_{12},
W_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart
distributions are provided as well."
"['stat.ML', 'stat.ME', 'stat.TH']",Joint Estimation and Inference for Data Integration Problems based on Multiple Multi-layered Gaussian Graphical Models,"The rapid development of high-throughput technologies has enabled the
generation of data from biological or disease processes that span multiple
layers, like genomic, proteomic or metabolomic data, and further pertain to
multiple sources, like disease subtypes or experimental conditions. In this
work, we propose a general statistical framework based on Gaussian graphical
models for horizontal (i.e. across conditions or subtypes) and vertical (i.e.
across different layers containing data on molecular compartments) integration
of information in such datasets. We start with decomposing the multi-layer
problem into a series of two-layer problems. For each two-layer problem, we
model the outcomes at a node in the lower layer as dependent on those of other
nodes in that layer, as well as all nodes in the upper layer. We use a
combination of neighborhood selection and group-penalized regression to obtain
sparse estimates of all model parameters. Following this, we develop a
debiasing technique and asymptotic distributions of inter-layer directed edge
weights that utilize already computed neighborhood selection coefficients for
nodes in the upper layer. Subsequently, we establish global and simultaneous
testing procedures for these edge weights. Performance of the proposed
methodology is evaluated on synthetic and real data."
['stat.TH'],Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models,"We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal."
"['stat.ML', 'stat.CO']",SG-PALM: a Fast Physically Interpretable Tensor Graphical Model,"We propose a new graphical model inference procedure, called SG-PALM, for
learning conditional dependency structure of high-dimensional tensor-variate
data. Unlike most other tensor graphical models the proposed model is
interpretable and computationally scalable to high dimension. Physical
interpretability follows from the Sylvester generative (SG) model on which
SG-PALM is based: the model is exact for any observation process that is a
solution of a partial differential equation of Poisson type. Scalability
follows from the fast proximal alternating linearized minimization (PALM)
procedure that SG-PALM uses during training. We establish that SG-PALM
converges linearly (i.e., geometric convergence rate) to a global optimum of
its objective function. We demonstrate the scalability and accuracy of SG-PALM
for an important but challenging climate prediction problem: spatio-temporal
forecasting of solar flares from multimodal imaging data."
['stat.ME'],Definite Non-Ancestral Relations and Structure Learning,"In causal graphical models based on directed acyclic graphs (DAGs), directed
paths represent causal pathways between the corresponding variables. The
variable at the beginning of such a path is referred to as an ancestor of the
variable at the end of the path. Ancestral relations between variables play an
important role in causal modeling. In existing literature on structure
learning, these relations are usually deduced from learned structures and used
for orienting edges or formulating constraints of the space of possible DAGs.
However, they are usually not posed as immediate target of inference. In this
work we investigate the graphical characterization of ancestral relations via
CPDAGs and d-separation relations. We propose a framework that can learn
definite non-ancestral relations without first learning the skeleton. This
frame-work yields structural information that can be used in both score- and
constraint-based algorithms to learn causal DAGs more efficiently."
"['stat.CO', 'stat.ME']",Temporal Gaussian Process Regression in Logarithmic Time,"The aim of this article is to present a novel parallelization method for
temporal Gaussian process (GP) regression problems. The method allows for
solving GP regression problems in logarithmic O(log N) time, where N is the
number of time steps. Our approach uses the state-space representation of GPs
which in its original form allows for linear O(N) time GP regression by
leveraging the Kalman filtering and smoothing methods. By using a recently
proposed parallelization method for Bayesian filters and smoothers, we are able
to reduce the linear computational complexity of the temporal GP regression
problems into logarithmic span complexity. This ensures logarithmic time
complexity when run on parallel hardware such as a graphics processing unit
(GPU). We experimentally demonstrate the computational benefits on simulated
and real datasets via our open-source implementation leveraging the GPflow
framework."
"['stat.ML', 'stat.ME']",High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach,"Undirected graphical models have been widely used to model the conditional
independence structure of high-dimensional random vector data for years. In
many modern applications such as EEG and fMRI data, the observations are
multivariate random functions rather than scalars. To model the conditional
independence of this type of data, functional graphical models are proposed and
have attracted an increasing attention in recent years. In this paper, we
propose a neighborhood selection approach to estimate Gaussian functional
graphical models. We first estimate the neighborhood of all nodes via
function-on-function regression, and then we can recover the whole graph
structure based on the neighborhood information. By estimating conditional
structure directly, we can circumvent the need of a well-defined precision
operator which generally does not exist. Besides, we can better explore the
effect of the choice of function basis for dimension reduction. We give a
criterion for choosing the best function basis and motivate two practically
useful choices, which we justified by both theory and experiments and show that
they are better than expanding each function onto its own FPCA basis as in
previous literature. In addition, the neighborhood selection approach is
computationally more efficient than fglasso as it is more easy to do parallel
computing. The statistical consistency of our proposed methods in
high-dimensional setting are supported by both theory and experiment."
['stat.TH'],Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables,"The problem of selecting optimal valid backdoor adjustment sets to estimate
causal effects in graphical models with hidden and conditioned variables is
addressed. Previous work has defined optimality as achieving the smallest
asymptotic variance compared to other adjustment sets and identified a
graphical criterion for an optimal set for the case without hidden variables.
For the case with hidden variables currently a sufficient graphical criterion
and a corresponding construction algorithm exists. Here optimality is
characterized by an information-theoretic approach based on the conditional
mutual informations among cause, effect, adjustment set, and conditioned
variables. This characterization allows to derive the main contributions of
this paper: A necessary and sufficient graphical criterion for the existence of
an optimal adjustment set and a definition and algorithm to construct it.
Further, the optimal set is valid if and only if a valid adjustment set exists
and has smaller (or equal) asymptotic variance compared to the Adjust-set
proposed in Perkovic et al. (2018) (arXiv:1606.06903) for any graph, whether
graphical optimality holds or not. The results are valid for a class of
estimators whose asymptotic variance follows a certain information-theoretic
relation. Numerical experiments indicate that the asymptotic results also hold
for relatively small sample sizes. For estimators outside of the class studied
here none of the considered adjustment sets outperforms all others, but a
minimized variant of the optimal set proposed here tends to have lower
variance. Surprisingly, among the randomly created setups more than 80\%
fulfill the optimality conditions indicating that also in many real-world
scenarios graphical optimality may hold. Code is available as part of the
python package \url{https://github.com/jakobrunge/tigramite}."
"['stat.ML', 'stat.ME']",Thresholded Graphical Lasso Adjusts for Latent Variables: Application to Functional Neural Connectivity,"In neuroscience, researchers seek to uncover the connectivity of neurons from
large-scale neural recordings or imaging; often people employ graphical model
selection and estimation techniques for this purpose. But, existing
technologies can only record from a small subset of neurons leading to a
challenging problem of graph selection in the presence of extensive latent
variables. Chandrasekaran et al. (2012) proposed a convex program to address
this problem that poses challenges from both a computational and statistical
perspective. To solve this problem, we propose an incredibly simple solution:
apply a hard thresholding operator to existing graph selection methods.
Conceptually simple and computationally attractive, we demonstrate that
thresholding the graphical Lasso, neighborhood selection, or CLIME estimators
have superior theoretical properties in terms of graph selection consistency as
well as stronger empirical results than existing approaches for the latent
variable graphical model problem. We also demonstrate the applicability of our
approach through a neuroscience case study on calcium-imaging data to estimate
functional neural connections."
"['stat.ML', 'stat.ME']","Graph matching between bipartite and unipartite networks: to collapse, or not to collapse, that is the question","Graph matching consists of aligning the vertices of two unlabeled graphs in
order to maximize the shared structure across networks; when the graphs are
unipartite, this is commonly formulated as minimizing their edge disagreements.
In this paper, we address the common setting in which one of the graphs to
match is a bipartite network and one is unipartite. Commonly, the bipartite
networks are collapsed or projected into a unipartite graph, and graph matching
proceeds as in the classical setting. This potentially leads to noisy edge
estimates and loss of information. We formulate the graph matching problem
between a bipartite and a unipartite graph using an undirected graphical model,
and introduce methods to find the alignment with this model without collapsing.
We theoretically demonstrate that our methodology is consistent, and provide
non-asymptotic conditions that ensure exact recovery of the matching solution.
In simulations and real data examples, we show how our methods can result in a
more accurate matching than the naive approach of transforming the bipartite
networks into unipartite, and we demonstrate the performance gains achieved by
our method in simulated and real data networks, including a
co-authorship-citation network pair, and brain structural and functional data."
['stat.ME'],A Quotient Space Formulation for Generative Statistical Analysis of Graphical Data,"Complex analyses involving multiple, dependent random quantities often lead
to graphical models - a set of nodes denoting variables of interest, and
corresponding edges denoting statistical interactions between nodes. To develop
statistical analyses for graphical data, especially towards generative
modeling, one needs mathematical representations and metrics for matching and
comparing graphs, and subsequent tools, such as geodesics, means, and
covariances. This paper utilizes a quotient structure to develop efficient
algorithms for computing these quantities, leading to useful statistical tools,
including principal component analysis, statistical testing, and modeling. We
demonstrate the efficacy of this framework using datasets taken from several
problem areas, including letters, biochemical structures, and social networks."
"['stat.ML', 'stat.ME']",Thresholded Adaptive Validation: Tuning the Graphical Lasso for Graph Recovery,"Many Machine Learning algorithms are formulated as regularized optimization
problems, but their performance hinges on a regularization parameter that needs
to be calibrated to each application at hand. In this paper, we propose a
general calibration scheme for regularized optimization problems and apply it
to the graphical lasso, which is a method for Gaussian graphical modeling. The
scheme is equipped with theoretical guarantees and motivates a thresholding
pipeline that can improve graph recovery. Moreover, requiring at most one line
search over the regularization path, the calibration scheme is computationally
more efficient than competing schemes that are based on resampling. Finally, we
show in simulations that our approach can improve on the graph recovery of
other approaches considerably."
"['stat.ML', 'stat.ME']",Efficient Variational Bayesian Structure Learning of Dynamic Graphical Models,"Estimating time-varying graphical models are of paramount importance in
various social, financial, biological, and engineering systems, since the
evolution of such networks can be utilized for example to spot trends, detect
anomalies, predict vulnerability, and evaluate the impact of interventions.
Existing methods require extensive tuning of parameters that control the graph
sparsity and temporal smoothness. Furthermore, these methods are
computationally burdensome with time complexity O(NP^3) for P variables and N
time points. As a remedy, we propose a low-complexity tuning-free Bayesian
approach, named BADGE. Specifically, we impose temporally-dependent
spike-and-slab priors on the graphs such that they are sparse and varying
smoothly across time. A variational inference algorithm is then derived to
learn the graph structures from the data automatically. Owning to the
pseudo-likelihood and the mean-field approximation, the time complexity of
BADGE is only O(NP^2). Additionally, by identifying the frequency-domain
resemblance to the time-varying graphical models, we show that BADGE can be
extended to learning frequency-varying inverse spectral density matrices, and
yields graphical models for multivariate stationary time series. Numerical
results on both synthetic and real data show that that BADGE can better recover
the underlying true graphs, while being more efficient than the existing
methods, especially for high-dimensional cases."
"['stat.ML', 'stat.TH']",Predictive Learning on Hidden Tree-Structured Ising Models,"We provide high-probability sample complexity guarantees for exact structure
recovery and accurate predictive learning using noise-corrupted samples from an
acyclic (tree-shaped) graphical model. The hidden variables follow a
tree-structured Ising model distribution, whereas the observable variables are
generated by a binary symmetric channel taking the hidden variables as its
input (flipping each bit independently with some constant probability $q\in
[0,1/2)$). In the absence of noise, predictive learning on Ising models was
recently studied by Bresler and Karzand (2020); this paper quantifies how noise
in the hidden model impacts the tasks of structure recovery and marginal
distribution estimation by proving upper and lower bounds on the sample
complexity. Our results generalize state-of-the-art bounds reported in prior
work, and they exactly recover the noiseless case ($q=0$). In fact, for any
tree with $p$ vertices and probability of incorrect recovery $\delta>0$, the
sufficient number of samples remains logarithmic as in the noiseless case,
i.e., $\mathcal{O}(\log(p/\delta))$, while the dependence on $q$ is
$\mathcal{O}\big( 1/(1-2q)^{4} \big)$, for both aforementioned tasks. We also
present a new equivalent of Isserlis' Theorem for sign-valued tree-structured
distributions, yielding a new low-complexity algorithm for higher-order moment
estimation."
"['stat.ML', 'stat.TH']",Robust estimation of tree structured models,"Consider the problem of learning undirected graphical models on trees from
corrupted data. Recently Katiyar et al. showed that it is possible to recover
trees from noisy binary data up to a small equivalence class of possible trees.
Their other paper on the Gaussian case follows a similar pattern. By framing
this as a special phylogenetic recovery problem we largely generalize these two
settings. Using the framework of linear latent tree models we discuss tree
identifiability for binary data under a continuous corruption model. For the
Ising and the Gaussian tree model we also provide a characterisation of when
the Chow-Liu algorithm consistently learns the underlying tree from the noisy
data."
['stat.TH'],Few-shot time series segmentation using prototype-defined infinite hidden Markov models,"We propose a robust framework for interpretable, few-shot analysis of
non-stationary sequential data based on flexible graphical models to express
the structured distribution of sequential events, using prototype radial basis
function (RBF) neural network emissions. A motivational link is demonstrated
between prototypical neural network architectures for few-shot learning and the
proposed RBF network infinite hidden Markov model (RBF-iHMM). We show that RBF
networks can be efficiently specified via prototypes allowing us to express
complex nonstationary patterns, while hidden Markov models are used to infer
principled high-level Markov dynamics. The utility of the framework is
demonstrated on biomedical signal processing applications such as automated
seizure detection from EEG data where RBF networks achieve state-of-the-art
performance using a fraction of the data needed to train long-short-term memory
variational autoencoders."
"['stat.CO', 'stat.ML']",Scalable Inference of Sparsely-changing Markov Random Fields with Strong Statistical Guarantees,"In this paper, we study the problem of inferring time-varying Markov random
fields (MRF), where the underlying graphical model is both sparse and changes
sparsely over time. Most of the existing methods for the inference of
time-varying MRFs rely on the regularized maximum likelihood estimation (MLE),
that typically suffer from weak statistical guarantees and high computational
time. Instead, we introduce a new class of constrained optimization problems
for the inference of sparsely-changing MRFs. The proposed optimization problem
is formulated based on the exact $\ell_0$ regularization, and can be solved in
near-linear time and memory. Moreover, we show that the proposed estimator
enjoys a provably small estimation error. As a special case, we derive sharp
statistical guarantees for the inference of sparsely-changing Gaussian MRFs
(GMRF) in the high-dimensional regime, showing that such problems can be
learned with as few as one sample per time. Our proposed method is extremely
efficient in practice: it can accurately estimate sparsely-changing graphical
models with more than 500 million variables in less than one hour."
"['stat.ML', 'stat.CO']",Learning non-Gaussian graphical models via Hessian scores and triangular transport,"Undirected probabilistic graphical models represent the conditional
dependencies, or Markov properties, of a collection of random variables.
Knowing the sparsity of such a graphical model is valuable for modeling
multivariate distributions and for efficiently performing inference. While the
problem of learning graph structure from data has been studied extensively for
certain parametric families of distributions, most existing methods fail to
consistently recover the graph structure for non-Gaussian data. Here we propose
an algorithm for learning the Markov structure of continuous and non-Gaussian
distributions. To characterize conditional independence, we introduce a score
based on integrated Hessian information from the joint log-density, and we
prove that this score upper bounds the conditional mutual information for a
general class of distributions. To compute the score, our algorithm SING
estimates the density using a deterministic coupling, induced by a triangular
transport map, and iteratively exploits sparse structure in the map to reveal
sparsity in the graph. For certain non-Gaussian datasets, we show that our
algorithm recovers the graph structure even with a biased approximation to the
density. Among other examples, we apply sing to learn the dependencies between
the states of a chaotic dynamical system with local interactions."
"['stat.ML', 'stat.ME']",Hard and Soft EM in Bayesian Network Learning from Incomplete Data,"Incomplete data are a common feature in many domains, from clinical trials to
industrial applications. Bayesian networks (BNs) are often used in these
domains because of their graphical and causal interpretations. BN parameter
learning from incomplete data is usually implemented with the
Expectation-Maximisation algorithm (EM), which computes the relevant sufficient
statistics (""soft EM"") using belief propagation. Similarly, the Structural
Expectation-Maximisation algorithm (Structural EM) learns the network structure
of the BN from those sufficient statistics using algorithms designed for
complete data. However, practical implementations of parameter and structure
learning often impute missing data (""hard EM"") to compute sufficient statistics
instead of using belief propagation, for both ease of implementation and
computational speed. In this paper, we investigate the question: what is the
impact of using imputation instead of belief propagation on the quality of the
resulting BNs? From a simulation study using synthetic data and reference BNs,
we find that it is possible to recommend one approach over the other in several
scenarios based on the characteristics of the data. We then use this
information to build a simple decision tree to guide practitioners in choosing
the EM algorithm best suited to their problem."
"['stat.ML', 'stat.CO', 'stat.ME']",Categorical exploratory data analysis on goodness-of-fit issues,"If the aphorism ""All models are wrong""- George Box, continues to be true in
data analysis, particularly when analyzing real-world data, then we should
annotate this wisdom with visible and explainable data-driven patterns. Such
annotations can critically shed invaluable light on validity as well as
limitations of statistical modeling as a data analysis approach. In an effort
to avoid holding our real data to potentially unattainable or even unrealistic
theoretical structures, we propose to utilize the data analysis paradigm called
Categorical Exploratory Data Analysis (CEDA). We illustrate the merits of this
proposal with two real-world data sets from the perspective of goodness-of-fit.
In both data sets, the Normal distribution's bell shape seemingly fits rather
well by first glance. We apply CEDA to bring out where and how each data fits
or deviates from the model shape via several important distributional aspects.
We also demonstrate that CEDA affords a version of tree-based p-value, and
compare it with p-values based on traditional statistical approaches. Along our
data analysis, we invest computational efforts in making graphic display to
illuminate the advantages of using CEDA as one primary way of data analysis in
Data Science education."
"['stat.ML', 'stat.TH']",A polynomial-time algorithm for learning nonparametric causal graphs,"We establish finite-sample guarantees for a polynomial-time algorithm for
learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from
data. The analysis is model-free and does not assume linearity, additivity,
independent noise, or faithfulness. Instead, we impose a condition on the
residual variances that is closely related to previous work on linear models
with equal variances. Compared to an optimal algorithm with oracle knowledge of
the variable ordering, the additional cost of the algorithm is linear in the
dimension $d$ and the number of samples $n$. Finally, we compare the proposed
algorithm to existing approaches in a simulation study."
"['stat.ML', 'stat.ME']",Change point detection for graphical models in the presence of missing values,"We propose estimation methods for change points in high-dimensional
covariance structures with an emphasis on challenging scenarios with missing
values. We advocate three imputation like methods and investigate their
implications on common losses used for change point detection. We also discuss
how model selection methods have to be adapted to the setting of incomplete
data. The methods are compared in a simulation study and applied to a time
series from an environmental monitoring system. An implementation of our
proposals within the R-package hdcd is available via the Supplementary
materials."
"['stat.ML', 'stat.ME']",Learning Exponential Family Graphical Models with Latent Variables using Regularized Conditional Likelihood,"Fitting a graphical model to a collection of random variables given sample
observations is a challenging task if the observed variables are influenced by
latent variables, which can induce significant confounding statistical
dependencies among the observed variables. We present a new convex relaxation
framework based on regularized conditional likelihood for latent-variable
graphical modeling in which the conditional distribution of the observed
variables conditioned on the latent variables is given by an exponential family
graphical model. In comparison to previously proposed tractable methods that
proceed by characterizing the marginal distribution of the observed variables,
our approach is applicable in a broader range of settings as it does not
require knowledge about the specific form of distribution of the latent
variables and it can be specialized to yield tractable approaches to problems
in which the observed data are not well-modeled as Gaussian. We demonstrate the
utility and flexibility of our framework via a series of numerical experiments
on synthetic as well as real data."
"['stat.ML', 'stat.TH']",Learning Linear Non-Gaussian Graphical Models with Multidirected Edges,"In this paper we propose a new method to learn the underlying acyclic mixed
graph of a linear non-Gaussian structural equation model given observational
data. We build on an algorithm proposed by Wang and Drton, and we show that one
can augment the hidden variable structure of the recovered model by learning
{\em multidirected edges} rather than only directed and bidirected ones.
Multidirected edges appear when more than two of the observed variables have a
hidden common cause. We detect the presence of such hidden causes by looking at
higher order cumulants and exploiting the multi-trek rule. Our method recovers
the correct structure when the underlying graph is a bow-free acyclic mixed
graph with potential multi-directed edges."
"['stat.ML', 'stat.ME']",Regularized K-means through hard-thresholding,"We study a framework of regularized $K$-means methods based on direct
penalization of the size of the cluster centers. Different penalization
strategies are considered and compared through simulation and theoretical
analysis. Based on the results, we propose HT $K$-means, which uses an $\ell_0$
penalty to induce sparsity in the variables. Different techniques for selecting
the tuning parameter are discussed and compared. The proposed method stacks up
favorably with the most popular regularized $K$-means methods in an extensive
simulation study. Finally, HT $K$-means is applied to several real data
examples. Graphical displays are presented and used in these examples to gain
more insight into the datasets."
"['stat.ML', 'stat.TH']",Local Linear Forests,"Random forests are a powerful method for non-parametric regression, but are
limited in their ability to fit smooth signals, and can show poor predictive
performance in the presence of strong, smooth effects. Taking the perspective
of random forests as an adaptive kernel method, we pair the forest kernel with
a local linear regression adjustment to better capture smoothness. The
resulting procedure, local linear forests, enables us to improve on asymptotic
rates of convergence for random forests with smooth signals, and provides
substantial gains in accuracy on both real and simulated data. We prove a
central limit theorem valid under regularity conditions on the forest and
smoothness constraints, and propose a computationally efficient construction
for confidence intervals. Moving to a causal inference application, we discuss
the merits of local regression adjustments for heterogeneous treatment effect
estimation, and give an example on a dataset exploring the effect word choice
has on attitudes to the social safety net. Last, we include simulation results
on real and generated data."
"['stat.ME', 'stat.ML']","Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to LIFT, ROC & BIRD","Evaluation often aims to reduce the correctness or error characteristics of a
system down to a single number, but that always involves trade-offs. Another
way of dealing with this is to quote two numbers, such as Recall and Precision,
or Sensitivity and Specificity. But it can also be useful to see more than
this, and a graphical approach can explore sensitivity to cost, prevalence,
bias, noise, parameters and hyper-parameters.
  Moreover, most techniques are implicitly based on two balanced classes, and
our ability to visualize graphically is intrinsically two dimensional, but we
often want to visualize in a multiclass context. We review the dichotomous
approaches relating to Precision, Recall, and ROC as well as the related LIFT
chart, exploring how they handle unbalanced and multiclass data, and deriving
new probabilistic and information theoretic variants of LIFT that help deal
with the issues associated with the handling of multiple and unbalanced
classes."
"['stat.ML', 'stat.CO', 'stat.ME']",Deep Gaussian Markov Random Fields,"Gaussian Markov random fields (GMRFs) are probabilistic graphical models
widely used in spatial statistics and related fields to model dependencies over
spatial structures. We establish a formal connection between GMRFs and
convolutional neural networks (CNNs). Common GMRFs are special cases of a
generative model where the inverse mapping from data to latent variables is
given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to
multi-layer CNN architectures, effectively increasing the order of the
corresponding GMRF in a way which has favorable computational scaling. We
describe how well-established tools, such as autodiff and variational
inference, can be used for simple and efficient inference and learning of the
deep GMRF. We demonstrate the flexibility of the proposed model and show that
it outperforms the state-of-the-art on a dataset of satellite temperatures, in
terms of prediction and predictive uncertainty."
['stat.ME'],Latent Instrumental Variables as Priors in Causal Inference based on Independence of Cause and Mechanism,"Causal inference methods based on conditional independence construct Markov
equivalent graphs, and cannot be applied to bivariate cases. The approaches
based on independence of cause and mechanism state, on the contrary, that
causal discovery can be inferred for two observations. In our contribution, we
challenge to reconcile these two research directions. We study the role of
latent variables such as latent instrumental variables and hidden common causes
in the causal graphical structures. We show that the methods based on the
independence of cause and mechanism, indirectly contain traces of the existence
of the hidden instrumental variables. We derive a novel algorithm to infer
causal relationships between two variables, and we validate the proposed method
on simulated data and on a benchmark of cause-effect pairs. We illustrate by
our experiments that the proposed approach is simple and extremely competitive
in terms of empirical accuracy compared to the state-of-the-art methods."
"['stat.ML', 'stat.TH']",Efficient Learning of Discrete Graphical Models,"Graphical models are useful tools for describing structured high-dimensional
probability distributions. Development of efficient algorithms for learning
graphical models with least amount of data remains an active research topic.
Reconstruction of graphical models that describe the statistics of discrete
variables is a particularly challenging problem, for which the maximum
likelihood approach is intractable. In this work, we provide the first
sample-efficient method based on the Interaction Screening framework that
allows one to provably learn fully general discrete factor models with
node-specific discrete alphabets and multi-body interactions, specified in an
arbitrary basis. We identify a single condition related to model
parametrization that leads to rigorous guarantees on the recovery of model
structure and parameters in any error norm, and is readily verifiable for a
large class of models. Importantly, our bounds make explicit distinction
between parameters that are proper to the model and priors used as an input to
the algorithm. Finally, we show that the Interaction Screening framework
includes all models previously considered in the literature as special cases,
and for which our analysis shows a systematic improvement in sample complexity."
"['stat.ML', 'stat.CO']",algcomparison: Comparing the Performance of Graphical Structure Learning Algorithms with TETRAD,"In this report we describe a tool for comparing the performance of graphical
causal structure learning algorithms implemented in the TETRAD freeware suite
of causal analysis methods. Currently the tool is available as package in the
TETRAD source code (written in Java). Simulations can be done varying the
number of runs, sample sizes, and data modalities. Performance on this
simulated data can then be compared for a number of algorithms, with parameters
varied and with performance statistics as selected, producing a publishable
report. The package presented here may also be used to compare structure
learning methods across platforms and programming languages, i.e., to compare
algorithms implemented in TETRAD with those implemented in MATLAB, Python, or
R."
"['stat.ML', 'stat.TH']",The Many-to-Many Mapping Between the Concordance Correlation Coefficient and the Mean Square Error,"We derive the mapping between two of the most pervasive utility functions,
the mean square error ($MSE$) and the concordance correlation coefficient (CCC,
$\rho_c$). Despite its drawbacks, $MSE$ is one of the most popular performance
metrics (and a loss function); along with lately $\rho_c$ in many of the
sequence prediction challenges. Despite the ever-growing simultaneous usage,
e.g., inter-rater agreement, assay validation, a mapping between the two
metrics is missing, till date. While minimisation of $L_p$ norm of the errors
or of its positive powers (e.g., $MSE$) is aimed at $\rho_c$ maximisation, we
reason the often-witnessed ineffectiveness of this popular loss function with
graphical illustrations. The discovered formula uncovers not only the
counterintuitive revelation that `$MSE_1<MSE_2$' does not imply
`$\rho_{c_1}>\rho_{c_2}$', but also provides the precise range for the $\rho_c$
metric for a given $MSE$. We discover the conditions for $\rho_c$ optimisation
for a given $MSE$; and as a logical next step, for a given set of errors. We
generalise and discover the conditions for any given $L_p$ norm, for an even p.
We present newly discovered, albeit apparent, mathematical paradoxes. The study
inspires and anticipates a growing use of $\rho_c$-inspired loss functions
e.g., $\left|\frac{MSE}{\sigma_{XY}}\right|$, replacing the traditional
$L_p$-norm loss functions in multivariate regressions."
"['stat.ML', 'stat.CO', 'stat.TH']",Efficient Statistics for Sparse Graphical Models from Truncated Samples,"In this paper, we study high-dimensional estimation from truncated samples.
We focus on two fundamental and classical problems: (i) inference of sparse
Gaussian graphical models and (ii) support recovery of sparse linear models.
  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\bf x}$
are generated from a Gaussian $N(\mu,\Sigma)$ and observed only if they belong
to a subset $S \subseteq \mathbb{R}^d$. We show that ${\mu}$ and ${\Sigma}$ can
be estimated with error $\epsilon$ in the Frobenius norm, using
$\tilde{O}\left(\frac{\textrm{nz}({\Sigma}^{-1})}{\epsilon^2}\right)$ samples
from a truncated $\mathcal{N}({\mu},{\Sigma})$ and having access to a
membership oracle for $S$. The set $S$ is assumed to have non-trivial measure
under the unknown distribution but is otherwise arbitrary.
  (ii) For sparse linear regression, suppose samples $({\bf x},y)$ are
generated where $y = {\bf x}^\top{{\Omega}^*} + \mathcal{N}(0,1)$ and $({\bf
x}, y)$ is seen only if $y$ belongs to a truncation set $S \subseteq
\mathbb{R}$. We consider the case that ${\Omega}^*$ is sparse with a support
set of size $k$. Our main result is to establish precise conditions on the
problem dimension $d$, the support size $k$, the number of observations $n$,
and properties of the samples and the truncation that are sufficient to recover
the support of ${\Omega}^*$. Specifically, we show that under some mild
assumptions, only $O(k^2 \log d)$ samples are needed to estimate ${\Omega}^*$
in the $\ell_\infty$-norm up to a bounded error.
  For both problems, our estimator minimizes the sum of the finite population
negative log-likelihood function and an $\ell_1$-regularization term."
"['stat.ML', 'stat.ME']",Imputation estimators for unnormalized models with missing data,"Several statistical models are given in the form of unnormalized densities,
and calculation of the normalization constant is intractable. We propose
estimation methods for such unnormalized models with missing data. The key
concept is to combine imputation techniques with estimators for unnormalized
models including noise contrastive estimation and score matching. In addition,
we derive asymptotic distributions of the proposed estimators and construct
confidence intervals. Simulation results with truncated Gaussian graphical
models and the application to real data of wind direction reveal that the
proposed methods effectively enable statistical inference with unnormalized
models from missing data."
"['stat.ML', 'stat.TH']",High-Dimensional Inference for Cluster-Based Graphical Models,"Motivated by modern applications in which one constructs graphical models
based on a very large number of features, this paper introduces a new class of
cluster-based graphical models, in which variable clustering is applied as an
initial step for reducing the dimension of the feature space. We employ model
assisted clustering, in which the clusters contain features that are similar to
the same unobserved latent variable. Two different cluster-based Gaussian
graphical models are considered: the latent variable graph, corresponding to
the graphical model associated with the unobserved latent variables, and the
cluster-average graph, corresponding to the vector of features averaged over
clusters. Our study reveals that likelihood based inference for the latent
graph, not analyzed previously, is analytically intractable. Our main
contribution is the development and analysis of alternative estimation and
inference strategies, for the precision matrix of an unobservable latent vector
$Z$. We replace the likelihood of the data by an appropriate class of empirical
risk functions, that can be specialized to the latent graphical model and to
the simpler, but under-analyzed, cluster-average graphical model. The
estimators thus derived can be used for inference on the graph structure, for
instance on edge strength or pattern recovery. Inference is based on the
asymptotic limits of the entry-wise estimates of the precision matrices
associated with the conditional independence graphs under consideration. While
taking the uncertainty induced by the clustering step into account, we
establish Berry-Esseen central limit theorems for the proposed estimators. It
is noteworthy that, although the clusters are estimated adaptively from the
data, the central limit theorems regarding the entries of the estimated graphs
are proved under the same conditions one would use if the clusters were
known...."
"['stat.ML', 'stat.CO', 'stat.ME']",Graphical continuous Lyapunov models,"The linear Lyapunov equation of a covariance matrix parametrizes the
equilibrium covariance matrix of a stochastic process. This parametrization can
be interpreted as a new graphical model class, and we show how the model class
behaves under marginalization and introduce a method for structure learning via
$\ell_1$-penalized loss minimization. Our proposed method is demonstrated to
outperform alternative structure learning algorithms in a simulation study, and
we illustrate its application for protein phosphorylation network
reconstruction."
['stat.ME'],"Representations, Metrics and Statistics For Shape Analysis of Elastic Graphs","Past approaches for statistical shape analysis of objects have focused mainly
on objects within the same topological classes, e.g., scalar functions,
Euclidean curves, or surfaces, etc. For objects that differ in more complex
ways, the current literature offers only topological methods. This paper
introduces a far-reaching geometric approach for analyzing shapes of graphical
objects, such as road networks, blood vessels, brain fiber tracts, etc. It
represents such objects, exhibiting differences in both geometries and
topologies, as graphs made of curves with arbitrary shapes (edges) and
connected at arbitrary junctions (nodes). To perform statistical analyses, one
needs mathematical representations, metrics and other geometrical tools, such
as geodesics, means, and covariances. This paper utilizes a quotient structure
to develop efficient algorithms for computing these quantities, leading to
useful statistical tools, including principal component analysis and analytical
statistical testing and modeling of graphical shapes. The efficacy of this
framework is demonstrated using various simulated as well as the real data from
neurons and brain arterial networks."
"['stat.ML', 'stat.ME']",High-dimensional Gaussian graphical model for network-linked data,"Graphical models are commonly used to represent conditional dependence
relationships between variables. There are multiple methods available for
exploring them from high-dimensional data, but almost all of them rely on the
assumption that the observations are independent and identically distributed.
At the same time, observations connected by a network are becoming increasingly
common, and tend to violate these assumptions. Here we develop a Gaussian
graphical model for observations connected by a network with potentially
different mean vectors, varying smoothly over the network. We propose an
efficient estimation algorithm and demonstrate its effectiveness on both
simulated and real data, obtaining meaningful and interpretable results on a
statistics coauthorship network. We also prove that our method estimates both
the inverse covariance matrix and the corresponding graph structure correctly
under the assumption of network “cohesion”, which refers to the empirically
observed phenomenon of network neighbors sharing similar traits."
"['stat.ML', 'stat.TH']",Learning Some Popular Gaussian Graphical Models without Condition Number Bounds,"Gaussian Graphical Models (GGMs) have wide-ranging applications in machine
learning and the natural and social sciences. In most of the settings in which
they are applied, the number of observed samples is much smaller than the
dimension and they are assumed to be sparse. While there are a variety of
algorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph
structure with a logarithmic number of samples, they assume various conditions
that require the precision matrix to be in some sense well-conditioned.
  Here we give the first polynomial-time algorithms for learning attractive
GGMs and walk-summable GGMs with a logarithmic number of samples without any
such assumptions. In particular, our algorithms can tolerate strong
dependencies among the variables. Our result for structure recovery in
walk-summable GGMs is derived from a more general result for efficient sparse
linear regression in walk-summable models without any norm dependencies. We
complement our results with experiments showing that many existing algorithms
fail even in some simple settings where there are long dependency chains,
whereas ours do not."
"['stat.ML', 'stat.CO', 'stat.ME']",Robust Optimisation Monte Carlo,"This paper is on Bayesian inference for parametric statistical models that
are defined by a stochastic simulator which specifies how data is generated.
Exact sampling is then possible but evaluating the likelihood function is
typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a
framework to perform approximate inference in such situations. While basic ABC
algorithms are widely applicable, they are notoriously slow and much research
has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has
recently been proposed as an efficient and embarrassingly parallel method that
leverages optimisation to accelerate the inference. In this paper, we
demonstrate an important previously unrecognised failure mode of OMC: It
generates strongly overconfident approximations by collapsing regions of
similar or near-constant likelihood into a single point. We propose an
efficient, robust generalisation of OMC that corrects this. It makes fewer
assumptions, retains the main benefits of OMC, and can be performed either as
post-processing to OMC or as a stand-alone computation. We demonstrate the
effectiveness of the proposed Robust OMC on toy examples and tasks in
inverse-graphics where we perform Bayesian inference with a complex image
renderer."
"['stat.ML', 'stat.TH']",On the Sample Complexity of Learning Sum-Product Networks,"Sum-Product Networks (SPNs) can be regarded as a form of deep graphical
models that compactly represent deeply factored and mixed distributions. An SPN
is a rooted directed acyclic graph (DAG) consisting of a set of leaves
(corresponding to base distributions), a set of sum nodes (which represent
mixtures of their children distributions) and a set of product nodes
(representing the products of its children distributions).
  In this work, we initiate the study of the sample complexity of PAC-learning
the set of distributions that correspond to SPNs. We show that the sample
complexity of learning tree structured SPNs with the usual type of leaves
(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)
with the number of parameters of the SPN. More specifically, we show that the
class of distributions that corresponds to tree structured Gaussian SPNs with
$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned
within Total Variation error $\epsilon$ using at most
$\widetilde{O}(\frac{ed^2+k}{\epsilon^2})$ samples. A similar result holds for
tree structured SPNs with discrete leaves.
  We obtain the upper bounds based on the recently proposed notion of
distribution compression schemes. More specifically, we show that if a (base)
class of distributions $\mathcal{F}$ admits an ""efficient"" compression, then
the class of tree structured SPNs with leaves from $\mathcal{F}$ also admits an
efficient compression."
"['stat.ML', 'stat.TH']",Learning Gaussian Graphical Models via Multiplicative Weights,"Graphical model selection in Markov random fields is a fundamental problem in
statistics and machine learning. Two particularly prominent models, the Ising
model and Gaussian model, have largely developed in parallel using different
(though often related) techniques, and several practical algorithms with
rigorous sample complexity bounds have been established for each. In this
paper, we adapt a recently proposed algorithm of Klivans and Meka (FOCS, 2017),
based on the method of multiplicative weight updates, from the Ising model to
the Gaussian model, via non-trivial modifications to both the algorithm and its
analysis. The algorithm enjoys a sample complexity bound that is qualitatively
similar to others in the literature, has a low runtime $O(mp^2)$ in the case of
$m$ samples and $p$ nodes, and can trivially be implemented in an online
manner."
"['stat.ML', 'stat.ME']",The Sylvester Graphical Lasso (SyGlasso),"This paper introduces the Sylvester graphical lasso (SyGlasso) that captures
multiway dependencies present in tensor-valued data. The model is based on the
Sylvester equation that defines a generative model. The proposed model
complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a
Kronecker sum model for the inverse covariance matrix by providing an
alternative Kronecker sum model that is generative and interpretable. A
nodewise regression approach is adopted for estimating the conditional
independence relationships among variables. The statistical convergence of the
method is established, and empirical studies are provided to demonstrate the
recovery of meaningful conditional dependency graphs. We apply the SyGlasso to
an electroencephalography (EEG) study to compare the brain connectivity of
alcoholic and nonalcoholic subjects. We demonstrate that our model can
simultaneously estimate both the brain connectivity and its temporal
dependencies."
"['stat.ML', 'stat.ME']",Efficient Sampling and Structure Learning of Bayesian Networks,"Bayesian networks are probabilistic graphical models widely employed to
understand dependencies in high dimensional data, and even to facilitate causal
discovery. Learning the underlying network structure, which is encoded as a
directed acyclic graph (DAG) is highly challenging mainly due to the vast
number of possible networks. Efforts have focussed on two fronts:
constraint-based methods that perform conditional independence tests to exclude
edges and score and search approaches which explore the DAG space with greedy
or MCMC schemes. Here we synthesise these two fields in a novel hybrid method
which reduces the complexity of MCMC approaches to that of a constraint-based
method. Individual steps in the MCMC scheme only require simple table lookups
so that very long chains can be efficiently obtained. Furthermore, the scheme
includes an iterative procedure to correct for errors from the conditional
independence tests. The algorithm offers markedly superior performance to
alternatives, particularly because DAGs can also be sampled from the posterior
distribution, enabling full Bayesian model averaging for much larger Bayesian
networks."
"['stat.ML', 'stat.TH']",Bayesian Tensor Network with Polynomial Complexity for Probabilistic Machine Learning,"It is known that describing or calculating the conditional probabilities of
multiple events is exponentially expensive. In this work, Bayesian tensor
network (BTN) is proposed to efficiently capture the conditional probabilities
of multiple sets of events with polynomial complexity. BTN is a directed
acyclic graphical model that forms a subset of TN. To testify its validity for
exponentially many events, BTN is implemented to the image recognition, where
the classification is mapped to capturing the conditional probabilities in an
exponentially large sample space. Competitive performance is achieved by the
BTN with simple tree network structures. Analogous to the tensor network
simulations of quantum systems, the validity of the simple-tree BTN implies an
``area law'' of fluctuations in image recognition problems."
"['stat.ML', 'stat.ME']",Projection pursuit based on Gaussian mixtures and evolutionary algorithms,"We propose a projection pursuit (PP) algorithm based on Gaussian mixture
models (GMMs). The negentropy obtained from a multivariate density estimated by
GMMs is adopted as the PP index to be maximised. For a fixed dimension of the
projection subspace, the GMM-based density estimation is projected onto that
subspace, where an approximation of the negentropy for Gaussian mixtures is
computed. Then, Genetic Algorithms (GAs) are used to find the optimal,
orthogonal projection basis by maximising the former approximation. We show
that this semi-parametric approach to PP is flexible and allows highly
informative structures to be detected, by projecting multivariate datasets onto
a subspace, where the data can be feasibly visualised. The performance of the
proposed approach is shown on both artificial and real datasets."
"['stat.ML', 'stat.TH']",A Theory of Uncertainty Variables for State Estimation and Inference,"We develop a new framework of uncertainty variables to model uncertainty. An
uncertainty variable is characterized by an uncertainty set, in which its
realization is bound to lie, while the conditional uncertainty is characterized
by a set map, from a given realization of a variable to a set of possible
realizations of another variable. We prove Bayes' law and the law of total
probability equivalents for uncertainty variables. We define a notion of
independence, conditional independence, and pairwise independence for a
collection of uncertainty variables, and show that this new notion of
independence preserves the properties of independence defined over random
variables. We then develop a graphical model, namely Bayesian uncertainty
network, a Bayesian network equivalent defined over a collection of uncertainty
variables, and show that all the natural conditional independence properties,
expected out of a Bayesian network, hold for the Bayesian uncertainty network.
We also define the notion of point estimate, and show its relation with the
maximum a posteriori estimate. Probability theory starts with a distribution
function (equivalently a probability measure) as a primitive and builds all
other useful concepts, such as law of total probability, Bayes' law,
independence, graphical models, point estimate, on it. Our work shows that it
is perfectly possible to start with a set, instead of a distribution function,
and retain all the useful ideas needed for state estimation and inference."
"['stat.ML', 'stat.ME']",Direct Estimation of Differential Functional Graphical Models,"We consider the problem of estimating the difference between two functional
undirected graphical models with shared structures. In many applications, data
are naturally regarded as high-dimensional random function vectors rather than
multivariate scalars. For example, electroencephalography (EEG) data are more
appropriately treated as functions of time. In these problems, not only can the
number of functions measured per sample be large, but each function is itself
an infinite dimensional object, making estimation of model parameters
challenging. We develop a method that directly estimates the difference of
graphs, avoiding separate estimation of each graph, and show it is consistent
in certain high-dimensional settings. We illustrate finite sample properties of
our method through simulation studies. Finally, we apply our method to EEG data
to uncover differences in functional brain connectivity between alcoholics and
control subjects."
"['stat.ML', 'stat.TH']",Distributionally Robust Formulation and Model Selection for the Graphical Lasso,"Building on a recent framework for distributionally robust optimization, we
consider estimation of the inverse covariance matrix for multivariate data. We
provide a novel notion of a Wasserstein ambiguity set specifically tailored to
this estimation problem, leading to a tractable class of regularized
estimators. Special cases include penalized likelihood estimators for Gaussian
data, specifically the graphical lasso estimator. As a consequence of this
formulation, the radius of the Wasserstein ambiguity set is directly related to
the regularization parameter in the estimation problem. Using this
relationship, the level of robustness of the estimation procedure can be shown
to correspond to the level of confidence with which the ambiguity set contains
a distribution with the population covariance. Furthermore, a unique feature of
our formulation is that the radius can be expressed in closed-form as a
function of the ordinary sample covariance matrix. Taking advantage of this
finding, we develop a simple algorithm to determine a regularization parameter
for graphical lasso, using only the bootstrapped sample covariance matrices,
meaning that computationally expensive repeated evaluation of the graphical
lasso algorithm is not necessary. Alternatively, the distributionally robust
formulation can also quantify the robustness of the corresponding estimator if
one uses an off-the-shelf method such as cross-validation. Finally, we
numerically study the obtained regularization criterion and analyze the
robustness of other automated tuning procedures used in practice."
['stat.ME'],Dependence Structure Estimation via Copula,"Dependence strucuture estimation is one of the important problems in machine
learning domain and has many applications in different scientific areas. In
this paper, a theoretical framework for such estimation based on copula and
copula entropy -- the probabilistic theory of representation and measurement of
statistical dependence, is proposed. Graphical models are considered as a
special case of the copula framework. A method of the framework for estimating
maximum spanning copula is proposed. Due to copula, the method is irrelevant to
the properties of individual variables, insensitive to outlier and able to deal
with non-Gaussianity. Experiments on both simulated data and real dataset
demonstrated the effectiveness of the proposed method."
"['stat.ML', 'stat.CO', 'stat.ME']",Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization,"Convex clustering is a promising new approach to the classical problem of
clustering, combining strong performance in empirical studies with rigorous
theoretical foundations. Despite these advantages, convex clustering has not
been widely adopted, due to its computationally intensive nature and its lack
of compelling visualizations. To address these impediments, we introduce
Algorithmic Regularization, an innovative technique for obtaining high-quality
estimates of regularization paths using an iterative one-step approximation
scheme. We justify our approach with a novel theoretical result, guaranteeing
global convergence of the approximate path to the exact solution under
easily-checked non-data-dependent assumptions. The application of algorithmic
regularization to convex clustering yields the Convex Clustering via
Algorithmic Regularization Paths (CARP) algorithm for computing the clustering
solution path. On example data sets from genomics and text analysis, CARP
delivers over a 100-fold speed-up over existing methods, while attaining a
finer approximation grid than standard methods. Furthermore, CARP enables
improved visualization of clustering solutions: the fine solution grid returned
by CARP can be used to construct a convex clustering-based dendrogram, as well
as forming the basis of a dynamic path-wise visualization based on modern web
technologies. Our methods are implemented in the open-source R package
clustRviz, available at https://github.com/DataSlingers/clustRviz."
"['stat.ML', 'stat.TH']",Sparse Logistic Regression Learns All Discrete Pairwise Graphical Models,"We characterize the effectiveness of a classical algorithm for recovering the
Markov graph of a general discrete pairwise graphical model from i.i.d.
samples. The algorithm is (appropriately regularized) maximum conditional
log-likelihood, which involves solving a convex program for each node; for
Ising models this is $\ell_1$-constrained logistic regression, while for more
general alphabets an $\ell_{2,1}$ group-norm constraint needs to be used. We
show that this algorithm can recover any arbitrary discrete pairwise graphical
model, and also characterize its sample complexity as a function of model
width, alphabet size, edge parameter accuracy, and the number of variables. We
show that along every one of these axes, it matches or improves on all existing
results and algorithms for this problem. Our analysis applies a sharp
generalization error bound for logistic regression when the weight vector has
an $\ell_1$ constraint (or $\ell_{2,1}$ constraint) and the sample vector has
an $\ell_{\infty}$ constraint (or $\ell_{2, \infty}$ constraint). We also show
that the proposed convex programs can be efficiently solved in $\tilde{O}(n^2)$
running time (where $n$ is the number of variables) under the same statistical
guarantees. We provide experimental results to support our analysis."
"['stat.ML', 'stat.ME']",Clustered Gaussian Graphical Model via Symmetric Convex Clustering,"Knowledge of functional groupings of neurons can shed light on structures of
neural circuits and is valuable in many types of neuroimaging studies. However,
accurately determining which neurons carry out similar neurological tasks via
controlled experiments is both labor-intensive and prohibitively expensive on a
large scale. Thus, it is of great interest to cluster neurons that have similar
connectivity profiles into functionally coherent groups in a data-driven
manner. In this work, we propose the clustered Gaussian graphical model (GGM)
and a novel symmetric convex clustering penalty in an unified convex
optimization framework for inferring functional clusters among neurons from
neural activity data. A parallelizable multi-block Alternating Direction Method
of Multipliers (ADMM) algorithm is used to solve the corresponding convex
optimization problem. In addition, we establish convergence guarantees for the
proposed ADMM algorithm. Experimental results on both synthetic data and
real-world neuroscientific data demonstrate the effectiveness of our approach."
"['stat.ML', 'stat.ME']",Multiple Causes: A Causal Graphical View,"Unobserved confounding is a major hurdle for causal inference from
observational data. Confounders---the variables that affect both the causes and
the outcome---induce spurious non-causal correlations between the two. Wang &
Blei (2018) lower this hurdle with ""the blessings of multiple causes,"" where
the correlation structure of multiple causes provides indirect evidence for
unobserved confounding. They leverage these blessings with an algorithm, called
the deconfounder, that uses probabilistic factor models to correct for the
confounders. In this paper, we take a causal graphical view of the
deconfounder. In a graph that encodes shared confounding, we show how the
multiplicity of causes can help identify intervention distributions. We then
justify the deconfounder, showing that it makes valid inferences of the
intervention. Finally, we expand the class of graphs, and its theory, to those
that include other confounders and selection variables. Our results expand the
theory in Wang & Blei (2018), justify the deconfounder for causal graphs, and
extend the settings where it can be used."
"['stat.ML', 'stat.ME', 'stat.OT']",Generalized Score Matching for Non-Negative Data,"A common challenge in estimating parameters of probability density functions
is the intractability of the normalizing constant. While in such cases maximum
likelihood estimation may be implemented using numerical integration, the
approach becomes computationally intensive. The score matching method of
Hyv\""arinen [2005] avoids direct calculation of the normalizing constant and
yields closed-form estimates for exponential families of continuous
distributions over $\mathbb{R}^m$. Hyv\""arinen [2007] extended the approach to
distributions supported on the non-negative orthant, $\mathbb{R}_+^m$. In this
paper, we give a generalized form of score matching for non-negative data that
improves estimation efficiency. As an example, we consider a general class of
pairwise interaction models. Addressing an overlooked inexistence problem, we
generalize the regularized score matching method of Lin et al. [2016] and
improve its theoretical guarantees for non-negative Gaussian graphical models."
"['stat.ML', 'stat.ME']",Model-based Kernel Sum Rule: Kernel Bayesian Inference with Probabilistic Models,"Kernel Bayesian inference is a principled approach to nonparametric inference
in probabilistic graphical models, where probabilistic relationships between
variables are learned from data in a nonparametric manner. Various algorithms
of kernel Bayesian inference have been developed by combining kernelized basic
probabilistic operations such as the kernel sum rule and kernel Bayes' rule.
However, the current framework is fully nonparametric, and it does not allow a
user to flexibly combine nonparametric and model-based inferences. This is
inefficient when there are good probabilistic models (or simulation models)
available for some parts of a graphical model; this is in particular true in
scientific fields where ""models"" are the central topic of study. Our
contribution in this paper is to introduce a novel approach, termed the {\em
model-based kernel sum rule} (Mb-KSR), to combine a probabilistic model and
kernel Bayesian inference. By combining the Mb-KSR with the existing kernelized
probabilistic rules, one can develop various algorithms for hybrid (i.e.,
nonparametric and model-based) inferences. As an illustrative example, we
consider Bayesian filtering in a state space model, where typically there
exists an accurate probabilistic model for the state transition process. We
propose a novel filtering method that combines model-based inference for the
state transition process and data-driven, nonparametric inference for the
observation generating process. We empirically validate our approach with
synthetic and real-data experiments, the latter being the problem of
vision-based mobile robot localization in robotics, which illustrates the
effectiveness of the proposed hybrid approach."
"['stat.ML', 'stat.ME']",Precision Matrix Estimation with Noisy and Missing Data,"Estimating conditional dependence graphs and precision matrices are some of
the most common problems in modern statistics and machine learning. When data
are fully observed, penalized maximum likelihood-type estimators have become
standard tools for estimating graphical models under sparsity conditions.
Extensions of these methods to more complex settings where data are
contaminated with additive or multiplicative noise have been developed in
recent years. In these settings, however, the relative performance of different
methods is not well understood and algorithmic gaps still exist. In particular,
in high-dimensional settings these methods require using non-positive
semidefinite matrices as inputs, presenting novel optimization challenges. We
develop an alternating direction method of multipliers (ADMM) algorithm for
these problems, providing a feasible algorithm to estimate precision matrices
with indefinite input and potentially nonconvex penalties. We compare this
method with existing alternative solutions and empirically characterize the
tradeoffs between them. Finally, we use this method to explore the networks
among US senators estimated from voting records data."
"['stat.ML', 'stat.CO']",Elements of Sequential Monte Carlo,"A core problem in statistics and probabilistic machine learning is to compute
probability distributions and expectations. This is the fundamental problem of
Bayesian statistics and machine learning, which frames all inference as
expectations with respect to the posterior distribution. The key challenge is
to approximate these intractable expectations. In this tutorial, we review
sequential Monte Carlo (SMC), a random-sampling-based class of methods for
approximate inference. First, we explain the basics of SMC, discuss practical
issues, and review theoretical results. We then examine two of the main user
design choices: the proposal distributions and the so called intermediate
target distributions. We review recent results on how variational inference and
amortization can be used to learn efficient proposals and target distributions.
Next, we discuss the SMC estimate of the normalizing constant, how this can be
used for pseudo-marginal inference and inference evaluation. Throughout the
tutorial we illustrate the use of SMC on various models commonly used in
machine learning, such as stochastic recurrent neural networks, probabilistic
graphical models, and probabilistic programs."
"['stat.ML', 'stat.CO', 'stat.ME']",Bayesian Allocation Model: Inference by Sequential Monte Carlo for Nonnegative Tensor Factorizations and Topic Models using Polya Urns,"We introduce a dynamic generative model, Bayesian allocation model (BAM),
which establishes explicit connections between nonnegative tensor factorization
(NTF), graphical models of discrete probability distributions and their
Bayesian extensions, and the topic models such as the latent Dirichlet
allocation. BAM is based on a Poisson process, whose events are marked by using
a Bayesian network, where the conditional probability tables of this network
are then integrated out analytically. We show that the resulting marginal
process turns out to be a Polya urn, an integer valued self-reinforcing
process. This urn processes, which we name a Polya-Bayes process, obey certain
conditional independence properties that provide further insight about the
nature of NTF. These insights also let us develop space efficient simulation
algorithms that respect the potential sparsity of data: we propose a class of
sequential importance sampling algorithms for computing NTF and approximating
their marginal likelihood, which would be useful for model selection. The
resulting methods can also be viewed as a model scoring method for topic models
and discrete Bayesian networks with hidden variables. The new algorithms have
favourable properties in the sparse data regime when contrasted with
variational algorithms that become more accurate when the total sum of the
elements of the observed tensor goes to infinity. We illustrate the performance
on several examples and numerically study the behaviour of the algorithms for
various data regimes."
"['stat.ML', 'stat.ME']",Tensor Graphical Model: Non-convex Optimization and Statistical Inference,"We consider the estimation and inference of graphical models that
characterize the dependency structure of high-dimensional tensor-valued data.
To facilitate the estimation of the precision matrix corresponding to each way
of the tensor, we assume the data follow a tensor normal distribution whose
covariance has a Kronecker product structure. A critical challenge in the
estimation and inference of this model is the fact that its penalized maximum
likelihood estimation involves minimizing a non-convex objective function. To
address it, this paper makes two contributions: (i) In spite of the
non-convexity of this estimation problem, we prove that an alternating
minimization algorithm, which iteratively estimates each sparse precision
matrix while fixing the others, attains an estimator with an optimal
statistical rate of convergence. (ii) We propose a de-biased statistical
inference procedure for testing hypotheses on the true support of the sparse
precision matrices, and employ it for testing a growing number of hypothesis
with false discovery rate (FDR) control. The asymptotic normality of our test
statistic and the consistency of FDR control procedure are established. Our
theoretical results are backed up by thorough numerical studies and our real
applications on neuroimaging studies of Autism spectrum disorder and users'
advertising click analysis bring new scientific findings and business insights.
The proposed methods are encoded into a publicly available R package Tlasso."
"['stat.ML', 'stat.ME']",Robust multivariate and functional archetypal analysis with application to financial time series analysis,"Archetypal analysis approximates data by means of mixtures of actual extreme
cases (archetypoids) or archetypes, which are a convex combination of cases in
the data set. Archetypes lie on the boundary of the convex hull. This makes the
analysis very sensitive to outliers. A robust methodology by means of
M-estimators for classical multivariate and functional data is proposed. This
unsupervised methodology allows complex data to be understood even by
non-experts. The performance of the new procedure is assessed in a simulation
study, where a comparison with a previous methodology for the multivariate case
is also carried out, and our proposal obtains favorable results. Finally,
robust bivariate functional archetypoid analysis is applied to a set of
companies in the S\&P 500 described by two time series of stock quotes. A new
graphic representation is also proposed to visualize the results. The analysis
shows how the information can be easily interpreted and how even non-experts
can gain a qualitative understanding of the data."
['stat.TH'],Information Theoretic Optimal Learning of Gaussian Graphical Models,"What is the optimal number of independent observations from which a sparse
Gaussian Graphical Model can be correctly recovered? Information-theoretic
arguments provide a lower bound on the minimum number of samples necessary to
perfectly identify the support of any multivariate normal distribution as a
function of model parameters. For a model defined on a sparse graph with $p$
nodes, a maximum degree $d$ and minimum normalized edge strength $\kappa$, this
necessary number of samples scales at least as $d \log p/\kappa^2$. The sample
complexity requirements of existing methods for perfect graph reconstruction
exhibit dependency on additional parameters that do not enter in the lower
bound. The question of whether the lower bound is tight and achievable by a
polynomial time algorithm remains open. In this paper, we constructively answer
this question and propose an algorithm, termed DICE, whose sample complexity
matches the information-theoretic lower bound up to a universal constant
factor. We also propose a related algorithm SLICE that has a slightly higher
sample complexity, but can be implemented as a mixed integer quadratic program
which makes it attractive in practice. Importantly, SLICE retains a critical
advantage of DICE in that its sample complexity only depends on quantities
present in the information theoretic lower bound. We anticipate that this
result will stimulate future search of computationally efficient sample-optimal
algorithms."
['stat.TH'],Divergence Network: Graphical calculation method of divergence functions,"In this paper, we introduce directed networks called `divergence network' in
order to perform graphical calculation of divergence functions. By using the
divergence networks, we can easily understand the geometric meaning of
calculation results and grasp relations among divergence functions intuitively."
"['stat.ML', 'stat.TH']",Maximum Regularized Likelihood Estimators: A General Prediction Theory and Applications,"Maximum regularized likelihood estimators (MRLEs) are arguably the most
established class of estimators in high-dimensional statistics. In this paper,
we derive guarantees for MRLEs in Kullback-Leibler divergence, a general
measure of prediction accuracy. We assume only that the densities have a convex
parametrization and that the regularization is definite and positive
homogenous. The results thus apply to a very large variety of models and
estimators, such as tensor regression and graphical models with convex and
non-convex regularized methods. A main conclusion is that MRLEs are broadly
consistent in prediction - regardless of whether restricted eigenvalues or
similar conditions hold."
"['stat.ML', 'stat.TH']",Sequential change-point detection in high-dimensional Gaussian graphical models,"High dimensional piecewise stationary graphical models represent a versatile
class for modelling time varying networks arising in diverse application areas,
including biology, economics, and social sciences. There has been recent work
in offline detection and estimation of regime changes in the topology of sparse
graphical models. However, the online setting remains largely unexplored,
despite its high relevance to applications in sensor networks and other
engineering monitoring systems, as well as financial markets. To that end, this
work introduces a novel scalable online algorithm for detecting an unknown
number of abrupt changes in the inverse covariance matrix of sparse Gaussian
graphical models with small delay. The proposed algorithm is based upon
monitoring the conditional log-likelihood of all nodes in the network and can
be extended to a large class of continuous and discrete graphical models. We
also investigate asymptotic properties of our procedure under certain mild
regularity conditions on the graph size, sparsity level, number of samples, and
pre- and post-changes in the topology of the network. Numerical works on both
synthetic and real data illustrate the good performance of the proposed
methodology both in terms of computational and statistical efficiency across
numerous experimental settings."
"['stat.ML', 'stat.TH']",Restricted Boltzmann Machines: Introduction and Review,"The restricted Boltzmann machine is a network of stochastic units with
undirected interactions between pairs of visible and hidden units. This model
was popularized as a building block of deep learning architectures and has
continued to play an important role in applied and theoretical machine
learning. Restricted Boltzmann machines carry a rich structure, with
connections to geometry, applied algebra, probability, statistics, machine
learning, and other areas. The analysis of these models is attractive in its
own right and also as a platform to combine and generalize mathematical tools
for graphical models with hidden variables. This article gives an introduction
to the mathematical analysis of restricted Boltzmann machines, reviews recent
results on the geometry of the sets of probability distributions representable
by these models, and suggests a few directions for further investigation."
"['stat.ML', 'stat.CO']",Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion,"The sparse inverse covariance estimation problem is commonly solved using an
$\ell_{1}$-regularized Gaussian maximum likelihood estimator known as
""graphical lasso"", but its computational cost becomes prohibitive for large
data sets. A recent line of results showed--under mild assumptions--that the
graphical lasso estimator can be retrieved by soft-thresholding the sample
covariance matrix and solving a maximum determinant matrix completion (MDMC)
problem. This paper proves an extension of this result, and describes a
Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the
thresholded sample covariance matrix is sparse with a sparse Cholesky
factorization, we prove that the algorithm converges to an $\epsilon$-accurate
solution in $O(n\log(1/\epsilon))$ time and $O(n)$ memory. The algorithm is
highly efficient in practice: we solve the associated MDMC problems with as
many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a
standard laptop computer running MATLAB."
"['stat.ML', 'stat.ME']",Distributed Cartesian Power Graph Segmentation for Graphon Estimation,"We study an extention of total variation denoising over images to over
Cartesian power graphs and its applications to estimating non-parametric
network models. The power graph fused lasso (PGFL) segments a matrix by
exploiting a known graphical structure, $G$, over the rows and columns. Our
main results shows that for any connected graph, under subGaussian noise, the
PGFL achieves the same mean-square error rate as 2D total variation denoising
for signals of bounded variation. We study the use of the PGFL for denoising an
observed network $H$, where we learn the graph $G$ as the $K$-nearest
neighborhood graph of an estimated metric over the vertices. We provide
theoretical and empirical results for estimating graphons, a non-parametric
exchangeable network model, and compare to the state of the art graphon
estimation methods."
"['stat.ML', 'stat.CO', 'stat.ME', 'stat.TH']",Bayesian Regularization for Graphical Models with Unequal Shrinkage,"We consider a Bayesian framework for estimating a high-dimensional sparse
precision matrix, in which adaptive shrinkage and sparsity are induced by a
mixture of Laplace priors. Besides discussing our formulation from the Bayesian
standpoint, we investigate the MAP (maximum a posteriori) estimator from a
penalized likelihood perspective that gives rise to a new non-convex penalty
approximating the $\ell_0$ penalty. Optimal error rates for estimation
consistency in terms of various matrix norms along with selection consistency
for sparse structure recovery are shown for the unique MAP estimator under mild
conditions. For fast and efficient computation, an EM algorithm is proposed to
compute the MAP estimator of the precision matrix and (approximate) posterior
probabilities on the edges of the underlying sparse structure. Through
extensive simulation studies and a real application to a call center data, we
have demonstrated the fine performance of our method compared with existing
alternatives."
"['stat.ML', 'stat.ME', 'stat.TH']","Predictive Independence Testing, Predictive Conditional Independence Testing, and Predictive Graphical Modelling","Testing (conditional) independence of multivariate random variables is a task
central to statistical inference and modelling in general - though
unfortunately one for which to date there does not exist a practicable
workflow. State-of-art workflows suffer from the need for heuristic or
subjective manual choices, high computational complexity, or strong parametric
assumptions.
  We address these problems by establishing a theoretical link between
multivariate/conditional independence testing, and model comparison in the
multivariate predictive modelling aka supervised learning task. This link
allows advances in the extensively studied supervised learning workflow to be
directly transferred to independence testing workflows - including automated
tuning of machine learning type which addresses the need for a heuristic
choice, the ability to quantitatively trade-off computational demand with
accuracy, and the modern black-box philosophy for checking and interfacing.
  As a practical implementation of this link between the two workflows, we
present a python package 'pcit', which implements our novel multivariate and
conditional independence tests, interfacing the supervised learning API of the
scikit-learn package. Theory and package also allow for straightforward
independence test based learning of graphical model structure.
  We empirically show that our proposed predictive independence test outperform
or are on par to current practice, and the derived graphical model structure
learning algorithms asymptotically recover the 'true' graph. This paper, and
the 'pcit' package accompanying it, thus provide powerful, scalable,
generalizable, and easy-to-use methods for multivariate and conditional
independence testing, as well as for graphical model structure learning."
"['stat.ML', 'stat.CO', 'stat.ME']",Learning Large-Scale Bayesian Networks with the sparsebn Package,"Learning graphical models from data is an important problem with wide
applications, ranging from genomics to the social sciences. Nowadays datasets
often have upwards of thousands---sometimes tens or hundreds of thousands---of
variables and far fewer samples. To meet this challenge, we have developed a
new R package called sparsebn for learning the structure of large, sparse
graphical models with a focus on Bayesian networks. While there are many
existing software packages for this task, this package focuses on the unique
setting of learning large networks from high-dimensional data, possibly with
interventions. As such, the methods provided place a premium on scalability and
consistency in a high-dimensional setting. Furthermore, in the presence of
interventions, the methods implemented here achieve the goal of learning a
causal network from data. Additionally, the sparsebn package is fully
compatible with existing software packages for network analysis."
"['stat.ML', 'stat.TH']",Simultaneous Clustering and Estimation of Heterogeneous Graphical Models,"We consider joint estimation of multiple graphical models arising from
heterogeneous and high-dimensional observations. Unlike most previous
approaches which assume that the cluster structure is given in advance, an
appealing feature of our method is to learn cluster structure while estimating
heterogeneous graphical models. This is achieved via a high dimensional version
of Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).
A joint graphical lasso penalty is imposed on the conditional maximization step
to extract both homogeneity and heterogeneity components across all clusters.
Our algorithm is computationally efficient due to fast sparse learning routines
and can be implemented without unsupervised learning knowledge. The superior
performance of our method is demonstrated by extensive experiments and its
application to a Glioblastoma cancer dataset reveals some new insights in
understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound
is established for the output directly from our high dimensional ECM algorithm,
and it consists of two quantities: statistical error (statistical accuracy) and
optimization error (computational complexity). Such a result gives a
theoretical guideline in terminating our ECM iterations."
"['stat.ML', 'stat.CO']",Sparse Inverse Covariance Estimation for Chordal Structures,"In this paper, we consider the Graphical Lasso (GL), a popular optimization
problem for learning the sparse representations of high-dimensional datasets,
which is well-known to be computationally expensive for large-scale problems.
Recently, we have shown that the sparsity pattern of the optimal solution of GL
is equivalent to the one obtained from simply thresholding the sample
covariance matrix, for sparse graphs under different conditions. We have also
derived a closed-form solution that is optimal when the thresholded sample
covariance matrix has an acyclic structure. As a major generalization of the
previous result, in this paper we derive a closed-form solution for the GL for
graphs with chordal structures. We show that the GL and thresholding
equivalence conditions can significantly be simplified and are expected to hold
for high-dimensional problems if the thresholded sample covariance matrix has a
chordal structure. We then show that the GL and thresholding equivalence is
enough to reduce the GL to a maximum determinant matrix completion problem and
drive a recursive closed-form solution for the GL when the thresholded sample
covariance matrix has a chordal structure. For large-scale problems with up to
450 million variables, the proposed method can solve the GL problem in less
than 2 minutes, while the state-of-the-art methods converge in more than 2
hours."
"['stat.ML', 'stat.CO', 'stat.TH']",Probabilistic Integration: A Role in Statistical Computation?,"A research frontier has emerged in scientific computation, wherein numerical
error is regarded as a source of epistemic uncertainty that can be modelled.
This raises several statistical challenges, including the design of statistical
methods that enable the coherent propagation of probabilities through a
(possibly deterministic) computational work-flow. This paper examines the case
for probabilistic numerical methods in routine statistical computation. Our
focus is on numerical integration, where a probabilistic integrator is equipped
with a full distribution over its output that reflects the presence of an
unknown numerical error. Our main technical contribution is to establish, for
the first time, rates of posterior contraction for these methods. These show
that probabilistic integrators can in principle enjoy the ""best of both
worlds"", leveraging the sampling efficiency of Monte Carlo methods whilst
providing a principled route to assess the impact of numerical error on
scientific conclusions. Several substantial applications are provided for
illustration and critical evaluation, including examples from statistical
modelling, computer graphics and a computer model for an oil reservoir."
"['stat.ML', 'stat.ME']",Sparse Partially Collapsed MCMC for Parallel Inference in Topic Models,"Topic models, and more specifically the class of Latent Dirichlet Allocation
(LDA), are widely used for probabilistic modeling of text. MCMC sampling from
the posterior distribution is typically performed using a collapsed Gibbs
sampler. We propose a parallel sparse partially collapsed Gibbs sampler and
compare its speed and efficiency to state-of-the-art samplers for topic models
on five well-known text corpora of differing sizes and properties. In
particular, we propose and compare two different strategies for sampling the
parameter block with latent topic indicators. The experiments show that the
increase in statistical inefficiency from only partial collapsing is smaller
than commonly assumed, and can be more than compensated by the speedup from
parallelization and sparsity on larger corpora. We also prove that the
partially collapsed samplers scale well with the size of the corpus. The
proposed algorithm is fast, efficient, exact, and can be used in more modeling
situations than the ordinary collapsed sampler."
"['stat.ML', 'stat.CO']",Bayesian Optimization for Probabilistic Programs,"We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization."
['stat.TH'],Learning Graphical Models Using Multiplicative Weights,"We give a simple, multiplicative-weight update algorithm for learning
undirected graphical models or Markov random fields (MRFs). The approach is
new, and for the well-studied case of Ising models or Boltzmann machines, we
obtain an algorithm that uses a nearly optimal number of samples and has
quadratic running time (up to logarithmic factors), subsuming and improving on
all prior work. Additionally, we give the first efficient algorithm for
learning Ising models over general alphabets.
  Our main application is an algorithm for learning the structure of t-wise
MRFs with nearly-optimal sample complexity (up to polynomial losses in
necessary terms that depend on the weights) and running time that is
$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the
parameters of the model and generate a hypothesis that is close in statistical
distance to the true MRF. All prior work runs in time $n^{\Omega(d)}$ for
graphs of bounded degree d and does not generate a hypothesis close in
statistical distance even for t=3. We observe that our runtime has the correct
dependence on n and t assuming the hardness of learning sparse parities with
noise.
  Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)
and holds in the on-line setting. Its analysis applies a regret bound from
Freund and Schapire's classic Hedge algorithm. It also gives the first solution
to the problem of learning sparse Generalized Linear Models (GLMs)."
"['stat.ML', 'stat.TH']",Graphical Nonconvex Optimization for Optimal Estimation in Gaussian Graphical Models,"We consider the problem of learning high-dimensional Gaussian graphical
models. The graphical lasso is one of the most popular methods for estimating
Gaussian graphical models. However, it does not achieve the oracle rate of
convergence. In this paper, we propose the graphical nonconvex optimization for
optimal estimation in Gaussian graphical models, which is then approximated by
a sequence of convex programs. Our proposal is computationally tractable and
produces an estimator that achieves the oracle rate of convergence. The
statistical error introduced by the sequential approximation using the convex
programs are clearly demonstrated via a contraction property. The rate of
convergence can be further improved using the notion of sparsity pattern. The
proposed methodology is then extended to semiparametric graphical models. We
show through numerical studies that the proposed estimator outperforms other
popular methods for estimating Gaussian graphical models."
"['stat.ML', 'stat.ME']",Integrating Additional Knowledge Into Estimation of Graphical Models,"In applications of graphical models, we typically have more information than
just the samples themselves. A prime example is the estimation of brain
connectivity networks based on fMRI data, where in addition to the samples
themselves, the spatial positions of the measurements are readily available.
With particular regard for this application, we are thus interested in ways to
incorporate additional knowledge most effectively into graph estimation. Our
approach to this is to make neighborhood selection receptive to additional
knowledge by strengthening the role of the tuning parameters. We demonstrate
that this concept (i) can improve reproducibility, (ii) is computationally
convenient and efficient, and (iii) carries a lucid Bayesian interpretation. We
specifically show that the approach provides effective estimations of brain
connectivity graphs from fMRI data. However, providing a general scheme for the
inclusion of additional knowledge, our concept is expected to have applications
in a wide range of domains."
"['stat.ML', 'stat.CO']",Sparse Quadratic Discriminant Analysis and Community Bayes,"We develop a class of rules spanning the range between quadratic discriminant
analysis and naive Bayes, through a path of sparse graphical models. A group
lasso penalty is used to introduce shrinkage and encourage a similar pattern of
sparsity across precision matrices. It gives sparse estimates of interactions
and produces interpretable models. Inspired by the connected-components
structure of the estimated precision matrices, we propose the community Bayes
model, which partitions features into several conditional independent
communities and splits the classification problem into separate smaller ones.
The community Bayes idea is quite general and can be applied to non-Gaussian
data and likelihood-based classifiers."
"['stat.ML', 'stat.CO', 'stat.ME']",Non-convex Global Minimization and False Discovery Rate Control for the TREX,"The TREX is a recently introduced method for performing sparse
high-dimensional regression. Despite its statistical promise as an alternative
to the lasso, square-root lasso, and scaled lasso, the TREX is computationally
challenging in that it requires solving a non-convex optimization problem. This
paper shows a remarkable result: despite the non-convexity of the TREX problem,
there exists a polynomial-time algorithm that is guaranteed to find the global
minimum. This result adds the TREX to a very short list of non-convex
optimization problems that can be globally optimized (principal components
analysis being a famous example). After deriving and developing this new
approach, we demonstrate that (i) the ability of the preexisting TREX heuristic
to reach the global minimum is strongly dependent on the difficulty of the
underlying statistical problem, (ii) the new polynomial-time algorithm for TREX
permits a novel variable ranking and selection scheme, (iii) this scheme can be
incorporated into a rule that controls the false discovery rate (FDR) of
included features in the model. To achieve this last aim, we provide an
extension of the results of Barber & Candes (2015) to establish that the
knockoff filter framework can be applied to the TREX. This investigation thus
provides both a rare case study of a heuristic for non-convex optimization and
a novel way of exploiting non-convexity for statistical inference."
"['stat.ML', 'stat.ME']",High-dimensional Mixed Graphical Models,"While graphical models for continuous data (Gaussian graphical models) and
discrete data (Ising models) have been extensively studied, there is little
work on graphical models linking both continuous and discrete variables (mixed
data), which are common in many scientific applications. We propose a novel
graphical model for mixed data, which is simple enough to be suitable for
high-dimensional data, yet flexible enough to represent all possible graph
structures. We develop a computationally efficient regression-based algorithm
for fitting the model by focusing on the conditional log-likelihood of each
variable given the rest. The parameters have a natural group structure, and
sparsity in the fitted graph is attained by incorporating a group lasso
penalty, approximated by a weighted $\ell_1$ penalty for computational
efficiency. We demonstrate the effectiveness of our method through an extensive
simulation study and apply it to a music annotation data set (CAL500),
obtaining a sparse and interpretable graphical model relating the continuous
features of the audio signal to categorical variables such as genre, emotions,
and usage associated with particular songs. While we focus on binary discrete
variables, we also show that the proposed methodology can be easily extended to
general discrete variables."
"['stat.ML', 'stat.TH']",Active Learning Algorithms for Graphical Model Selection,"The problem of learning the structure of a high dimensional graphical model
from data has received considerable attention in recent years. In many
applications such as sensor networks and proteomics it is often expensive to
obtain samples from all the variables involved simultaneously. For instance,
this might involve the synchronization of a large number of sensors or the
tagging of a large number of proteins. To address this important issue, we
initiate the study of a novel graphical model selection problem, where the goal
is to optimize the total number of scalar samples obtained by allowing the
collection of samples from only subsets of the variables. We propose a general
paradigm for graphical model selection where feedback is used to guide the
sampling to high degree vertices, while obtaining only few samples from the
ones with the low degrees. We instantiate this framework with two specific
active learning algorithms, one of which makes mild assumptions but is
computationally expensive, while the other is more computationally efficient
but requires stronger (nevertheless standard) assumptions. Whereas the sample
complexity of passive algorithms is typically a function of the maximum degree
of the graph, we show that the sample complexity of our algorithms is provable
smaller and that it depends on a novel local complexity measure that is akin to
the average degree of the graph. We finally demonstrate the efficacy of our
framework via simulations."
"['stat.ML', 'stat.TH']",A U-statistic Approach to Hypothesis Testing for Structure Discovery in Undirected Graphical Models,"Structure discovery in graphical models is the determination of the topology
of a graph that encodes conditional independence properties of the joint
distribution of all variables in the model. For some class of probability
distributions, an edge between two variables is present if and only if the
corresponding entry in the precision matrix is non-zero. For a finite sample
estimate of the precision matrix, entries close to zero may be due to low
sample effects, or due to an actual association between variables; these two
cases are not readily distinguishable. %Fisher provided a hypothesis test based
on a parametric approximation to the distribution of an entry in the precision
matrix of a Gaussian distribution, but this may not provide valid upper bounds
on $p$-values for non-Gaussian distributions. Many related works on this topic
consider potentially restrictive distributional or sparsity assumptions that
may not apply to a data sample of interest, and direct estimation of the
uncertainty of an estimate of the precision matrix for general distributions
remains challenging. Consequently, we make use of results for $U$-statistics
and apply them to the covariance matrix. By probabilistically bounding the
distortion of the covariance matrix, we can apply Weyl's theorem to bound the
distortion of the precision matrix, yielding a conservative, but sound test
threshold for a much wider class of distributions than considered in previous
works. The resulting test enables one to answer with statistical significance
whether an edge is present in the graph, and convergence results are known for
a wide range of distributions. The computational complexities is linear in the
sample size enabling the application of the test to large data samples for
which computation time becomes a limiting factor. We experimentally validate
the correctness and scalability of the test on multivariate distributions for
which the distributional assumptions of competing tests result in
underestimates of the false positive ratio. By contrast, the proposed test
remains sound, promising to be a useful tool for hypothesis testing for diverse
real-world problems."
"['stat.CO', 'stat.ML']",On the inconsistency of $\ell_1$-penalised sparse precision matrix estimation,"Various $\ell_1$-penalised estimation methods such as graphical lasso and
CLIME are widely used for sparse precision matrix estimation. Many of these
methods have been shown to be consistent under various quantitative assumptions
about the underlying true covariance matrix. Intuitively, these conditions are
related to situations where the penalty term will dominate the optimisation. In
this paper, we explore the consistency of $\ell_1$-based methods for a class of
sparse latent variable -like models, which are strongly motivated by several
types of applications. We show that all $\ell_1$-based methods fail
dramatically for models with nearly linear dependencies between the variables.
We also study the consistency on models derived from real gene expression data
and note that the assumptions needed for consistency never hold even for modest
sized gene networks and $\ell_1$-based methods also become unreliable in
practice for larger networks."
"['stat.ML', 'stat.CO']",On the Nyström and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets,"In this paper we analyze approximate methods for undertaking a principal
components analysis (PCA) on large data sets. PCA is a classical dimension
reduction method that involves the projection of the data onto the subspace
spanned by the leading eigenvectors of the covariance matrix. This projection
can be used either for exploratory purposes or as an input for further
analysis, e.g. regression. If the data have billions of entries or more, the
computational and storage requirements for saving and manipulating the design
matrix in fast memory is prohibitive. Recently, the Nystr\""om and
column-sampling methods have appeared in the numerical linear algebra community
for the randomized approximation of the singular value decomposition of large
matrices. However, their utility for statistical applications remains unclear.
We compare these approximations theoretically by bounding the distance between
the induced subspaces and the desired, but computationally infeasible, PCA
subspace. Additionally we show empirically, through simulations and a real data
example involving a corpus of emails, the trade-off of approximation accuracy
and computational complexity."
['stat.ME'],Sparse Reconstruction of Compressive Sensing MRI using Cross-Domain Stochastically Fully Connected Conditional Random Fields,"Magnetic Resonance Imaging (MRI) is a crucial medical imaging technology for
the screening and diagnosis of frequently occurring cancers. However image
quality may suffer by long acquisition times for MRIs due to patient motion, as
well as result in great patient discomfort. Reducing MRI acquisition time can
reduce patient discomfort and as a result reduces motion artifacts from the
acquisition process. Compressive sensing strategies, when applied to MRI, have
been demonstrated to be effective at decreasing acquisition times significantly
by sparsely sampling the \emph{k}-space during the acquisition process.
However, such a strategy requires advanced reconstruction algorithms to produce
high quality and reliable images from compressive sensing MRI. This paper
proposes a new reconstruction approach based on cross-domain stochastically
fully connected conditional random fields (CD-SFCRF) for compressive sensing
MRI. The CD-SFCRF introduces constraints in both \emph{k}-space and spatial
domains within a stochastically fully connected graphical model to produce
improved MRI reconstruction. Experimental results using T2-weighted (T2w)
imaging and diffusion-weighted imaging (DWI) of the prostate show strong
performance in preserving fine details and tissue structures in the
reconstructed images when compared to other tested methods even at low sampling
rates."
"['stat.ML', 'stat.ME']",Bayesian Inference in Cumulative Distribution Fields,"One approach for constructing copula functions is by multiplication. Given
that products of cumulative distribution functions (CDFs) are also CDFs, an
adjustment to this multiplication will result in a copula model, as discussed
by Liebscher (J Mult Analysis, 2008). Parameterizing models via products of
CDFs has some advantages, both from the copula perspective (e.g., it is
well-defined for any dimensionality) and from general multivariate analysis
(e.g., it provides models where small dimensional marginal distributions can be
easily read-off from the parameters). Independently, Huang and Frey (J Mach
Learn Res, 2011) showed the connection between certain sparse graphical models
and products of CDFs, as well as message-passing (dynamic programming) schemes
for computing the likelihood function of such models. Such schemes allows
models to be estimated with likelihood-based methods. We discuss and
demonstrate MCMC approaches for estimating such models in a Bayesian context,
their application in copula modeling, and how message-passing can be strongly
simplified. Importantly, our view of message-passing opens up possibilities to
scaling up such methods, given that even dynamic programming is not a scalable
solution for calculating likelihood functions in many models."
"['stat.ML', 'stat.ME']",IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family,"Previously, we proposed a physically-inspired method to construct data points
into an effective in-tree (IT) structure, in which the underlying cluster
structure in the dataset is well revealed. Although there are some edges in the
IT structure requiring to be removed, such undesired edges are generally
distinguishable from other edges and thus are easy to be determined. For
instance, when the IT structures for the 2-dimensional (2D) datasets are
graphically presented, those undesired edges can be easily spotted and
interactively determined. However, in practice, there are many datasets that do
not lie in the 2D Euclidean space, thus their IT structures cannot be
graphically presented. But if we can effectively map those IT structures into a
visualized space in which the salient features of those undesired edges are
preserved, then the undesired edges in the IT structures can still be visually
determined in a visualization environment. Previously, this purpose was reached
by our method called IT-map. The outstanding advantage of IT-map is that
clusters can still be found even with the so-called crowding problem in the
embedding.
  In this paper, we propose another method, called IT-Dendrogram, to achieve
the same goal through an effective combination of the IT structure and the
single link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram
can also effectively represent the IT structures in a visualization
environment, whereas using another form, called the Dendrogram. IT-Dendrogram
can serve as another visualization method to determine the undesired edges in
the IT structures and thus benefit the IT-based clustering analysis. This was
demonstrated on several datasets with different shapes, dimensions, and
attributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem,
which could help users make more reliable cluster analysis in certain problems."
"['stat.ML', 'stat.TH']",Spectral Learning of Large Structured HMMs for Comparative Epigenomics,"We develop a latent variable model and an efficient spectral algorithm
motivated by the recent emergence of very large data sets of chromatin marks
from multiple human cell types. A natural model for chromatin data in one cell
type is a Hidden Markov Model (HMM); we model the relationship between multiple
cell types by connecting their hidden states by a fixed tree of known
structure. The main challenge with learning parameters of such models is that
iterative methods such as EM are very slow, while naive spectral methods result
in time and space complexity exponential in the number of cell types. We
exploit properties of the tree structure of the hidden states to provide
spectral algorithms that are more computationally efficient for current
biological datasets. We provide sample complexity bounds for our algorithm and
evaluate it experimentally on biological data from nine human cell types.
Finally, we show that beyond our specific model, some of our algorithmic ideas
can be applied to other graphical models."
"['stat.ML', 'stat.ME']",A deep-structured fully-connected random field model for structured inference,"There has been significant interest in the use of fully-connected graphical
models and deep-structured graphical models for the purpose of structured
inference. However, fully-connected and deep-structured graphical models have
been largely explored independently, leaving the unification of these two
concepts ripe for exploration. A fundamental challenge with unifying these two
types of models is in dealing with computational complexity. In this study, we
investigate the feasibility of unifying fully-connected and deep-structured
models in a computationally tractable manner for the purpose of structured
inference. To accomplish this, we introduce a deep-structured fully-connected
random field (DFRF) model that integrates a series of intermediate sparse
auto-encoding layers placed between state layers to significantly reduce
computational complexity. The problem of image segmentation was used to
illustrate the feasibility of using the DFRF for structured inference in a
computationally tractable manner. Results in this study show that it is
feasible to unify fully-connected and deep-structured models in a
computationally tractable manner for solving structured inference problems such
as image segmentation."
"['stat.CO', 'stat.ML']",Learning graphical models from the Glauber dynamics,"In this paper we consider the problem of learning undirected graphical models
from data generated according to the Glauber dynamics. The Glauber dynamics is
a Markov chain that sequentially updates individual nodes (variables) in a
graphical model and it is frequently used to sample from the stationary
distribution (to which it converges given sufficient time). Additionally, the
Glauber dynamics is a natural dynamical model in a variety of settings. This
work deviates from the standard formulation of graphical model learning in the
literature, where one assumes access to i.i.d. samples from the distribution.
  Much of the research on graphical model learning has been directed towards
finding algorithms with low computational cost. As the main result of this
work, we establish that the problem of reconstructing binary pairwise graphical
models is computationally tractable when we observe the Glauber dynamics.
Specifically, we show that a binary pairwise graphical model on $p$ nodes with
maximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function
$f(d)$, using nearly the information-theoretic minimum number of samples."
"['stat.ML', 'stat.ME']",PLUTO: Penalized Unbiased Logistic Regression Trees,"We propose a new algorithm called PLUTO for building logistic regression
trees to binary response data. PLUTO can capture the nonlinear and interaction
patterns in messy data by recursively partitioning the sample space. It fits a
simple or a multiple linear logistic regression model in each partition. PLUTO
employs the cyclical coordinate descent method for estimation of multiple
linear logistic regression models with elastic net penalties, which allows it
to deal with high-dimensional data efficiently. The tree structure comprises a
graphical description of the data. Together with the logistic regression
models, it provides an accurate classifier as well as a piecewise smooth
estimate of the probability of ""success"". PLUTO controls selection bias by: (1)
separating split variable selection from split point selection; (2) applying an
adjusted chi-squared test to find the split variable instead of exhaustive
search. A bootstrap calibration technique is employed to further correct
selection bias. Comparison on real datasets shows that on average, the multiple
linear PLUTO models predict more accurately than other algorithms."
"['stat.ML', 'stat.ME']",Topology Adaptive Graph Estimation in High Dimensions,"We introduce Graphical TREX (GTREX), a novel method for graph estimation in
high-dimensional Gaussian graphical models. By conducting neighborhood
selection with TREX, GTREX avoids tuning parameters and is adaptive to the
graph topology. We compare GTREX with standard methods on a new simulation
set-up that is designed to assess accurately the strengths and shortcomings of
different methods. These simulations show that a neighborhood selection scheme
based on Lasso and an optimal (in practice unknown) tuning parameter
outperforms other standard methods over a large spectrum of scenarios.
Moreover, we show that GTREX can rival this scheme and, therefore, can provide
competitive graph estimation without the need for tuning parameter calibration."
"['stat.ML', 'stat.CO', 'stat.ME']",Learning Graphical Models With Hubs,"We consider the problem of learning a high-dimensional graphical model in
which certain hub nodes are highly-connected to many other nodes. Many authors
have studied the use of an l1 penalty in order to learn a sparse graph in
high-dimensional setting. However, the l1 penalty implicitly assumes that each
edge is equally likely and independent of all other edges. We propose a general
framework to accommodate more realistic networks with hub nodes, using a convex
formulation that involves a row-column overlap norm penalty. We apply this
general framework to three widely-used probabilistic graphical models: the
Gaussian graphical model, the covariance graph model, and the binary Ising
model. An alternating direction method of multipliers algorithm is used to
solve the corresponding convex optimization problems. On synthetic data, we
demonstrate that our proposed framework outperforms competitors that do not
explicitly model hub nodes. We illustrate our proposal on a webpage data set
and a gene expression data set."
"['stat.ML', 'stat.TH']",Learning Latent Variable Gaussian Graphical Models,"Gaussian graphical models (GGM) have been widely used in many
high-dimensional applications ranging from biological and financial data to
recommender systems. Sparsity in GGM plays a central role both statistically
and computationally. Unfortunately, real-world data often does not fit well to
sparse graphical models. In this paper, we focus on a family of latent variable
Gaussian graphical models (LVGGM), where the model is conditionally sparse
given latent variables, but marginally non-sparse. In LVGGM, the inverse
covariance matrix has a low-rank plus sparse structure, and can be learned in a
regularized maximum likelihood framework. We derive novel parameter estimation
error bounds for LVGGM under mild conditions in the high-dimensional setting.
These results complement the existing theory on the structural learning, and
open up new possibilities of using LVGGM for statistical inference."
"['stat.ML', 'stat.CO', 'stat.ME']",Learning directed acyclic graphs via bootstrap aggregating,"Probabilistic graphical models are graphical representations of probability
distributions. Graphical models have applications in many fields including
biology, social sciences, linguistic, neuroscience. In this paper, we propose
directed acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed
procedure is named as DAGBag. Specifically, an ensemble of DAGs is first
learned based on bootstrap resamples of the data and then an aggregated DAG is
derived by minimizing the overall distance to the entire ensemble. A family of
metrics based on the structural hamming distance is defined for the space of
DAGs (of a given node set) and is used for aggregation. Under the
high-dimensional-low-sample size setting, the graph learned on one data set
often has excessive number of false positive edges due to over-fitting of the
noise. Aggregation overcomes over-fitting through variance reduction and thus
greatly reduces false positives. We also develop an efficient implementation of
the hill climbing search algorithm of DAG learning which makes the proposed
method computationally competitive for the high-dimensional regime. The DAGBag
procedure is implemented in the R package dagbag."
"['stat.ML', 'stat.TH']",Gemini: Graph estimation with matrix variate normal instances,"Undirected graphs can be used to describe matrix variate distributions. In
this paper, we develop new methods for estimating the graphical structures and
underlying parameters, namely, the row and column covariance and inverse
covariance matrices from the matrix variate data. Under sparsity conditions, we
show that one is able to recover the graphs and covariance matrices with a
single random matrix from the matrix variate normal distribution. Our method
extends, with suitable adaptation, to the general setting where replicates are
available. We establish consistency and obtain the rates of convergence in the
operator and the Frobenius norm. We show that having replicates will allow one
to estimate more complicated graphical structures and achieve faster rates of
convergence. We provide simulation evidence showing that we can recover
graphical structures as well as estimating the precision matrices, as predicted
by theory."
"['stat.ML', 'stat.TH']",Active Learning for Undirected Graphical Model Selection,"This paper studies graphical model selection, i.e., the problem of estimating
a graph of statistical relationships among a collection of random variables.
Conventional graphical model selection algorithms are passive, i.e., they
require all the measurements to have been collected before processing begins.
We propose an active learning algorithm that uses junction tree representations
to adapt future measurements based on the information gathered from prior
measurements. We prove that, under certain conditions, our active learning
algorithm requires fewer scalar measurements than any passive algorithm to
reliably estimate a graph. A range of numerical results validate our theory and
demonstrates the benefits of active learning."
"['stat.ML', 'stat.CO']",Splitting Methods for Convex Clustering,"Clustering is a fundamental problem in many scientific applications. Standard
methods such as $k$-means, Gaussian mixture models, and hierarchical
clustering, however, are beset by local minima, which are sometimes drastically
suboptimal. Recently introduced convex relaxations of $k$-means and
hierarchical clustering shrink cluster centroids toward one another and ensure
a unique global minimizer. In this work we present two splitting methods for
solving the convex clustering problem. The first is an instance of the
alternating direction method of multipliers (ADMM); the second is an instance
of the alternating minimization algorithm (AMA). In contrast to previously
considered algorithms, our ADMM and AMA formulations provide simple and unified
frameworks for solving the convex clustering problem under the previously
studied norms and open the door to potentially novel norms. We demonstrate the
performance of our algorithm on both simulated and real data examples. While
the differences between the two algorithms appear to be minor on the surface,
complexity analysis and numerical experiments show AMA to be significantly more
efficient."
"['stat.ML', 'stat.ME']",Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs,"Graphical models are popular statistical tools which are used to represent
dependent or causal complex systems. Statistically equivalent causal or
directed graphical models are said to belong to a Markov equivalent class. It
is of great interest to describe and understand the space of such classes.
However, with currently known algorithms, sampling over such classes is only
feasible for graphs with fewer than approximately 20 vertices. In this paper,
we design reversible irreducible Markov chains on the space of Markov
equivalent classes by proposing a perfect set of operators that determine the
transitions of the Markov chain. The stationary distribution of a proposed
Markov chain has a closed form and can be computed easily. Specifically, we
construct a concrete perfect set of operators on sparse Markov equivalence
classes by introducing appropriate conditions on each possible operator.
Algorithms and their accelerated versions are provided to efficiently generate
Markov chains and to explore properties of Markov equivalence classes of sparse
directed acyclic graphs (DAGs) with thousands of vertices. We find
experimentally that in most Markov equivalence classes of sparse DAGs, (1) most
edges are directed, (2) most undirected subgraphs are small and (3) the number
of these undirected subgraphs grows approximately linearly with the number of
vertices. The article contains supplement arXiv:1303.0632,
http://dx.doi.org/10.1214/13-AOS1125SUPP"
"['stat.ML', 'stat.TH']",Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses,"We investigate the relationship between the structure of a discrete graphical
model and the support of the inverse of a generalized covariance matrix. We
show that for certain graph structures, the support of the inverse covariance
matrix of indicator variables on the vertices of a graph reflects the
conditional independence structure of the graph. Our work extends results that
have previously been established only in the context of multivariate Gaussian
graphical models, thereby addressing an open question about the significance of
the inverse covariance matrix of a non-Gaussian distribution. The proof
exploits a combination of ideas from the geometry of exponential families,
junction tree theory and convex analysis. These population-level results have
various consequences for graph selection methods, both known and novel,
including a novel method for structure estimation for missing or corrupted
observations. We provide nonasymptotic guarantees for such methods and
illustrate the sharpness of these predictions via simulations."
"['stat.ML', 'stat.ME']",Parametric Modelling of Multivariate Count Data Using Probabilistic Graphical Models,"Multivariate count data are defined as the number of items of different
categories issued from sampling within a population, which individuals are
grouped into categories. The analysis of multivariate count data is a recurrent
and crucial issue in numerous modelling problems, particularly in the fields of
biology and ecology (where the data can represent, for example, children counts
associated with multitype branching processes), sociology and econometrics. We
focus on I) Identifying categories that appear simultaneously, or on the
contrary that are mutually exclusive. This is achieved by identifying
conditional independence relationships between the variables; II)Building
parsimonious parametric models consistent with these relationships; III)
Characterising and testing the effects of covariates on the joint distribution
of the counts. To achieve these goals, we propose an approach based on
graphical probabilistic models, and more specifically partially directed
acyclic graphs."
"['stat.ML', 'stat.CO', 'stat.ME']",Statistical estimation for optimization problems on graphs,"Large graphs abound in machine learning, data mining, and several related
areas. A useful step towards analyzing such graphs is that of obtaining certain
summary statistics - e.g., or the expected length of a shortest path between
two nodes, or the expected weight of a minimum spanning tree of the graph, etc.
These statistics provide insight into the structure of a graph, and they can
help predict global properties of a graph. Motivated thus, we propose to study
statistical properties of structured subgraphs (of a given graph), in
particular, to estimate the expected objective function value of a
combinatorial optimization problem over these subgraphs. The general task is
very difficult, if not unsolvable; so for concreteness we describe a more
specific statistical estimation problem based on spanning trees. We hope that
our position paper encourages others to also study other types of graphical
structures for which one can prove nontrivial statistical estimates."
"['stat.ML', 'stat.TH']",Optimal classification in sparse Gaussian graphic model,"Consider a two-class classification problem where the number of features is
much larger than the sample size. The features are masked by Gaussian noise
with mean zero and covariance matrix $\Sigma$, where the precision matrix
$\Omega=\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,
also unknown, are sparse and each contributes weakly (i.e., rare and weak) to
the classification decision. By obtaining a reasonably good estimate of
$\Omega$, we formulate the setting as a linear regression model. We propose a
two-stage classification method where we first select features by the method of
Innovated Thresholding (IT), and then use the retained features and Fisher's
LDA for classification. In this approach, a crucial problem is how to set the
threshold of IT. We approach this problem by adapting the recent innovation of
Higher Criticism Thresholding (HCT). We find that when useful features are rare
and weak, the limiting behavior of HCT is essentially just as good as the
limiting behavior of ideal threshold, the threshold one would choose if the
underlying distribution of the signals is known (if only). Somewhat
surprisingly, when $\Omega$ is sufficiently sparse, its off-diagonal
coordinates usually do not have a major influence over the classification
decision. Compared to recent work in the case where $\Omega$ is the identity
matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.
Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current
setting is much more general, which needs a new approach and much more
sophisticated analysis. One key component of the analysis is the intimate
relationship between HCT and Fisher's separation. Another key component is the
tight large-deviation bounds for empirical processes for data with
unconventional correlation structures, where graph theory on vertex coloring
plays an important role."
"['stat.ML', 'stat.ME', 'stat.TH']",Bayesian inference as iterated random functions with applications to sequential inference in graphical models,"We propose a general formalism of iterated random functions with semigroup
property, under which exact and approximate Bayesian posterior updates can be
viewed as specific instances. A convergence theory for iterated random
functions is presented. As an application of the general theory we analyze
convergence behaviors of exact and approximate message-passing algorithms that
arise in a sequential change point detection problem formulated via a latent
variable directed graphical model. The sequential inference algorithm and its
supporting theory are illustrated by simulated examples."
['stat.ME'],Speedy Model Selection (SMS) for Copula Models,"We tackle the challenge of efficiently learning the structure of expressive
multivariate real-valued densities of copula graphical models. We start by
theoretically substantiating the conjecture that for many copula families the
magnitude of Spearman's rank correlation coefficient is monotone in the
expected contribution of an edge in network, namely the negative copula
entropy. We then build on this theory and suggest a novel Bayesian approach
that makes use of a prior over values of Spearman's rho for learning
copula-based models that involve a mix of copula families. We demonstrate the
generalization effectiveness of our highly efficient approach on sizable and
varied real-life datasets."
"['stat.ML', 'stat.ME']",The Cluster Graphical Lasso for improved estimation of Gaussian graphical models,"We consider the task of estimating a Gaussian graphical model in the
high-dimensional setting. The graphical lasso, which involves maximizing the
Gaussian log likelihood subject to an l1 penalty, is a well-studied approach
for this task. We begin by introducing a surprising connection between the
graphical lasso and hierarchical clustering: the graphical lasso in effect
performs a two-step procedure, in which (1) single linkage hierarchical
clustering is performed on the variables in order to identify connected
components, and then (2) an l1-penalized log likelihood is maximized on the
subset of variables within each connected component. In other words, the
graphical lasso determines the connected components of the estimated network
via single linkage clustering. Unfortunately, single linkage clustering is
known to perform poorly in certain settings. Therefore, we propose the cluster
graphical lasso, which involves clustering the features using an alternative to
single linkage clustering, and then performing the graphical lasso on the
subset of variables within each cluster. We establish model selection
consistency for this technique, and demonstrate its improved performance
relative to the graphical lasso in a simulation study, as well as in
applications to an equities data set, a university webpage data set, and a gene
expression data set."
"['stat.ML', 'stat.ME']",On Identifying Significant Edges in Graphical Models of Molecular Networks,"Objective: Modelling the associations from high-throughput experimental
molecular data has provided unprecedented insights into biological pathways and
signalling mechanisms. Graphical models and networks have especially proven to
be useful abstractions in this regard. Ad-hoc thresholds are often used in
conjunction with structure learning algorithms to determine significant
associations. The present study overcomes this limitation by proposing a
statistically-motivated approach for identifying significant associations in a
network.
  Methods and Materials: A new method that identifies significant associations
in graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$
norm between the cumulative distribution function (CDF) of the observed edge
confidences and those of its asymptotic counterpart is proposed. The
effectiveness of the proposed method is demonstrated on popular synthetic data
sets as well as publicly available experimental molecular data corresponding to
gene and protein expression profiles.
  Results: The improved performance of the proposed approach is demonstrated
across the synthetic data sets using sensitivity, specificity and accuracy as
performance metrics. The results are also demonstrated across varying sample
sizes and three different structure learning algorithms with widely varying
assumptions. In all cases, the proposed approach has specificity and accuracy
close to 1, while sensitivity increases linearly in the logarithm of the sample
size. The estimated threshold systematically outperforms common ad-hoc ones in
terms of sensitivity while maintaining comparable levels of specificity and
accuracy. Networks from experimental data sets are reconstructed accurately
with respect to the results from the original papers."
"['stat.ML', 'stat.TH']",Learning loopy graphical models with latent variables: Efficient methods and guarantees,"The problem of structure estimation in graphical models with latent variables
is considered. We characterize conditions for tractable graph estimation and
develop efficient methods with provable guarantees. We consider models where
the underlying Markov graph is locally tree-like, and the model is in the
regime of correlation decay. For the special case of the Ising model, the
number of samples $n$ required for structural consistency of our method scales
as $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is the
number of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ is
the depth (i.e., distance from a hidden node to the nearest observed nodes),
and $\eta$ is a parameter which depends on the bounds on node and edge
potentials in the Ising model. Necessary conditions for structural consistency
under any algorithm are derived and our method nearly matches the lower bound
on sample requirements. Further, the proposed method is practical to implement
and provides flexibility to control the number of latent variables and the
cycle lengths in the output graph."
"['stat.CO', 'stat.ML']",Herded Gibbs Sampling,"The Gibbs sampler is one of the most popular algorithms for inference in
statistical models. In this paper, we introduce a herding variant of this
algorithm, called herded Gibbs, that is entirely deterministic. We prove that
herded Gibbs has an $O(1/T)$ convergence rate for models with independent
variables and for fully connected probabilistic graphical models. Herded Gibbs
is shown to outperform Gibbs in the tasks of image denoising with MRFs and
named entity recognition with CRFs. However, the convergence for herded Gibbs
for sparsely connected probabilistic graphical models is still an open problem."
"['stat.CO', 'stat.ML']",Monte Carlo Inference via Greedy Importance Sampling,"We present a new method for conducting Monte Carlo inference in graphical
models which combines explicit search with generalized importance sampling. The
idea is to reduce the variance of importance sampling by searching for
significant points in the target distribution. We prove that it is possible to
introduce search and still maintain unbiasedness. We then demonstrate our
procedure on a few simple inference tasks and show that it can improve the
inference quality of standard MCMC methods, including Gibbs sampling,
Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous work
which showed how greedy importance sampling could be correctly realized in the
one-dimensional case."
"['stat.ML', 'stat.ME']",Network-based clustering with mixtures of L1-penalized Gaussian graphical models: an empirical investigation,"In many applications, multivariate samples may harbor previously unrecognized
heterogeneity at the level of conditional independence or network structure.
For example, in cancer biology, disease subtypes may differ with respect to
subtype-specific interplay between molecular components. Then, both subtype
discovery and estimation of subtype-specific networks present important and
related challenges. To enable such analyses, we put forward a mixture model
whose components are sparse Gaussian graphical models. This brings together
model-based clustering and graphical modeling to permit simultaneous estimation
of cluster assignments and cluster-specific networks. We carry out estimation
within an L1-penalized framework, and investigate several specific penalization
regimes. We present empirical results on simulated data and provide general
recommendations for the formulation and use of mixtures of L1-penalized
Gaussian graphical models."
"['stat.ML', 'stat.ME']",Sparse Nonparametric Graphical Models,"We present some nonparametric methods for graphical modeling. In the discrete
case, where the data are binary or drawn from a finite alphabet, Markov random
fields are already essentially nonparametric, since the cliques can take only a
finite number of values. Continuous data are different. The Gaussian graphical
model is the standard parametric model for continuous data, but it makes
distributional assumptions that are often unrealistic. We discuss two
approaches to building more flexible graphical models. One allows arbitrary
graphs and a nonparametric extension of the Gaussian; the other uses kernel
density estimation and restricts the graphs to trees and forests. Examples of
both methods are presented. We also discuss possible future research directions
for nonparametric graphical modeling."
"['stat.ML', 'stat.TH']",High-dimensional structure estimation in Ising models: Local separation criterion,"We consider the problem of high-dimensional Ising (graphical) model
selection. We propose a simple algorithm for structure estimation based on the
thresholding of the empirical conditional variation distances. We introduce a
novel criterion for tractable graph families, where this method is efficient,
based on the presence of sparse local separators between node pairs in the
underlying graph. For such graphs, the proposed algorithm has a sample
complexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number of
variables, and $J_{\min}$ is the minimum (absolute) edge potential in the
model. We also establish nonasymptotic necessary and sufficient conditions for
structure estimation."
"['stat.ML', 'stat.CO']",A note on the lack of symmetry in the graphical lasso,"The graphical lasso (glasso) is a widely-used fast algorithm for estimating
sparse inverse covariance matrices. The glasso solves an L1 penalized maximum
likelihood problem and is available as an R library on CRAN. The output from
the glasso, a regularized covariance matrix estimate a sparse inverse
covariance matrix estimate, not only identify a graphical model but can also
serve as intermediate inputs into multivariate procedures such as PCA, LDA,
MANOVA, and others. The glasso indeed produces a covariance matrix estimate
which solves the L1 penalized optimization problem in a dual sense; however,
the method for producing the inverse covariance matrix estimator after this
optimization is inexact and may produce asymmetric estimates. This problem is
exacerbated when the amount of L1 regularization that is applied is small,
which in turn is more likely to occur if the true underlying inverse covariance
matrix is not sparse. The lack of symmetry can potentially have consequences.
First, it implies that the covariance and inverse covariance estimates are not
numerical inverses of one another, and second, asymmetry can possibly lead to
negative or complex eigenvalues,rendering many multivariate procedures which
may depend on the inverse covariance estimator unusable. We demonstrate this
problem, explain its causes, and propose possible remedies."
"['stat.ML', 'stat.TH']",Reading Dependencies from Covariance Graphs,"The covariance graph (aka bi-directed graph) of a probability distribution
$p$ is the undirected graph $G$ where two nodes are adjacent iff their
corresponding random variables are marginally dependent in $p$. In this paper,
we present a graphical criterion for reading dependencies from $G$, under the
assumption that $p$ satisfies the graphoid properties as well as weak
transitivity and composition. We prove that the graphical criterion is sound
and complete in certain sense. We argue that our assumptions are not too
restrictive. For instance, all the regular Gaussian probability distributions
satisfy them."
"['stat.ME', 'stat.ML']",Learning the Bayesian Network Structure: Dirichlet Prior versus Data,"In the Bayesian approach to structure learning of graphical models, the
equivalent sample size (ESS) in the Dirichlet prior over the model parameters
was recently shown to have an important effect on the maximum-a-posteriori
estimate of the Bayesian network structure. In our first contribution, we
theoretically analyze the case of large ESS-values, which complements previous
work: among other results, we find that the presence of an edge in a Bayesian
network is favoured over its absence even if both the Dirichlet prior and the
data imply independence, as long as the conditional empirical distribution is
notably different from uniform. In our second contribution, we focus on
realistic ESS-values, and provide an analytical approximation to the ""optimal""
ESS-value in a predictive sense (its accuracy is also validated
experimentally): this approximation provides an understanding as to which
properties of the data have the main effect determining the ""optimal""
ESS-value."
"['stat.ML', 'stat.ME']",Modeling Discrete Interventional Data using Directed Cyclic Graphical Models,"We outline a representation for discrete multivariate distributions in terms
of interventional potential functions that are globally normalized. This
representation can be used to model the effects of interventions, and the
independence properties encoded in this model can be represented as a directed
graph that allows cycles. In addition to discussing inference and sampling with
this representation, we give an exponential family parametrization that allows
parameter estimation to be stated as a convex optimization problem; we also
give a convex relaxation of the task of simultaneous parameter and structure
learning using group l1-regularization. The model is evaluated on simulated
data and intracellular flow cytometry data."
['stat.TH'],High-Dimensional Gaussian Graphical Model Selection: Walk Summability and Local Separation Criterion,"We consider the problem of high-dimensional Gaussian graphical model
selection. We identify a set of graphs for which an efficient estimation
algorithm exists, and this algorithm is based on thresholding of empirical
conditional covariances. Under a set of transparent conditions, we establish
structural consistency (or sparsistency) for the proposed algorithm, when the
number of samples n=omega(J_{min}^{-2} log p), where p is the number of
variables and J_{min} is the minimum (absolute) edge potential of the graphical
model. The sufficient conditions for sparsistency are based on the notion of
walk-summability of the model and the presence of sparse local vertex
separators in the underlying graph. We also derive novel non-asymptotic
necessary conditions on the number of samples required for sparsistency."
"['stat.ML', 'stat.TH']",High-dimensional Sparse Inverse Covariance Estimation using Greedy Methods,"In this paper we consider the task of estimating the non-zero pattern of the
sparse inverse covariance matrix of a zero-mean Gaussian random vector from a
set of iid samples. Note that this is also equivalent to recovering the
underlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We
present two novel greedy approaches to solving this problem. The first
estimates the non-zero covariates of the overall inverse covariance matrix
using a series of global forward and backward greedy steps. The second
estimates the neighborhood of each node in the graph separately, again using
greedy forward and backward steps, and combines the intermediate neighborhoods
to form an overall estimate. The principal contribution of this paper is a
rigorous analysis of the sparsistency, or consistency in recovering the
sparsity pattern of the inverse covariance matrix. Surprisingly, we show that
both the local and global greedy methods learn the full structure of the model
with high probability given just $O(d\log(p))$ samples, which is a
\emph{significant} improvement over state of the art $\ell_1$-regularized
Gaussian MLE (Graphical Lasso) that requires $O(d^2\log(p))$ samples. Moreover,
the restricted eigenvalue and smoothness conditions imposed by our greedy
methods are much weaker than the strong irrepresentable conditions required by
the $\ell_1$-regularization based methods. We corroborate our results with
extensive simulations and examples, comparing our local and global greedy
methods to the $\ell_1$-regularized Gaussian MLE as well as the Neighborhood
Greedy method to that of nodewise $\ell_1$-regularized linear regression
(Neighborhood Lasso)."
"['stat.ML', 'stat.CO']",Exact covariance thresholding into connected components for large-scale Graphical Lasso,"We consider the sparse inverse covariance regularization problem or graphical
lasso with regularization parameter $\rho$. Suppose the co- variance graph
formed by thresholding the entries of the sample covariance matrix at $\rho$ is
decomposed into connected components. We show that the vertex-partition induced
by the thresholded covariance graph is exactly equal to that induced by the
estimated concentration graph. This simple rule, when used as a wrapper around
existing algorithms, leads to enormous performance gains. For large values of
$\rho$, our proposal splits a large graphical lasso problem into smaller
tractable problems, making it possible to solve an otherwise infeasible large
scale graphical lasso problem."
"['stat.ML', 'stat.TH']",On Learning Discrete Graphical Models Using Greedy Methods,"In this paper, we address the problem of learning the structure of a pairwise
graphical model from samples in a high-dimensional setting. Our first main
result studies the sparsistency, or consistency in sparsity pattern recovery,
properties of a forward-backward greedy algorithm as applied to general
statistical models. As a special case, we then apply this algorithm to learn
the structure of a discrete graphical model via neighborhood estimation. As a
corollary of our general result, we derive sufficient conditions on the number
of samples n, the maximum node-degree d and the problem size p, as well as
other conditions on the model parameters, so that the algorithm recovers all
the edges with high probability. Our result guarantees graph selection for
samples scaling as n = Omega(d^2 log(p)), in contrast to existing
convex-optimization based algorithms that require a sample complexity of
\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted
strong convexity condition which is typically milder than irrepresentability
assumptions. We corroborate these results using numerical simulations at the
end."
"['stat.ML', 'stat.TH']",Introduction to Graphical Modelling,"The aim of this chapter is twofold. In the first part we will provide a brief
overview of the mathematical and statistical foundations of graphical models,
along with their fundamental properties, estimation and basic inference
procedures. In particular we will develop Markov networks (also known as Markov
random fields) and Bayesian networks, which comprise most past and current
literature on graphical models. In the second part we will review some
applications of graphical models in systems biology."
"['stat.ML', 'stat.ME']",Dynamic Large Spatial Covariance Matrix Estimation in Application to Semiparametric Model Construction via Variable Clustering: the SCE approach,"To better understand the spatial structure of large panels of economic and
financial time series and provide a guideline for constructing semiparametric
models, this paper first considers estimating a large spatial covariance matrix
of the generalized $m$-dependent and $\beta$-mixing time series (with $J$
variables and $T$ observations) by hard thresholding regularization as long as
${{\log J \, \cx^*(\ct)}}/{T} = \Co(1)$ (the former scheme with some time
dependence measure $\cx^*(\ct)$) or $\log J /{T} = \Co(1)$ (the latter scheme
with some upper bounded mixing coefficient). We quantify the interplay between
the estimators' consistency rate and the time dependence level, discuss an
intuitive resampling scheme for threshold selection, and also prove a general
cross-validation result justifying this. Given a consistently estimated
covariance (correlation) matrix, by utilizing its natural links with graphical
models and semiparametrics, after ""screening"" the (explanatory) variables, we
implement a novel forward (and backward) label permutation procedure to cluster
the ""relevant"" variables and construct the corresponding semiparametric model,
which is further estimated by the groupwise dimension reduction method with
sign constraints. We call this the SCE (screen - cluster - estimate) approach
for modeling high dimensional data with complex spatial structure. Finally we
apply this method to study the spatial structure of large panels of economic
and financial time series and find the proper semiparametric structure for
estimating the consumer price index (CPI) to illustrate its superiority over
the linear models."
"['stat.ML', 'stat.TH']",High-dimensional covariance estimation based on Gaussian graphical models,"Undirected graphs are often used to describe high dimensional distributions.
Under sparsity conditions, the graph can be estimated using
$\ell_1$-penalization methods. We propose and study the following method. We
combine a multiple regression approach with ideas of thresholding and
refitting: first we infer a sparse undirected graphical model structure via
thresholding of each among many $\ell_1$-norm penalized regression functions;
we then estimate the covariance matrix and its inverse using the maximum
likelihood estimator. We show that under suitable conditions, this approach
yields consistent estimation in terms of graphical structure and fast
convergence rates with respect to the operator and Frobenius norm for the
covariance matrix and its inverse. We also derive an explicit bound for the
Kullback Leibler divergence."
"['stat.ML', 'stat.TH']",Trek separation for Gaussian graphical models,"Gaussian graphical models are semi-algebraic subsets of the cone of positive
definite covariance matrices. Submatrices with low rank correspond to
generalizations of conditional independence constraints on collections of
random variables. We give a precise graph-theoretic characterization of when
submatrices of the covariance matrix have small rank for a general class of
mixed graphs that includes directed acyclic and undirected graphs as special
cases. Our new trek separation criterion generalizes the familiar
$d$-separation criterion. Proofs are based on the trek rule, the resulting
matrix factorizations and classical theorems of algebraic combinatorics on the
expansions of determinants of path polynomials."
"['stat.ML', 'stat.CO']",High-dimensional Graphical Model Search with gRapHD R Package,"This paper presents the R package gRapHD for efficient selection of
high-dimensional undirected graphical models. The package provides tools for
selecting trees, forests and decomposable models minimizing information
criteria such as AIC or BIC, and for displaying the independence graphs of the
models. It has also some useful tools for analysing graphical structures. It
supports the use of discrete, continuous, or both types of variables
simultaneously."
"['stat.ML', 'stat.ME']",Graph-Valued Regression,"Undirected graphical models encode in a graph $G$ the dependency structure of
a random vector $Y$. In many applications, it is of interest to model $Y$ given
another random vector $X$ as input. We refer to the problem of estimating the
graph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In
this paper, we propose a semiparametric method for estimating $G(x)$ that
builds a tree on the $X$ space just as in CART (classification and regression
trees), but at each leaf of the tree estimates a graph. We call the method
``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of
Go-CART using dyadic partitioning trees, establishing oracle inequalities on
risk minimization and tree partition consistency. We also demonstrate the
application of Go-CART to a meteorological dataset, showing how graph-valued
regression can provide a useful tool for analyzing complex data."
"['stat.ML', 'stat.ME']",An empirical comparative study of approximate methods for binary graphical models; application to the search of associations among causes of death in French death certificates,"Looking for associations among multiple variables is a topical issue in
statistics due to the increasing amount of data encountered in biology,
medicine and many other domains involving statistical applications. Graphical
models have recently gained popularity for this purpose in the statistical
literature. Following the ideas of the LASSO procedure designed for the linear
regression framework, recent developments dealing with graphical model
selection have been based on $\ell_1$-penalization. In the binary case,
however, exact inference is generally very slow or even intractable because of
the form of the so-called log-partition function. Various approximate methods
have recently been proposed in the literature and the main objective of this
paper is to compare them. Through an extensive simulation study, we show that a
simple modification of a method relying on a Gaussian approximation achieves
good performance and is very fast. We present a real application in which we
search for associations among causes of death recorded on French death
certificates."
"['stat.ML', 'stat.TH']",Learning Gaussian Tree Models: Analysis of Error Exponents and Extremal Structures,"The problem of learning tree-structured Gaussian graphical models from
independent and identically distributed (i.i.d.) samples is considered. The
influence of the tree structure and the parameters of the Gaussian distribution
on the learning rate as the number of samples increases is discussed.
Specifically, the error exponent corresponding to the event that the estimated
tree structure differs from the actual unknown tree structure of the
distribution is analyzed. Finding the error exponent reduces to a least-squares
problem in the very noisy learning regime. In this regime, it is shown that the
extremal tree structure that minimizes the error exponent is the star for any
fixed set of correlation coefficients on the edges of the tree. If the
magnitudes of all the correlation coefficients are less than 0.63, it is also
shown that the tree structure that maximizes the error exponent is the Markov
chain. In other words, the star and the chain graphs represent the hardest and
the easiest structures to learn in the class of tree-structured Gaussian
graphical models. This result can also be intuitively explained by correlation
decay: pairs of nodes which are far apart, in terms of graph distance, are
unlikely to be mistaken as edges by the maximum-likelihood estimator in the
asymptotic regime."
"['stat.ML', 'stat.ME', 'stat.TH']",Penalized Likelihood Methods for Estimation of Sparse High Dimensional Directed Acyclic Graphs,"Directed acyclic graphs (DAGs) are commonly used to represent causal
relationships among random variables in graphical models. Applications of these
models arise in the study of physical, as well as biological systems, where
directed edges between nodes represent the influence of components of the
system on each other. The general problem of estimating DAGs from observed data
is computationally NP-hard, Moreover two directed graphs may be observationally
equivalent. When the nodes exhibit a natural ordering, the problem of
estimating directed graphs reduces to the problem of estimating the structure
of the network. In this paper, we propose a penalized likelihood approach that
directly estimates the adjacency matrix of DAGs. Both lasso and adaptive lasso
penalties are considered and an efficient algorithm is proposed for estimation
of high dimensional DAGs. We study variable selection consistency of the two
penalties when the number of variables grows to infinity with the sample size.
We show that although lasso can only consistently estimate the true network
under stringent assumptions, adaptive lasso achieves this task under mild
regularity conditions. The performance of the proposed methods is compared to
alternative methods in simulated, as well as real, data examples."
"['stat.ML', 'stat.TH']",Parameterizations and fitting of bi-directed graph models to categorical data,"We discuss two parameterizations of models for marginal independencies for
discrete distributions which are representable by bi-directed graph models,
under the global Markov property. Such models are useful data analytic tools
especially if used in combination with other graphical models. The first
parameterization, in the saturated case, is also known as the multivariate
logistic transformation, the second is a variant that allows, in some (but not
all) cases, variation independent parameters. An algorithm for maximum
likelihood fitting is proposed, based on an extension of the Aitchison and
Silvey method."
"['stat.ML', 'stat.TH']",Degrees of freedom for off-the-grid sparse estimation,"A central question in modern machine learning and imaging sciences is to
quantify the number of effective parameters of vastly over-parameterized
models. The degrees of freedom is a mathematically convenient way to define
this number of parameters. Its computation and properties are well understood
when dealing with discretized linear models, possibly regularized using
sparsity. In this paper, we argue that this way of thinking is plagued when
dealing with models having very large parameter spaces. In this case it makes
more sense to consider ""off-the-grid"" approaches, using a continuous parameter
space. This type of approach is the one favoured when training multi-layer
perceptrons, and is also becoming popular to solve super-resolution problems in
imaging. Training these off-the-grid models with a sparsity inducing prior can
be achieved by solving a convex optimization problem over the space of
measures, which is often called the Beurling Lasso (Blasso), and is the
continuous counterpart of the celebrated Lasso parameter selection method. In
previous works, the degrees of freedom for the Lasso was shown to coincide with
the size of the smallest solution support. Our main contribution is a proof of
a continuous counterpart to this result for the Blasso. Our findings suggest
that discretized methods actually vastly over-estimate the number of intrinsic
continuous degrees of freedom. Our second contribution is a detailed study of
the case of sampling Fourier coefficients in 1D, which corresponds to a
super-resolution problem. We show that our formula for the degrees of freedom
is valid outside of a set of measure zero of observations, which in turn
justifies its use to compute an unbiased estimator of the prediction risk using
the Stein Unbiased Risk Estimator (SURE)."
"['stat.ML', 'stat.ME']",An Iterative Algorithm for Fitting Nonconvex Penalized Generalized Linear Models with Grouped Predictors,"High-dimensional data pose challenges in statistical learning and modeling.
Sometimes the predictors can be naturally grouped where pursuing the
between-group sparsity is desired. Collinearity may occur in real-world
high-dimensional applications where the popular $l_1$ technique suffers from
both selection inconsistency and prediction inaccuracy. Moreover, the problems
of interest often go beyond Gaussian models. To meet these challenges,
nonconvex penalized generalized linear models with grouped predictors are
investigated and a simple-to-implement algorithm is proposed for computation. A
rigorous theoretical result guarantees its convergence and provides tight
preliminary scaling. This framework allows for grouped predictors and nonconvex
penalties, including the discrete $l_0$ and the `$l_0+l_2$' type penalties.
Penalty design and parameter tuning for nonconvex penalties are examined.
Applications of super-resolution spectrum estimation in signal processing and
cancer classification with joint gene selection in bioinformatics show the
performance improvement by nonconvex penalized estimation."
['stat.TH'],Network Clustering by Embedding of Attribute-augmented Graphs,"In this paper we propose a new approach to detect clusters in undirected
graphs with attributed vertices. The aim is to group vertices which are similar
not only in terms of structural connectivity but also in terms of attribute
values. We incorporate structural and attribute similarities between the
vertices in an augmented graph by creating additional vertices and edges as
proposed in [5, 27]. The augmented graph is embedded in a Euclidean space
associated to its Laplacian and apply a modified K-means algorithm to identify
clusters. The modified K-means uses a vector distance measure where to each
original vertex is assigned a vector-valued set of coordinates depending on
both structural connectivity and attribute similarities. To define the
coordinate vectors we employ an adaptive AMG (Algebraic MultiGrid) method to
identify the coordinate directions in the embedding Euclidean space extending
our previous result for graphs without attributes. We demonstrate the
effectiveness of our proposed clustering method on both synthetic and
real-world attributed graphs."
['stat.TH'],A Fast PC Algorithm with Reversed-order Pruning and A Parallelization Strategy,"The PC algorithm is the state-of-the-art algorithm for causal structure
discovery on observational data. It can be computationally expensive in the
worst case due to the conditional independence tests are performed in an
exhaustive-searching manner. This makes the algorithm computationally
intractable when the task contains several hundred or thousand nodes,
particularly when the true underlying causal graph is dense. We propose a
critical observation that the conditional set rendering two nodes independent
is non-unique, and including certain redundant nodes do not sacrifice result
accuracy. Based on this finding, the innovations of our work are two-folds.
First, we innovate on a reserve order linkage pruning PC algorithm which
significantly increases the algorithm's efficiency. Second, we propose a
parallel computing strategy for statistical independence tests by leveraging
tensor computation, which brings further speedup. We also prove the proposed
algorithm does not induce statistical power loss under mild graph and data
dimensionality assumptions. Experimental results show that the single-threaded
version of the proposed algorithm can achieve a 6-fold speedup compared to the
PC algorithm on a dense 95-node graph, and the parallel version can make a
825-fold speed-up. We also provide proof that the proposed algorithm is
consistent under the same set of conditions with conventional PC algorithm."
"['stat.ML', 'stat.TH']",Almost exact recovery in noisy semi-supervised learning,"Graph-based semi-supervised learning methods combine the graph structure and
labeled data to classify unlabeled data. In this work, we study the effect of a
noisy oracle on classification. In particular, we derive the Maximum A
Posteriori (MAP) estimator for clustering a Degree Corrected Stochastic Block
Model (DC-SBM) when a noisy oracle reveals a fraction of the labels. We then
propose an algorithm derived from a continuous relaxation of the MAP, and we
establish its consistency. Numerical experiments show that our approach
achieves promising performance on synthetic and real data sets, even in the
case of very noisy labeled data."
"['stat.ML', 'stat.ME']",Estimating a Latent Tree for Extremes,"The Latent River Problem has emerged as a flagship problem for causal
discovery in extreme value statistics. This paper gives QTree, a simple and
efficient algorithm to solve the Latent River Problem that outperforms existing
methods. QTree returns a directed graph and achieves almost perfect recovery on
the Upper Danube, the existing benchmark dataset, as well as on new data from
the Lower Colorado River in Texas. It can handle missing data, has an automated
parameter tuning procedure, and runs in time $O(n |V|^2)$, where $n$ is the
number of observations and $|V|$ the number of nodes in the graph. In addition,
under a Bayesian network model for extreme values with propagating noise, we
show that the QTree estimator returns for $n\to\infty$ a.s. the correct tree."
"['stat.ML', 'stat.CO']",On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity,"We study the multi-marginal partial optimal transport (POT) problem between
$m$ discrete (unbalanced) measures with at most $n$ supports. We first prove
that we can obtain two equivalence forms of the multimarginal POT problem in
terms of the multimarginal optimal transport problem via novel extensions of
cost tensor. The first equivalence form is derived under the assumptions that
the total masses of each measure are sufficiently close while the second
equivalence form does not require any conditions on these masses but at the
price of more sophisticated extended cost tensor. Our proof techniques for
obtaining these equivalence forms rely on novel procedures of moving mass in
graph theory to push transportation plan into appropriate regions. Finally,
based on the equivalence forms, we develop optimization algorithm, named
ApproxMPOT algorithm, that builds upon the Sinkhorn algorithm for solving the
entropic regularized multimarginal optimal transport. We demonstrate that the
ApproxMPOT algorithm can approximate the optimal value of multimarginal POT
problem with a computational complexity upper bound of the order
$\tilde{\mathcal{O}}(m^3(n+1)^{m}/ \varepsilon^2)$ where $\varepsilon > 0$
stands for the desired tolerance."
"['stat.ML', 'stat.ME', 'stat.TH']",Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation,"We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Specially, under both
Gaussian and sub-Gaussian models, we study the sample size conditions for the
well-known Chow-Liu algorithm to exactly recover the equivalence class of the
polytree, which is uniquely represented by a CPDAG. We also study the error
rate for the estimation of the inverse correlation matrix under such models.
Our theoretical findings are illustrated by comprehensive numerical
simulations, and experiments on benchmark data also demonstrate the robustness
of the method when the ground truth graphical structure can only be
approximated by a polytree."
['stat.ME'],Improving COVID-19 Forecasting using eXogenous Variables,"In this work, we study the pandemic course in the United States by
considering national and state levels data. We propose and compare multiple
time-series prediction techniques which incorporate auxiliary variables. One
type of approach is based on spatio-temporal graph neural networks which
forecast the pandemic course by utilizing a hybrid deep learning architecture
and human mobility data. Nodes in this graph represent the state-level deaths
due to COVID-19, edges represent the human mobility trend and temporal edges
correspond to node attributes across time. The second approach is based on a
statistical technique for COVID-19 mortality prediction in the United States
that uses the SARIMA model and eXogenous variables. We evaluate these
techniques on both state and national levels COVID-19 data in the United States
and claim that the SARIMA and MCP models generated forecast values by the
eXogenous variables can enrich the underlying model to capture complexity in
respectively national and state levels data. We demonstrate significant
enhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum
improvement in forecasting accuracy by 64.58% and 59.18% (on average) over the
GCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)
over the GCN-LSTM model in the state level data. Additionally, our proposed
model outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy
on average."
['stat.ME'],Fractional order graph neural network,"This paper proposes fractional order graph neural networks (FGNNs), optimized
by the approximation strategy to address the challenges of local optimum of
classic and fractional graph neural networks which are specialised at
aggregating information from the feature and adjacent matrices of connected
nodes and their neighbours to solve learning tasks on non-Euclidean data such
as graphs. Meanwhile the approximate calculation of fractional order gradients
also overcomes the high computational complexity of fractional order
derivations. We further prove that such an approximation is feasible and the
FGNN is unbiased towards global optimization solution. Extensive experiments on
citation networks show that FGNN achieves great advantage over baseline models
when selected appropriate fractional order."
"['stat.ML', 'stat.TH']",Asymptotics of Network Embeddings Learned via Subsampling,"Network data are ubiquitous in modern machine learning, with tasks of
interest including node classification, node clustering and link prediction. A
frequent approach begins by learning an Euclidean embedding of the network, to
which algorithms developed for vector-valued data are applied. For large
networks, embeddings are learned using stochastic gradient methods where the
sub-sampling scheme can be freely chosen. Despite the strong empirical
performance of such methods, they are not well understood theoretically. Our
work encapsulates representation methods using a subsampling approach, such as
node2vec, into a single unifying framework. We prove, under the assumption that
the graph is exchangeable, that the distribution of the learned embedding
vectors asymptotically decouples. Moreover, we characterize the asymptotic
distribution and provided rates of convergence, in terms of the latent
parameters, which includes the choice of loss function and the embedding
dimension. This provides a theoretical foundation to understand what the
embedding vectors represent and how well these methods perform on downstream
tasks. Notably, we observe that typically used loss functions may lead to
shortcomings, such as a lack of Fisher consistency."
"['stat.ML', 'stat.TH']",Impossibility of Partial Recovery in the Graph Alignment Problem,"Random graph alignment refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges. This can be
viewed as an average-case and noisy version of the well-known graph isomorphism
problem. For the correlated Erd\""os-R\'enyi model, we prove an impossibility
result for partial recovery in the sparse regime, with constant average degree
and correlation, as well as a general bound on the maximal reachable overlap.
Our bound is tight in the noiseless case (the graph isomorphism problem) and we
conjecture that it is still tight with noise. Our proof technique relies on a
careful application of the probabilistic method to build automorphisms between
tree components of a subcritical Erd\""os-R\'enyi graph."
"['stat.ML', 'stat.OT']",Evolving-Graph Gaussian Processes,"Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph
structured domains. Existing approaches have focused on static structures,
whereas many real graph data represent a dynamic structure, limiting the
applications of GGPs. To overcome this we propose evolving-Graph Gaussian
Processes (e-GGPs). The proposed method is capable of learning the transition
function of graph vertices over time with a neighbourhood kernel to model the
connectivity and interaction changes between vertices. We assess the
performance of our method on time-series regression problems where graphs
evolve over time. We demonstrate the benefits of e-GGPs over static graph
Gaussian Process approaches."
"['stat.ML', 'stat.ME']",Interpretable Network Representation Learning with Principal Component Analysis,"We consider the problem of interpretable network representation learning for
samples of network-valued data. We propose the Principal Component Analysis for
Networks (PCAN) algorithm to identify statistically meaningful low-dimensional
representations of a network sample via subgraph count statistics. The PCAN
procedure provides an interpretable framework for which one can readily
visualize, explore, and formulate predictive models for network samples. We
furthermore introduce a fast sampling-based algorithm, sPCAN, which is
significantly more computationally efficient than its counterpart, but still
enjoys advantages of interpretability. We investigate the relationship between
these two methods and analyze their large-sample properties under the common
regime where the sample of networks is a collection of kernel-based random
graphs. We show that under this regime, the embeddings of the sPCAN method
enjoy a central limit theorem and moreover that the population level embeddings
of PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,
cluster, and classify observations in network samples arising in nature,
including functional connectivity network samples and dynamic networks
describing the political co-voting habits of the U.S. Senate. Our analyses
reveal that our proposed algorithm provides informative and discriminatory
features describing the networks in each sample. The PCAN and sPCAN methods
build on the current literature of network representation learning and set the
stage for a new line of research in interpretable learning on network-valued
data. Publicly available software for the PCAN and sPCAN methods are available
at https://www.github.com/jihuilee/."
['stat.ME'],Leveraging semantically similar queries for ranking via combining representations,"In modern ranking problems, different and disparate representations of the
items to be ranked are often available. It is sensible, then, to try to combine
these representations to improve ranking. Indeed, learning to rank via
combining representations is both principled and practical for learning a
ranking function for a particular query. In extremely data-scarce settings,
however, the amount of labeled data available for a particular query can lead
to a highly variable and ineffective ranking function. One way to mitigate the
effect of the small amount of data is to leverage information from semantically
similar queries. Indeed, as we demonstrate in simulation settings and real data
examples, when semantically similar queries are available it is possible to
gainfully use them when ranking with respect to a particular query. We describe
and explore this phenomenon in the context of the bias-variance trade off and
apply it to the data-scarce settings of a Bing navigational graph and the
Drosophila larva connectome."
"['stat.ML', 'stat.ME']",A Multilayered Block Network Model to Forecast Large Dynamic Transportation Graphs: an Application to US Air Transport,"Dynamic transportation networks have been analyzed for years by means of
static graph-based indicators in order to study the temporal evolution of
relevant network components, and to reveal complex dependencies that would not
be easily detected by a direct inspection of the data. This paper presents a
state-of-the-art latent network model to forecast multilayer dynamic graphs
that are increasingly common in transportation and proposes a community-based
extension to reduce the computational burden. Flexible time series analysis is
obtained by modeling the probability of edges between vertices through latent
Gaussian processes. The models and Bayesian inference are illustrated on a
sample of 10-year data from four major airlines within the US air
transportation system. Results show how the estimated latent parameters from
the models are related to the airline's connectivity dynamics, and their
ability to project the multilayer graph into the future for out-of-sample full
network forecasts, while stochastic blockmodeling allows for the identification
of relevant communities. Reliable network predictions would allow policy-makers
to better understand the dynamics of the transport system, and help in their
planning on e.g. route development, or the deployment of new regulations."
"['stat.ML', 'stat.TH']",On the Power of Preconditioning in Sparse Linear Regression,"Sparse linear regression is a fundamental problem in high-dimensional
statistics, but strikingly little is known about how to efficiently solve it
without restrictive conditions on the design matrix. We consider the
(correlated) random design setting, where the covariates are independently
drawn from a multivariate Gaussian $N(0,\Sigma)$ with $\Sigma : n \times n$,
and seek estimators $\hat{w}$ minimizing $(\hat{w}-w^*)^T\Sigma(\hat{w}-w^*)$,
where $w^*$ is the $k$-sparse ground truth. Information theoretically, one can
achieve strong error bounds with $O(k \log n)$ samples for arbitrary $\Sigma$
and $w^*$; however, no efficient algorithms are known to match these guarantees
even with $o(n)$ samples, without further assumptions on $\Sigma$ or $w^*$. As
far as hardness, computational lower bounds are only known with worst-case
design matrices. Random-design instances are known which are hard for the
Lasso, but these instances can generally be solved by Lasso after a simple
change-of-basis (i.e. preconditioning).
  In this work, we give upper and lower bounds clarifying the power of
preconditioning in sparse linear regression. First, we show that the
preconditioned Lasso can solve a large class of sparse linear regression
problems nearly optimally: it succeeds whenever the dependency structure of the
covariates, in the sense of the Markov property, has low treewidth -- even if
$\Sigma$ is highly ill-conditioned. Second, we construct (for the first time)
random-design instances which are provably hard for an optimally preconditioned
Lasso. In fact, we complete our treewidth classification by proving that for
any treewidth-$t$ graph, there exists a Gaussian Markov Random Field on this
graph such that the preconditioned Lasso, with any choice of preconditioner,
requires $\Omega(t^{1/20})$ samples to recover $O(\log n)$-sparse signals when
covariates are drawn from this model."
['stat.TH'],Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models,"We consider the problem of learning a tree-structured Ising model from data,
such that subsequent predictions computed using the model are accurate.
Concretely, we aim to learn a model such that posteriors $P(X_i|X_S)$ for small
sets of variables $S$ are accurate. Since its introduction more than 50 years
ago, the Chow-Liu algorithm, which efficiently computes the maximum likelihood
tree, has been the benchmark algorithm for learning tree-structured graphical
models. A bound on the sample complexity of the Chow-Liu algorithm with respect
to the prediction-centric local total variation loss was shown in [BK19]. While
those results demonstrated that it is possible to learn a useful model even
when recovering the true underlying graph is impossible, their bound depends on
the maximum strength of interactions and thus does not achieve the
information-theoretic optimum. In this paper, we introduce a new algorithm that
carefully combines elements of the Chow-Liu algorithm with tree metric
reconstruction methods to efficiently and optimally learn tree Ising models
under a prediction-centric loss. Our algorithm is robust to model
misspecification and adversarial corruptions. In contrast, we show that the
celebrated Chow-Liu algorithm can be arbitrarily suboptimal."
"['stat.ML', 'stat.TH']",Unsuitability of NOTEARS for Causal Graph Discovery,"Causal Discovery methods aim to identify a DAG structure that represents
causal relationships from observational data. In this article, we stress that
it is important to test such methods for robustness in practical settings. As
our main example, we analyze the NOTEARS method, for which we demonstrate a
lack of scale-invariance. We show that NOTEARS is a method that aims to
identify a parsimonious DAG from the data that explains the residual variance.
We conclude that NOTEARS is not suitable for identifying truly causal
relationships from the data."
['stat.ME'],A Meta Learning Approach to Discerning Causal Graph Structure,"We explore the usage of meta-learning to derive the causal direction between
variables by optimizing over a measure of distribution simplicity. We
incorporate a stochastic graph representation which includes latent variables
and allows for more generalizability and graph structure expression. Our model
is able to learn causal direction indicators for complex graph structures
despite effects of latent confounders. Further, we explore robustness of our
method with respect to violations of our distributional assumptions and data
scarcity. Our model is particularly robust to modest data scarcity, but is less
robust to distributional changes. By interpreting the model predictions as
stochastic events, we propose a simple ensemble method classifier to reduce the
outcome variability as an average of biased events. This methodology
demonstrates ability to infer the existence as well as the direction of a
causal relationship between data distributions."
"['stat.ME', 'stat.TH']",Inferring Granger Causality from Irregularly Sampled Time Series,"Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability."
"['stat.ML', 'stat.ME']",Statistical embedding: Beyond principal components,"There has been an intense recent activity in embedding of very high
dimensional and nonlinear data structures, much of it in the data science and
machine learning literature. We survey this activity in four parts. In the
first part we cover nonlinear methods such as principal curves,
multidimensional scaling, local linear methods, ISOMAP, graph based methods and
kernel based methods. The second part is concerned with topological embedding
methods, in particular mapping topological properties into persistence
diagrams. Another type of data sets with a tremendous growth is very
high-dimensional network data. The task considered in part three is how to
embed such data in a vector space of moderate dimension to make the data
amenable to traditional techniques such as cluster and classification
techniques. The final part of the survey deals with embedding in
$\mathbb{R}^2$, which is visualization. Three methods are presented: $t$-SNE,
UMAP and LargeVis based on methods in parts one, two and three, respectively.
The methods are illustrated and compared on two simulated data sets; one
consisting of a triple of noisy Ranunculoid curves, and one consisting of
networks of increasing complexity and with two types of nodes."
"['stat.ML', 'stat.CO']",Analysis of high-dimensional Continuous Time Markov Chains using the Local Bouncy Particle Sampler,"Sampling the parameters of high-dimensional Continuous Time Markov Chains
(CTMC) is a challenging problem with important applications in many fields of
applied statistics. In this work a recently proposed type of non-reversible
rejection-free Markov Chain Monte Carlo (MCMC) sampler, the Bouncy Particle
Sampler (BPS), is brought to bear to this problem. BPS has demonstrated its
favorable computational efficiency compared with state-of-the-art MCMC
algorithms, however to date applications to real-data scenario were scarce. An
important aspect of the practical implementation of BPS is the simulation of
event times. Default implementations use conservative thinning bounds. Such
bounds can slow down the algorithm and limit the computational performance. Our
paper develops an algorithm with an exact analytical solution to the random
event times in the context of CTMCs. Our local version of BPS algorithm takes
advantage of the sparse structure in the target factor graph and we also
provide a framework for assessing the computational complexity of local BPS
algorithms."
['stat.ME'],Definite Non-Ancestral Relations and Structure Learning,"In causal graphical models based on directed acyclic graphs (DAGs), directed
paths represent causal pathways between the corresponding variables. The
variable at the beginning of such a path is referred to as an ancestor of the
variable at the end of the path. Ancestral relations between variables play an
important role in causal modeling. In existing literature on structure
learning, these relations are usually deduced from learned structures and used
for orienting edges or formulating constraints of the space of possible DAGs.
However, they are usually not posed as immediate target of inference. In this
work we investigate the graphical characterization of ancestral relations via
CPDAGs and d-separation relations. We propose a framework that can learn
definite non-ancestral relations without first learning the skeleton. This
frame-work yields structural information that can be used in both score- and
constraint-based algorithms to learn causal DAGs more efficiently."
"['stat.ML', 'stat.TH']",Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data,"This study investigates the theoretical foundations of t-distributed
stochastic neighbor embedding (t-SNE), a popular nonlinear dimension reduction
and data visualization method. A novel theoretical framework for the analysis
of t-SNE based on the gradient descent approach is presented. For the early
exaggeration stage of t-SNE, we show its asymptotic equivalence to a power
iteration based on the underlying graph Laplacian, characterize its limiting
behavior, and uncover its deep connection to Laplacian spectral clustering, and
fundamental principles including early stopping as implicit regularization. The
results explain the intrinsic mechanism and the empirical benefits of such a
computational strategy. For the embedding stage of t-SNE, we characterize the
kinematics of the low-dimensional map throughout the iterations, and identify
an amplification phase, featuring the intercluster repulsion and the expansive
behavior of the low-dimensional map. The general theory explains the fast
convergence rate and the exceptional empirical performance of t-SNE for
visualizing clustered data, brings forth the interpretations of the t-SNE
output, and provides theoretical guidance for selecting tuning parameters in
various applications."
"['stat.ML', 'stat.TH']",Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time,"We study the problem of learning Bayesian networks where an
$\epsilon$-fraction of the samples are adversarially corrupted. We focus on the
fully-observable case where the underlying graph structure is known. In this
work, we present the first nearly-linear time algorithm for this problem with a
dimension-independent error guarantee. Previous robust algorithms with
comparable error guarantees are slower by at least a factor of $(d/\epsilon)$,
where $d$ is the number of variables in the Bayesian network and $\epsilon$ is
the fraction of corrupted samples.
  Our algorithm and analysis are considerably simpler than those in previous
work. We achieve this by establishing a direct connection between robust
learning of Bayesian networks and robust mean estimation. As a subroutine in
our algorithm, we develop a robust mean estimation algorithm whose runtime is
nearly-linear in the number of nonzeros in the input samples, which may be of
independent interest."
"['stat.ML', 'stat.ME']",High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach,"Undirected graphical models have been widely used to model the conditional
independence structure of high-dimensional random vector data for years. In
many modern applications such as EEG and fMRI data, the observations are
multivariate random functions rather than scalars. To model the conditional
independence of this type of data, functional graphical models are proposed and
have attracted an increasing attention in recent years. In this paper, we
propose a neighborhood selection approach to estimate Gaussian functional
graphical models. We first estimate the neighborhood of all nodes via
function-on-function regression, and then we can recover the whole graph
structure based on the neighborhood information. By estimating conditional
structure directly, we can circumvent the need of a well-defined precision
operator which generally does not exist. Besides, we can better explore the
effect of the choice of function basis for dimension reduction. We give a
criterion for choosing the best function basis and motivate two practically
useful choices, which we justified by both theory and experiments and show that
they are better than expanding each function onto its own FPCA basis as in
previous literature. In addition, the neighborhood selection approach is
computationally more efficient than fglasso as it is more easy to do parallel
computing. The statistical consistency of our proposed methods in
high-dimensional setting are supported by both theory and experiment."
['stat.TH'],Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables,"The problem of selecting optimal valid backdoor adjustment sets to estimate
causal effects in graphical models with hidden and conditioned variables is
addressed. Previous work has defined optimality as achieving the smallest
asymptotic variance compared to other adjustment sets and identified a
graphical criterion for an optimal set for the case without hidden variables.
For the case with hidden variables currently a sufficient graphical criterion
and a corresponding construction algorithm exists. Here optimality is
characterized by an information-theoretic approach based on the conditional
mutual informations among cause, effect, adjustment set, and conditioned
variables. This characterization allows to derive the main contributions of
this paper: A necessary and sufficient graphical criterion for the existence of
an optimal adjustment set and a definition and algorithm to construct it.
Further, the optimal set is valid if and only if a valid adjustment set exists
and has smaller (or equal) asymptotic variance compared to the Adjust-set
proposed in Perkovic et al. (2018) (arXiv:1606.06903) for any graph, whether
graphical optimality holds or not. The results are valid for a class of
estimators whose asymptotic variance follows a certain information-theoretic
relation. Numerical experiments indicate that the asymptotic results also hold
for relatively small sample sizes. For estimators outside of the class studied
here none of the considered adjustment sets outperforms all others, but a
minimized variant of the optimal set proposed here tends to have lower
variance. Surprisingly, among the randomly created setups more than 80\%
fulfill the optimality conditions indicating that also in many real-world
scenarios graphical optimality may hold. Code is available as part of the
python package \url{https://github.com/jakobrunge/tigramite}."
"['stat.ML', 'stat.ME']",Thresholded Graphical Lasso Adjusts for Latent Variables: Application to Functional Neural Connectivity,"In neuroscience, researchers seek to uncover the connectivity of neurons from
large-scale neural recordings or imaging; often people employ graphical model
selection and estimation techniques for this purpose. But, existing
technologies can only record from a small subset of neurons leading to a
challenging problem of graph selection in the presence of extensive latent
variables. Chandrasekaran et al. (2012) proposed a convex program to address
this problem that poses challenges from both a computational and statistical
perspective. To solve this problem, we propose an incredibly simple solution:
apply a hard thresholding operator to existing graph selection methods.
Conceptually simple and computationally attractive, we demonstrate that
thresholding the graphical Lasso, neighborhood selection, or CLIME estimators
have superior theoretical properties in terms of graph selection consistency as
well as stronger empirical results than existing approaches for the latent
variable graphical model problem. We also demonstrate the applicability of our
approach through a neuroscience case study on calcium-imaging data to estimate
functional neural connections."
"['stat.ML', 'stat.ME']","Graph matching between bipartite and unipartite networks: to collapse, or not to collapse, that is the question","Graph matching consists of aligning the vertices of two unlabeled graphs in
order to maximize the shared structure across networks; when the graphs are
unipartite, this is commonly formulated as minimizing their edge disagreements.
In this paper, we address the common setting in which one of the graphs to
match is a bipartite network and one is unipartite. Commonly, the bipartite
networks are collapsed or projected into a unipartite graph, and graph matching
proceeds as in the classical setting. This potentially leads to noisy edge
estimates and loss of information. We formulate the graph matching problem
between a bipartite and a unipartite graph using an undirected graphical model,
and introduce methods to find the alignment with this model without collapsing.
We theoretically demonstrate that our methodology is consistent, and provide
non-asymptotic conditions that ensure exact recovery of the matching solution.
In simulations and real data examples, we show how our methods can result in a
more accurate matching than the naive approach of transforming the bipartite
networks into unipartite, and we demonstrate the performance gains achieved by
our method in simulated and real data networks, including a
co-authorship-citation network pair, and brain structural and functional data."
"['stat.ML', 'stat.ME']",Sparse Partial Least Squares for Coarse Noisy Graph Alignment,"Graph signal processing (GSP) provides a powerful framework for analyzing
signals arising in a variety of domains. In many applications of GSP, multiple
network structures are available, each of which captures different aspects of
the same underlying phenomenon. To integrate these different data sources,
graph alignment techniques attempt to find the best correspondence between
vertices of two graphs. We consider a generalization of this problem, where
there is no natural one-to-one mapping between vertices, but where there is
correspondence between the community structures of each graph. Because we seek
to learn structure at this higher community level, we refer to this problem as
""coarse"" graph alignment. To this end, we propose a novel regularized partial
least squares method which both incorporates the observed graph structures and
imposes sparsity in order to reflect the underlying block community structure.
We provide efficient algorithms for our method and demonstrate its
effectiveness in simulations."
['stat.ME'],A Quotient Space Formulation for Generative Statistical Analysis of Graphical Data,"Complex analyses involving multiple, dependent random quantities often lead
to graphical models - a set of nodes denoting variables of interest, and
corresponding edges denoting statistical interactions between nodes. To develop
statistical analyses for graphical data, especially towards generative
modeling, one needs mathematical representations and metrics for matching and
comparing graphs, and subsequent tools, such as geodesics, means, and
covariances. This paper utilizes a quotient structure to develop efficient
algorithms for computing these quantities, leading to useful statistical tools,
including principal component analysis, statistical testing, and modeling. We
demonstrate the efficacy of this framework using datasets taken from several
problem areas, including letters, biochemical structures, and social networks."
"['stat.ML', 'stat.ME']",Thresholded Adaptive Validation: Tuning the Graphical Lasso for Graph Recovery,"Many Machine Learning algorithms are formulated as regularized optimization
problems, but their performance hinges on a regularization parameter that needs
to be calibrated to each application at hand. In this paper, we propose a
general calibration scheme for regularized optimization problems and apply it
to the graphical lasso, which is a method for Gaussian graphical modeling. The
scheme is equipped with theoretical guarantees and motivates a thresholding
pipeline that can improve graph recovery. Moreover, requiring at most one line
search over the regularization path, the calibration scheme is computationally
more efficient than competing schemes that are based on resampling. Finally, we
show in simulations that our approach can improve on the graph recovery of
other approaches considerably."
"['stat.ML', 'stat.ME']",Efficient Variational Bayesian Structure Learning of Dynamic Graphical Models,"Estimating time-varying graphical models are of paramount importance in
various social, financial, biological, and engineering systems, since the
evolution of such networks can be utilized for example to spot trends, detect
anomalies, predict vulnerability, and evaluate the impact of interventions.
Existing methods require extensive tuning of parameters that control the graph
sparsity and temporal smoothness. Furthermore, these methods are
computationally burdensome with time complexity O(NP^3) for P variables and N
time points. As a remedy, we propose a low-complexity tuning-free Bayesian
approach, named BADGE. Specifically, we impose temporally-dependent
spike-and-slab priors on the graphs such that they are sparse and varying
smoothly across time. A variational inference algorithm is then derived to
learn the graph structures from the data automatically. Owning to the
pseudo-likelihood and the mean-field approximation, the time complexity of
BADGE is only O(NP^2). Additionally, by identifying the frequency-domain
resemblance to the time-varying graphical models, we show that BADGE can be
extended to learning frequency-varying inverse spectral density matrices, and
yields graphical models for multivariate stationary time series. Numerical
results on both synthetic and real data show that that BADGE can better recover
the underlying true graphs, while being more efficient than the existing
methods, especially for high-dimensional cases."
"['stat.ML', 'stat.TH']",Clustering multilayer graphs with missing nodes,"Relationship between agents can be conveniently represented by graphs. When
these relationships have different modalities, they are better modelled by
multilayer graphs where each layer is associated with one modality. Such graphs
arise naturally in many contexts including biological and social networks.
Clustering is a fundamental problem in network analysis where the goal is to
regroup nodes with similar connectivity profiles. In the past decade, various
clustering methods have been extended from the unilayer setting to multilayer
graphs in order to incorporate the information provided by each layer. While
most existing works assume - rather restrictively - that all layers share the
same set of nodes, we propose a new framework that allows for layers to be
defined on different sets of nodes. In particular, the nodes not recorded in a
layer are treated as missing. Within this paradigm, we investigate several
generalizations of well-known clustering methods in the complete setting to the
incomplete one and prove some consistency results under the Multi-Layer
Stochastic Block Model assumption. Our theoretical results are complemented by
thorough numerical comparisons between our proposed algorithms on synthetic
data, and also on real datasets, thus highlighting the promising behaviour of
our methods in various settings."
['stat.ME'],A Simulation-Based Test of Identifiability for Bayesian Causal Inference,"This paper introduces a procedure for testing the identifiability of Bayesian
models for causal inference. Although the do-calculus is sound and complete
given a causal graph, many practical assumptions cannot be expressed in terms
of graph structure alone, such as the assumptions required by instrumental
variable designs, regression discontinuity designs, and within-subjects
designs. We present simulation-based identifiability (SBI), a fully automated
identification test based on a particle optimization scheme with simulated
observations. This approach expresses causal assumptions as priors over
functions in a structural causal model, including flexible priors using
Gaussian processes. We prove that SBI is asymptotically sound and complete, and
produces practical finite-sample bounds. We also show empirically that SBI
agrees with known results in graph-based identification as well as with
widely-held intuitions for designs in which graph-based methods are
inconclusive."
"['stat.ML', 'stat.TH']",Markov Random Geometric Graph (MRGG): A Growth Model for Temporal Dynamic Networks,"We introduce Markov Random Geometric Graphs (MRGGs), a growth model for
temporal dynamic networks. It is based on a Markovian latent space dynamic:
consecutive latent points are sampled on the Euclidean Sphere using an unknown
Markov kernel; and two nodes are connected with a probability depending on a
unknown function of their latent geodesic distance. More precisely, at each
stamp-time k we add a latent point X k sampled by jumping from the previous one
X k--1 in a direction chosen uniformly Y k and with a length r k drawn from an
unknown distribution called the latitude function. The connection probabilities
between each pair of nodes are equal to the envelope function of the distance
between these two latent points. We provide theoretical guarantees for the
non-parametric estimation of the latitude and the envelope functions. We
propose an efficient algorithm that achieves those non-parametric estimation
tasks based on an ad-hoc Hierarchical Agglomerative Clustering approach, and we
deploy this analysis on a real data-set given by exchange of messages on a
social network."
"['stat.ML', 'stat.TH']",A General Pairwise Comparison Model for Extremely Sparse Networks,"Statistical inference using pairwise comparison data has been an effective
approach to analyzing complex and sparse networks. In this paper we propose a
general framework for modeling the mutual interaction in a network, which
enjoys ample flexibility in terms of parametrization. Within this setup, we
establish that the maximum likelihood estimator (MLE) for the latent scores of
the subjects is uniformly consistent under a near-minimal condition on network
sparsity. This condition is sharp in terms of the leading order asymptotics
describing the sparsity. The proof utilizes a novel chaining technique based on
the error-induced metric as well as careful counting of comparison graph
structures. Our results guarantee that the MLE is a valid estimator for
inference in large-scale comparison networks where data is asymptotically
deficient. Numerical simulations are provided to complement the theoretical
analysis."
"['stat.ML', 'stat.ME']",Identifiability of Hierarchical Latent Attribute Models,"Hierarchical Latent Attribute Models (HLAMs) are a family of discrete latent
variable models that are attracting increasing attention in educational,
psychological, and behavioral sciences. The key ingredients of an HLAM include
a binary structural matrix and a directed acyclic graph specifying hierarchical
constraints on the configurations of latent attributes. These components encode
practitioners' design information and carry important scientific meanings.
Despite the popularity of HLAMs, the fundamental identifiability issue remains
unaddressed. The existence of the attribute hierarchy graph leads to degenerate
parameter space, and the potentially unknown structural matrix further
complicates the identifiability problem. This paper addresses this issue of
identifying the latent structure and model parameters underlying an HLAM. We
develop sufficient and necessary identifiability conditions. These results
directly and sharply characterize the different impacts on identifiability cast
by different attribute types in the graph. The proposed conditions not only
provide insights into diagnostic test designs under the attribute hierarchy,
but also serve as tools to assess the validity of an estimated HLAM."
"['stat.ML', 'stat.TH']",On the Estimation of Network Complexity: Dimension of Graphons,"Network complexity has been studied for over half a century and has found a
wide range of applications. Many methods have been developed to characterize
and estimate the complexity of networks. However, there has been little
research with statistical guarantees. In this paper, we develop a statistical
theory of graph complexity in a general model of random graphs, the so-called
graphon model.
  Given a graphon, we endow the latent space of the nodes with the neighborhood
distance that measures the propensity of two nodes to be connected with similar
nodes. Our complexity index is then based on the covering number and the
Minkowski dimension of (a purified version of) this metric space. Although the
latent space is not identifiable, these indices turn out to be identifiable.
This notion of complexity has simple interpretations on popular examples of
random graphs: it matches the number of communities in stochastic block models;
the dimension of the Euclidean space in random geometric graphs; the regularity
of the link function in H\""older graphon models.
  From a single observation of the graph, we construct an estimator of the
neighborhood-distance and show universal non-asymptotic bounds for its risk,
matching minimax lower bounds. Based on this estimated distance, we compute the
corresponding covering number and Minkowski dimension and we provide optimal
non-asymptotic error bounds for these two plug-in estimators."
"['stat.ML', 'stat.CO']",Learning non-Gaussian graphical models via Hessian scores and triangular transport,"Undirected probabilistic graphical models represent the conditional
dependencies, or Markov properties, of a collection of random variables.
Knowing the sparsity of such a graphical model is valuable for modeling
multivariate distributions and for efficiently performing inference. While the
problem of learning graph structure from data has been studied extensively for
certain parametric families of distributions, most existing methods fail to
consistently recover the graph structure for non-Gaussian data. Here we propose
an algorithm for learning the Markov structure of continuous and non-Gaussian
distributions. To characterize conditional independence, we introduce a score
based on integrated Hessian information from the joint log-density, and we
prove that this score upper bounds the conditional mutual information for a
general class of distributions. To compute the score, our algorithm SING
estimates the density using a deterministic coupling, induced by a triangular
transport map, and iteratively exploits sparse structure in the map to reveal
sparsity in the graph. For certain non-Gaussian datasets, we show that our
algorithm recovers the graph structure even with a biased approximation to the
density. Among other examples, we apply sing to learn the dependencies between
the states of a chaotic dynamical system with local interactions."
"['stat.ML', 'stat.TH']",Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks,"It is known that the current graph neural networks (GNNs) are difficult to
make themselves deep due to the problem known as over-smoothing. Multi-scale
GNNs are a promising approach for mitigating the over-smoothing problem.
However, there is little explanation of why it works empirically from the
viewpoint of learning theory. In this study, we derive the optimization and
generalization guarantees of transductive learning algorithms that include
multi-scale GNNs. Using the boosting theory, we prove the convergence of the
training error under weak learning-type conditions. By combining it with
generalization gap bounds in terms of transductive Rademacher complexity, we
show that a test error bound of a specific type of multi-scale GNNs that
decreases corresponding to the number of node aggregations under some
conditions. Our results offer theoretical explanations for the effectiveness of
the multi-scale structure against the over-smoothing problem. We apply boosting
algorithms to the training of multi-scale GNNs for real-world node prediction
tasks. We confirm that its performance is comparable to existing GNNs, and the
practical behaviors are consistent with theoretical observations. Code is
available at https://github.com/delta2323/GB-GNN."
"['stat.ML', 'stat.TH']",Pair-Matching: Links Prediction with Adaptive Queries,"The pair-matching problem appears in many applications where one wants to
discover good matches between pairs of entities or individuals. Formally, the
set of individuals is represented by the nodes of a graph where the edges,
unobserved at first, represent the good matches. The algorithm queries pairs of
nodes and observes the presence/absence of edges. Its goal is to discover as
many edges as possible with a fixed budget of queries. Pair-matching is a
particular instance of multi-armed bandit problem in which the arms are pairs
of individuals and the rewards are edges linking these pairs. This bandit
problem is non-standard though, as each arm can only be played once.
  Given this last constraint, sublinear regret can be expected only if the
graph presents some underlying structure. This paper shows that sublinear
regret is achievable in the case where the graph is generated according to a
Stochastic Block Model (SBM) with two communities. Optimal regret bounds are
computed for this pair-matching problem. They exhibit a phase transition
related to the Kesten-Stigum threshold for community detection in SBM. The
pair-matching problem is considered in the case where each node is constrained
to be sampled less than a given amount of times. We show how optimal regret
rates depend on this constraint. The paper is concluded by a conjecture
regarding the optimal regret when the number of communities is larger than 2.
Contrary to the two communities case, we argue that a statistical-computational
gap would appear in this problem."
"['stat.ML', 'stat.TH']",A polynomial-time algorithm for learning nonparametric causal graphs,"We establish finite-sample guarantees for a polynomial-time algorithm for
learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from
data. The analysis is model-free and does not assume linearity, additivity,
independent noise, or faithfulness. Instead, we impose a condition on the
residual variances that is closely related to previous work on linear models
with equal variances. Compared to an optimal algorithm with oracle knowledge of
the variable ordering, the additional cost of the algorithm is linear in the
dimension $d$ and the number of samples $n$. Finally, we compare the proposed
algorithm to existing approaches in a simulation study."
['stat.ME'],Statistical learning for change point and anomaly detection in graphs,"Complex systems which can be represented in the form of static and dynamic
graphs arise in different fields, e.g. communication, engineering and industry.
One of the interesting problems in analysing dynamic network structures is to
monitor changes in their development. Statistical learning, which encompasses
both methods based on artificial intelligence and traditional statistics, can
be used to progress in this research area. However, the majority of approaches
apply only one or the other framework. In this paper, we discuss the
possibility of bringing together both disciplines in order to create enhanced
network monitoring procedures focussing on the example of combining statistical
process control and deep learning algorithms. Together with the presentation of
change point and anomaly detection in network data, we propose to monitor the
response times of ambulance services, applying jointly the control chart for
quantile function values and a graph convolutional network."
"['stat.ML', 'stat.TH']",Regularized spectral methods for clustering signed networks,"We study the problem of $k$-way clustering in signed graphs. Considerable
attention in recent years has been devoted to analyzing and modeling signed
graphs, where the affinity measure between nodes takes either positive or
negative values. Recently, Cucuringu et al. [CDGT 2019] proposed a spectral
method, namely SPONGE (Signed Positive over Negative Generalized Eigenproblem),
which casts the clustering task as a generalized eigenvalue problem optimizing
a suitably defined objective function. This approach is motivated by social
balance theory, where the clustering task aims to decompose a given network
into disjoint groups, such that individuals within the same group are connected
by as many positive edges as possible, while individuals from different groups
are mainly connected by negative edges. Through extensive numerical
simulations, SPONGE was shown to achieve state-of-the-art empirical
performance. On the theoretical front, [CDGT 2019] analyzed SPONGE and the
popular Signed Laplacian method under the setting of a Signed Stochastic Block
Model (SSBM), for $k=2$ equal-sized clusters, in the regime where the graph is
moderately dense.
  In this work, we build on the results in [CDGT 2019] on two fronts for the
normalized versions of SPONGE and the Signed Laplacian. Firstly, for both
algorithms, we extend the theoretical analysis in [CDGT 2019] to the general
setting of $k \geq 2$ unequal-sized clusters in the moderately dense regime.
Secondly, we introduce regularized versions of both methods to handle sparse
graphs -- a regime where standard spectral methods underperform -- and provide
theoretical guarantees under the same SSBM model. To the best of our knowledge,
regularized spectral methods have so far not been considered in the setting of
clustering signed graphs. We complement our theoretical results with an
extensive set of numerical experiments on synthetic data."
"['stat.CO', 'stat.ML']",Integer Programming-based Error-Correcting Output Code Design for Robust Classification,"Error-Correcting Output Codes (ECOCs) offer a principled approach for
combining simple binary classifiers into multiclass classifiers. In this paper,
we investigate the problem of designing optimal ECOCs to achieve both nominal
and adversarial accuracy using Support Vector Machines (SVMs) and binary deep
learning models. In contrast to previous literature, we present an Integer
Programming (IP) formulation to design minimal codebooks with desirable error
correcting properties. Our work leverages the advances in IP solvers to
generate codebooks with optimality guarantees. To achieve tractability, we
exploit the underlying graph-theoretic structure of the constraint set in our
IP formulation. This enables us to use edge clique covers to substantially
reduce the constraint set. Our codebooks achieve a high nominal accuracy
relative to standard codebooks (e.g., one-vs-all, one-vs-one, and dense/sparse
codes). We also estimate the adversarial accuracy of our ECOC-based classifiers
in a white-box setting. Our IP-generated codebooks provide non-trivial
robustness to adversarial perturbations even without any adversarial training."
"['stat.ML', 'stat.TH']",Uncertainty Quantification for Inferring Hawkes Networks,"Multivariate Hawkes processes are commonly used to model streaming networked
event data in a wide variety of applications. However, it remains a challenge
to extract reliable inference from complex datasets with uncertainty
quantification. Aiming towards this, we develop a statistical inference
framework to learn causal relationships between nodes from networked data,
where the underlying directed graph implies Granger causality. We provide
uncertainty quantification for the maximum likelihood estimate of the network
multivariate Hawkes process by providing a non-asymptotic confidence set. The
main technique is based on the concentration inequalities of continuous-time
martingales. We compare our method to the previously-derived asymptotic Hawkes
process confidence interval, and demonstrate the strengths of our method in an
application to neuronal connectivity reconstruction."
['stat.TH'],Fundamental Linear Algebra Problem of Gaussian Inference,"Underlying many Bayesian inference techniques that seek to approximate the
posterior as a Gaussian distribution is a fundamental linear algebra problem
that must be solved for both the mean and key entries of the covariance. Even
when the true posterior is not Gaussian (e.g., in the case of nonlinear
measurement functions) we can use variational schemes that repeatedly solve
this linear algebra problem at each iteration. In most cases, the question is
not whether a solution to this problem exists, but rather how we can exploit
problem-specific structure to find it efficiently. Our contribution is to
clearly state the Fundamental Linear Algebra Problem of Gaussian Inference
(FLAPOGI) and to provide a novel presentation (using Kronecker algebra) of the
not-so-well-known result of Takahashi et al. (1973) that makes it possible to
solve for key entries of the covariance matrix. We first provide a global
solution and then a local version that can be implemented using local message
passing amongst a collection of agents calculating in parallel. Contrary to
belief propagation, our local scheme is guaranteed to converge in both the mean
and desired covariance quantities to the global solution even when the
underlying factor graph is loopy; in the case of synchronous updates, we
provide a bound on the number of iterations required for convergence. Compared
to belief propagation, this guaranteed convergence comes at the cost of
additional storage, calculations, and communication links in the case of loops;
however, we show how these can be automatically constructed on the fly using
only local information."
"['stat.ML', 'stat.TH']",Learning Linear Non-Gaussian Graphical Models with Multidirected Edges,"In this paper we propose a new method to learn the underlying acyclic mixed
graph of a linear non-Gaussian structural equation model given observational
data. We build on an algorithm proposed by Wang and Drton, and we show that one
can augment the hidden variable structure of the recovered model by learning
{\em multidirected edges} rather than only directed and bidirected ones.
Multidirected edges appear when more than two of the observed variables have a
hidden common cause. We detect the presence of such hidden causes by looking at
higher order cumulants and exploiting the multi-trek rule. Our method recovers
the correct structure when the underlying graph is a bow-free acyclic mixed
graph with potential multi-directed edges."
"['stat.ML', 'stat.CO']",Directed hypergraph neural network,"To deal with irregular data structure, graph convolution neural networks have
been developed by a lot of data scientists. However, data scientists just have
concentrated primarily on developing deep neural network method for un-directed
graph. In this paper, we will present the novel neural network method for
directed hypergraph. In the other words, we will develop not only the novel
directed hypergraph neural network method but also the novel directed
hypergraph based semi-supervised learning method. These methods are employed to
solve the node classification task. The two datasets that are used in the
experiments are the cora and the citeseer datasets. Among the classic directed
graph based semi-supervised learning method, the novel directed hypergraph
based semi-supervised learning method, the novel directed hypergraph neural
network method that are utilized to solve this node classification task, we
recognize that the novel directed hypergraph neural network achieves the
highest accuracies."
"['stat.ML', 'stat.TH']",Ranking and synchronization from pairwise measurements via SVD,"Given a measurement graph $G= (V,E)$ and an unknown signal $r \in
\mathbb{R}^n$, we investigate algorithms for recovering $r$ from pairwise
measurements of the form $r_i - r_j$; $\{i,j\} \in E$. This problem arises in a
variety of applications, such as ranking teams in sports data and time
synchronization of distributed networks. Framed in the context of ranking, the
task is to recover the ranking of $n$ teams (induced by $r$) given a small
subset of noisy pairwise rank offsets. We propose a simple SVD-based
algorithmic pipeline for both the problem of time synchronization and ranking.
We provide a detailed theoretical analysis in terms of robustness against both
sampling sparsity and noise perturbations with outliers, using results from
matrix perturbation and random matrix theory. Our theoretical findings are
complemented by a detailed set of numerical experiments on both synthetic and
real data, showcasing the competitiveness of our proposed algorithms with other
state-of-the-art methods."
"['stat.ML', 'stat.TH']",Fractal Gaussian Networks: A sparse random graph model based on Gaussian Multiplicative Chaos,"We propose a novel stochastic network model, called Fractal Gaussian Network
(FGN), that embodies well-defined and analytically tractable fractal
structures. Such fractal structures have been empirically observed in diverse
applications. FGNs interpolate continuously between the popular purely random
geometric graphs (a.k.a. the Poisson Boolean network), and random graphs with
increasingly fractal behavior. In fact, they form a parametric family of sparse
random geometric graphs that are parametrized by a fractality parameter $\nu$
which governs the strength of the fractal structure. FGNs are driven by the
latent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical
model of fractality in its own right. We asymptotically characterize the
expected number of edges and triangle in FGNs. We then examine the natural
question of detecting the presence of fractality and the problem of parameter
estimation based on observed network data, in addition to fundamental
properties of the FGN as a random graph model. We also explore fractality in
community structures by unveiling a natural stochastic block model in the
setting of FGNs."
"['stat.ML', 'stat.TH']",Efficient random graph matching via degree profiles,"Random graph matching refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges; a prominent
example is when the two random graphs are given by Erd\H{o}s-R\'{e}nyi graphs
$G(n,\frac{d}{n})$. This can be viewed as an average-case and noisy version of
the graph isomorphism problem. Under this model, the maximum likelihood
estimator is equivalent to solving the intractable quadratic assignment
problem. This work develops an $\tilde{O}(n d^2+n^2)$-time algorithm which
perfectly recovers the true vertex correspondence with high probability,
provided that the average degree is at least $d = \Omega(\log^2 n)$ and the two
graphs differ by at most $\delta = O( \log^{-2}(n) )$ fraction of edges. For
dense graphs and sparse graphs, this can be improved to $\delta = O(
\log^{-2/3}(n) )$ and $\delta = O( \log^{-2}(d) )$ respectively, both in
polynomial time. The methodology is based on appropriately chosen distance
statistics of the degree profiles (empirical distribution of the degrees of
neighbors). Before this work, the best known result achieves $\delta=O(1)$ and
$n^{o(1)} \leq d \leq n^c$ for some constant $c$ with an $n^{O(\log n)}$-time
algorithm \cite{barak2018nearly} and $\delta=\tilde O((d/n)^4)$ and $d =
\tilde{\Omega}(n^{4/5})$ with a polynomial-time algorithm
\cite{dai2018performance}."
['stat.ME'],Latent Instrumental Variables as Priors in Causal Inference based on Independence of Cause and Mechanism,"Causal inference methods based on conditional independence construct Markov
equivalent graphs, and cannot be applied to bivariate cases. The approaches
based on independence of cause and mechanism state, on the contrary, that
causal discovery can be inferred for two observations. In our contribution, we
challenge to reconcile these two research directions. We study the role of
latent variables such as latent instrumental variables and hidden common causes
in the causal graphical structures. We show that the methods based on the
independence of cause and mechanism, indirectly contain traces of the existence
of the hidden instrumental variables. We derive a novel algorithm to infer
causal relationships between two variables, and we validate the proposed method
on simulated data and on a benchmark of cause-effect pairs. We illustrate by
our experiments that the proposed approach is simple and extremely competitive
in terms of empirical accuracy compared to the state-of-the-art methods."
"['stat.ML', 'stat.CO']","Marginal Densities, Factor Graph Duality, and High-Temperature Series Expansions","We prove that the marginal densities of a global probability mass function in
a primal normal factor graph and the corresponding marginal densities in the
dual normal factor graph are related via local mappings. The mapping depends on
the Fourier transform of the local factors of the models. Details of the
mapping, including its fixed points, are derived for the Ising model, and then
extended to the Potts model. By employing the mapping, we can transform
simultaneously all the estimated marginal densities from one domain to the
other, which is advantageous if estimating the marginals can be carried out
more efficiently in the dual domain. An example of particular significance is
the ferromagnetic Ising model in a positive external field, for which there is
a rapidly mixing Markov chain (called the subgraphs-world process) to generate
configurations in the dual normal factor graph of the model. Our numerical
experiments illustrate that the proposed procedure can provide more accurate
estimates of marginal densities in various settings."
"['stat.ME', 'stat.ML']",Learning the Markov order of paths in a network,"We study the problem of learning the Markov order in categorical sequences
that represent paths in a network, i.e. sequences of variable lengths where
transitions between states are constrained to a known graph. Such data pose
challenges for standard Markov order detection methods and demand modelling
techniques that explicitly account for the graph constraint. Adopting a
multi-order modelling framework for paths, we develop a Bayesian learning
technique that (i) more reliably detects the correct Markov order compared to a
competing method based on the likelihood ratio test, (ii) requires considerably
less data compared to methods using AIC or BIC, and (iii) is robust against
partial knowledge of the underlying constraints. We further show that a
recently published method that uses a likelihood ratio test has a tendency to
overfit the true Markov order of paths, which is not the case for our Bayesian
technique. Our method is important for data scientists analyzing patterns in
categorical sequence data that are subject to (partially) known constraints,
e.g. sequences with forbidden words, mobility trajectories and click stream
data, or sequence data in bioinformatics. Addressing the key challenge of model
selection, our work is further relevant for the growing body of research that
emphasizes the need for higher-order models in network analysis."
"['stat.ML', 'stat.TH']",Causal Feature Selection via Orthogonal Search,"The problem of inferring the direct causal parents of a response variable
among a large set of explanatory variables is of high practical importance in
many disciplines. Recent work in the field of causal discovery exploits
invariance properties of models across different experimental conditions for
detecting direct causal links. However, these approaches generally do not scale
well with the number of explanatory variables, are difficult to extend to
nonlinear relationships, and require data across different experiments.
Inspired by {\em Debiased} machine learning methods, we study a
one-vs.-the-rest feature selection approach to discover the direct causal
parent of the response. We propose an algorithm that works for purely
observational data, while also offering theoretical guarantees, including the
case of partially nonlinear relationships. Requiring only one estimation for
each variable, we can apply our approach even to large graphs, demonstrating
significant improvements compared to established approaches."
"['stat.ML', 'stat.TH']",Maximum Likelihood Estimation and Graph Matching in Errorfully Observed Networks,"Given a pair of graphs with the same number of vertices, the inexact graph
matching problem consists in finding a correspondence between the vertices of
these graphs that minimizes the total number of induced edge disagreements. We
study this problem from a statistical framework in which one of the graphs is
an errorfully observed copy of the other. We introduce a corrupting channel
model, and show that in this model framework, the solution to the graph
matching problem is a maximum likelihood estimator. Necessary and sufficient
conditions for consistency of this MLE are presented, as well as a relaxed
notion of consistency in which a negligible fraction of the vertices need not
be matched correctly. The results are used to study matchability in several
families of random graphs, including edge independent models, random regular
graphs and small-world networks. We also use these results to introduce
measures of matching feasibility, and experimentally validate the results on
simulated and real-world networks."
"['stat.ML', 'stat.CO', 'stat.TH']",TRAMP: Compositional Inference with TRee Approximate Message Passing,"We introduce tramp, standing for TRee Approximate Message Passing, a python
package for compositional inference in high-dimensional tree-structured models.
The package provides an unifying framework to study several approximate message
passing algorithms previously derived for a variety of machine learning tasks
such as generalized linear models, inference in multi-layer networks, matrix
factorization, and reconstruction using non-separable penalties. For some
models, the asymptotic performance of the algorithm can be theoretically
predicted by the state evolution, and the measurements entropy estimated by the
free entropy formalism. The implementation is modular by design: each module,
which implements a factor, can be composed at will with other modules to solve
complex inference tasks. The user only needs to declare the factor graph of the
model: the inference algorithm, state evolution and entropy estimation are
fully automated."
"['stat.ML', 'stat.TH']",A maximum principle argument for the uniform convergence of graph Laplacian regressors,"This paper investigates the use of methods from partial differential
equations and the Calculus of variations to study learning problems that are
regularized using graph Laplacians. Graph Laplacians are a powerful, flexible
method for capturing local and global geometry in many classes of learning
problems, and the techniques developed in this paper help to broaden the
methodology of studying such problems. In particular, we develop the use of
maximum principle arguments to establish asymptotic consistency guarantees
within the context of noise corrupted, non-parametric regression with samples
living on an unknown manifold embedded in $\mathbb{R}^d$. The maximum principle
arguments provide a new technical tool which informs parameter selection by
giving concrete error estimates in terms of various regularization parameters.
A review of learning algorithms which utilize graph Laplacians, as well as
previous developments in the use of differential equation and variational
techniques to study those algorithms, is given. In addition, new connections
are drawn between Laplacian methods and other machine learning techniques, such
as kernel regression and k-nearest neighbor methods."
"['stat.ML', 'stat.CO']",Efficient Path Algorithms for Clustered Lasso and OSCAR,"In high dimensional regression, feature clustering by their effects on
outcomes is often as important as feature selection. For that purpose,
clustered Lasso and octagonal shrinkage and clustering algorithm for regression
(OSCAR) are used to make feature groups automatically by pairwise $L_1$ norm
and pairwise $L_\infty$ norm, respectively. This paper proposes efficient path
algorithms for clustered Lasso and OSCAR to construct solution paths with
respect to their regularization parameters. Despite too many terms in
exhaustive pairwise regularization, their computational costs are reduced by
using symmetry of those terms. Simple equivalent conditions to check
subgradient equations in each feature group are derived by some graph theories.
The proposed algorithms are shown to be more efficient than existing algorithms
in numerical experiments."
"['stat.ME', 'stat.ML']",A Nonparametric Bayesian Model for Sparse Dynamic Multigraphs,"As the availability and importance of temporal interaction data--such as
email communication--increases, it becomes increasingly important to understand
the underlying structure that underpins these interactions. Often these
interactions form a multigraph, where we might have multiple interactions
between two entities. Such multigraphs tend to be sparse yet structured, and
their distribution often evolves over time. Existing statistical models with
interpretable parameters can capture some, but not all, of these properties. We
propose a dynamic nonparametric model for interaction multigraphs that combines
the sparsity of edge-exchangeable multigraphs with dynamic clustering patterns
that tend to reinforce recent behavioral patterns. We show that our method
yields improved held-out likelihood over stationary variants, and impressive
predictive performance against a range of state-of-the-art dynamic graph
models."
"['stat.ML', 'stat.TH']",Kernel k-Groups via Hartigan's Method,"Energy statistics was proposed by Sz\' ekely in the 80's inspired by Newton's
gravitational potential in classical mechanics and it provides a model-free
hypothesis test for equality of distributions. In its original form, energy
statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
machine learning. To find local solutions of such an optimization problem, we
propose kernel k-groups, which is an extension of Hartigan's method to kernel
spaces. Kernel k-groups is cheaper than spectral clustering and has the same
computational cost as kernel k-means (which is based on Lloyd's heuristic) but
our numerical results show an improved performance, especially in higher
dimensions. Moreover, we verify the efficiency of kernel k-groups in community
detection in sparse stochastic block models which has fascinating applications
in several areas of science."
"['stat.ML', 'stat.TH']",High-Dimensional Inference for Cluster-Based Graphical Models,"Motivated by modern applications in which one constructs graphical models
based on a very large number of features, this paper introduces a new class of
cluster-based graphical models, in which variable clustering is applied as an
initial step for reducing the dimension of the feature space. We employ model
assisted clustering, in which the clusters contain features that are similar to
the same unobserved latent variable. Two different cluster-based Gaussian
graphical models are considered: the latent variable graph, corresponding to
the graphical model associated with the unobserved latent variables, and the
cluster-average graph, corresponding to the vector of features averaged over
clusters. Our study reveals that likelihood based inference for the latent
graph, not analyzed previously, is analytically intractable. Our main
contribution is the development and analysis of alternative estimation and
inference strategies, for the precision matrix of an unobservable latent vector
$Z$. We replace the likelihood of the data by an appropriate class of empirical
risk functions, that can be specialized to the latent graphical model and to
the simpler, but under-analyzed, cluster-average graphical model. The
estimators thus derived can be used for inference on the graph structure, for
instance on edge strength or pattern recovery. Inference is based on the
asymptotic limits of the entry-wise estimates of the precision matrices
associated with the conditional independence graphs under consideration. While
taking the uncertainty induced by the clustering step into account, we
establish Berry-Esseen central limit theorems for the proposed estimators. It
is noteworthy that, although the clusters are estimated adaptively from the
data, the central limit theorems regarding the entries of the estimated graphs
are proved under the same conditions one would use if the clusters were
known...."
"['stat.ML', 'stat.CO']",Learning DAGs without imposing acyclicity,"We explore if it is possible to learn a directed acyclic graph (DAG) from
data without imposing explicitly the acyclicity constraint. In particular, for
Gaussian distributions, we frame structural learning as a sparse matrix
factorization problem and we empirically show that solving an
$\ell_1$-penalized optimization yields to good recovery of the true graph and,
in general, to almost-DAG graphs. Moreover, this approach is computationally
efficient and is not affected by the explosion of combinatorial complexity as
in classical structural learning algorithms."
"['stat.ML', 'stat.ME', 'stat.TH']",Tree-Projected Gradient Descent for Estimating Gradient-Sparse Parameters on Graphs,"We study estimation of a gradient-sparse parameter vector
$\boldsymbol{\theta}^* \in \mathbb{R}^p$, having strong gradient-sparsity
$s^*:=\|\nabla_G \boldsymbol{\theta}^*\|_0$ on an underlying graph $G$. Given
observations $Z_1,\ldots,Z_n$ and a smooth, convex loss function $\mathcal{L}$
for which $\boldsymbol{\theta}^*$ minimizes the population risk
$\mathbb{E}[\mathcal{L}(\boldsymbol{\theta};Z_1,\ldots,Z_n)]$, we propose to
estimate $\boldsymbol{\theta}^*$ by a projected gradient descent algorithm that
iteratively and approximately projects gradient steps onto spaces of vectors
having small gradient-sparsity over low-degree spanning trees of $G$. We show
that, under suitable restricted strong convexity and smoothness assumptions for
the loss, the resulting estimator achieves the squared-error risk
$\frac{s^*}{n} \log (1+\frac{p}{s^*})$ up to a multiplicative constant that is
independent of $G$. In contrast, previous polynomial-time algorithms have only
been shown to achieve this guarantee in more specialized settings, or under
additional assumptions for $G$ and/or the sparsity pattern of $\nabla_G
\boldsymbol{\theta}^*$. As applications of our general framework, we apply our
results to the examples of linear models and generalized linear models with
random design."
"['stat.ML', 'stat.ME']",Graph-based calibration transfer,"The problem of transferring calibrations from a primary to a secondary
instrument, i.e. calibration transfer (CT), has been a matter of considerable
research in chemometrics over the past decades. Current state-of-the-art (SoA)
methods like (piecewise) direct standardization perform well when suitable
transfer standards are available. However, stable calibration standards that
share similar (spectral) features with the calibration samples are not always
available. Towards enabling CT with arbitrary calibration standards, we propose
a novel CT technique that employs manifold regularization of the partial least
squares (PLS) objective. In particular, our method enforces that calibration
standards, measured on primary and secondary instruments, have (nearly)
invariant projections in the latent variable space of the primary calibration
model. Thereby, our approach implicitly removes inter-device variation in the
predictive directions of X which is in contrast to most state-of-the-art
techniques that employ explicit pre-processing of the input data. We test our
approach on the well-known corn benchmark data set employing the NBS glass
standard spectra for instrument standardization and compare the results with
current SoA methods."
"['stat.CO', 'stat.ML']",Kernel methods library for pattern analysis and machine learning in python,"Kernel methods have proven to be powerful techniques for pattern analysis and
machine learning (ML) in a variety of domains. However, many of their original
or advanced implementations remain in Matlab. With the incredible rise and
adoption of Python in the ML and data science world, there is a clear need for
a well-defined library that enables not only the use of popular kernels, but
also allows easy definition of customized kernels to fine-tune them for diverse
applications. The kernelmethods library fills that important void in the python
ML ecosystem in a domain-agnostic fashion, allowing the sample data type to be
anything from numerical, categorical, graphs or a combination of them. In
addition, this library provides a number of well-defined classes to make
various kernel-based operations efficient (for large scale datasets), modular
(for ease of domain adaptation), and inter-operable (across different
ecosystems). The library is available at
https://github.com/raamana/kernelmethods."
['stat.ML'],COVID-19 growth prediction using multivariate long short term memory,"Coronavirus disease (COVID-19) spread forecasting is an important task to
track the growth of the pandemic. Existing predictions are merely based on
qualitative analyses and mathematical modeling. The use of available big data
with machine learning is still limited in COVID-19 growth prediction even
though the availability of data is abundance. To make use of big data in the
prediction using deep learning, we use long short-term memory (LSTM) method to
learn the correlation of COVID-19 growth over time. The structure of an LSTM
layer is searched heuristically until the best validation score is achieved.
First, we trained training data containing confirmed cases from around the
globe. We achieved favorable performance compared with that of the recurrent
neural network (RNN) method with a comparable low validation error. The
evaluation is conducted based on graph visualization and root mean squared
error (RMSE). We found that it is not easy to achieve the same quantity of
confirmed cases over time. However, LSTM provide a similar pattern between the
actual cases and prediction. In the future, our proposed prediction can be used
for anticipating forthcoming pandemics. The code is provided here:
https://github.com/cbasemaster/lstmcorona"
['stat.ML'],Causal Bayesian Optimization,"This paper studies the problem of globally optimizing a variable of interest
that is part of a causal model in which a sequence of interventions can be
performed. This problem arises in biology, operational research, communications
and, more generally, in all fields where the goal is to optimize an output
metric of a system of interconnected nodes. Our approach combines ideas from
causal inference, uncertainty quantification and sequential decision making. In
particular, it generalizes Bayesian optimization, which treats the input
variables of the objective function as independent, to scenarios where causal
information is available. We show how knowing the causal graph significantly
improves the ability to reason about optimal decision making strategies
decreasing the optimization cost while avoiding suboptimal solutions. We
propose a new algorithm called Causal Bayesian Optimization (CBO). CBO
automatically balances two trade-offs: the classical exploration-exploitation
and the new observation-intervention, which emerges when combining real
interventional data with the estimated intervention effects computed via
do-calculus. We demonstrate the practical benefits of this method in a
synthetic setting and in two real-world applications."
['stat.ML'],qDKT: Question-centric Deep Knowledge Tracing,"Knowledge tracing (KT) models, e.g., the deep knowledge tracing (DKT) model,
track an individual learner's acquisition of skills over time by examining the
learner's performance on questions related to those skills. A practical
limitation in most existing KT models is that all questions nested under a
particular skill are treated as equivalent observations of a learner's ability,
which is an inaccurate assumption in real-world educational scenarios. To
overcome this limitation we introduce qDKT, a variant of DKT that models every
learner's success probability on individual questions over time. First, qDKT
incorporates graph Laplacian regularization to smooth predictions under each
skill, which is particularly useful when the number of questions in the dataset
is big. Second, qDKT uses an initialization scheme inspired by the fastText
algorithm, which has found success in a variety of language modeling tasks. Our
experiments on several real-world datasets show that qDKT achieves state-of-art
performance on predicting learner outcomes. Because of this, qDKT can serve as
a simple, yet tough-to-beat, baseline for new question-centric KT models."
['stat.ML'],Structural Temporal Graph Neural Networks for Anomaly Detection in Dynamic Graphs,"Detecting anomalies in dynamic graphs is a vital task, with numerous
practical applications in areas such as security, finance, and social media.
Previous network embedding based methods have been mostly focusing on learning
good node representations, whereas largely ignoring the subgraph structural
changes related to the target nodes in dynamic graphs. In this paper, we
propose StrGNN, an end-to-end structural temporal Graph Neural Network model
for detecting anomalous edges in dynamic graphs. In particular, we first
extract the $h$-hop enclosing subgraph centered on the target edge and propose
the node labeling function to identify the role of each node in the subgraph.
Then, we leverage graph convolution operation and Sortpooling layer to extract
the fixed-size feature from each snapshot/timestamp. Based on the extracted
features, we utilize Gated recurrent units (GRUs) to capture the temporal
information for anomaly detection. Extensive experiments on six benchmark
datasets and a real enterprise security system demonstrate the effectiveness of
StrGNN."
['stat.ML'],Understanding Graph Isomorphism Network for rs-fMRI Functional Connectivity Analysis,"Graph neural networks (GNN) rely on graph operations that include neural
network training for various graph related tasks. Recently, several attempts
have been made to apply the GNNs to functional magnetic resonance image (fMRI)
data. Despite recent progresses, a common limitation is its difficulty to
explain the classification results in a neuroscientifically explainable way.
Here, we develop a framework for analyzing the fMRI data using the Graph
Isomorphism Network (GIN), which was recently proposed as a powerful GNN for
graph classification. One of the important contributions of this paper is the
observation that the GIN is a dual representation of convolutional neural
network (CNN) in the graph space where the shift operation is defined using the
adjacency matrix. This understanding enables us to exploit CNN-based saliency
map techniques for the GNN, which we tailor to the proposed GIN with one-hot
encoding, to visualize the important regions of the brain. We validate our
proposed framework using large-scale resting-state fMRI (rs-fMRI) data for
classifying the sex of the subject based on the graph structure of the brain.
The experiment was consistent with our expectation such that the obtained
saliency map show high correspondence with previous neuroimaging evidences
related to sex differences."
['stat.ML'],Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks,"Modeling multivariate time series has long been a subject that has attracted
researchers from a diverse range of fields including economics, finance, and
traffic. A basic assumption behind multivariate time series forecasting is that
its variables depend on one another but, upon looking closely, it is fair to
say that existing methods fail to fully exploit latent spatial dependencies
between pairs of variables. In recent years, meanwhile, graph neural networks
(GNNs) have shown high capability in handling relational dependencies. GNNs
require well-defined graph structures for information propagation which means
they cannot be applied directly for multivariate time series where the
dependencies are not known in advance. In this paper, we propose a general
graph neural network framework designed specifically for multivariate time
series data. Our approach automatically extracts the uni-directed relations
among variables through a graph learning module, into which external knowledge
like variable attributes can be easily integrated. A novel mix-hop propagation
layer and a dilated inception layer are further proposed to capture the spatial
and temporal dependencies within the time series. The graph learning, graph
convolution, and temporal convolution modules are jointly learned in an
end-to-end framework. Experimental results show that our proposed model
outperforms the state-of-the-art baseline methods on 3 of 4 benchmark datasets
and achieves on-par performance with other approaches on two traffic datasets
which provide extra structural information."
['stat.ML'],Adversarial Attack on Hierarchical Graph Pooling Neural Networks,"Recent years have witnessed the emergence and development of graph neural
networks (GNNs), which have been shown as a powerful approach for graph
representation learning in many tasks, such as node classification and graph
classification. The research on the robustness of these models has also started
to attract attentions in the machine learning field. However, most of the
existing work in this area focus on the GNNs for node-level tasks, while little
work has been done to study the robustness of the GNNs for the graph
classification task. In this paper, we aim to explore the vulnerability of the
Hierarchical Graph Pooling (HGP) Neural Networks, which are advanced GNNs that
perform very well in the graph classification in terms of prediction accuracy.
We propose an adversarial attack framework for this task. Specifically, we
design a surrogate model that consists of convolutional and pooling operators
to generate adversarial samples to fool the hierarchical GNN-based graph
classification models. We set the preserved nodes by the pooling operator as
our attack targets, and then we perturb the attack targets slightly to fool the
pooling operator in hierarchical GNNs so that they will select the wrong nodes
to preserve. We show the adversarial samples generated from multiple datasets
by our surrogate model have enough transferability to attack current
state-of-art graph classification models. Furthermore, we conduct the robust
train on the target models and demonstrate that the retrained graph
classification models are able to better defend against the attack from the
adversarial samples. To the best of our knowledge, this is the first work on
the adversarial attack against hierarchical GNN-based graph classification
models."
['stat.ML'],Non-IID Graph Neural Networks,"Graph classification is an important task on graph-structured data with many
real-world applications. The goal of graph classification task is to train a
classifier using a set of training graphs. Recently, Graph Neural Networks
(GNNs) have greatly advanced the task of graph classification. When building a
GNN model for graph classification, the graphs in the training set are usually
assumed to be identically distributed. However, in many real-world
applications, graphs in the same dataset could have dramatically different
structures, which indicates that these graphs are likely non-identically
distributed. Therefore, in this paper, we aim to develop graph neural networks
for graphs that are not non-identically distributed. Specifically, we propose a
general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a
graph, Non-IID-GNN can adapt any existing graph neural network model to
generate a sample-specific model for this graph. Comprehensive experiments on
various graph classification benchmarks demonstrate the effectiveness of the
proposed framework. We will release the code of the proposed framework upon the
acceptance of the paper."
['stat.ML'],Deterministic Completion of Rectangular Matrices Using Asymmetric Ramanujan Graphs: Exact and Stable Recovery,"In this paper we study the matrix completion problem: Suppose $X \in {\mathbb
R}^{n_r \times n_c}$ is unknown except for a known upper bound $r$ on its rank.
By measuring a small number $m \ll n_r n_c$ of elements of $X$, is it possible
to recover $X$ exactly with noise-free measurements, or to construct a good
approximation of $X$ with noisy measurements? Existing solutions to these
problems involve sampling the elements uniformly and at random, and can
guarantee exact recovery of the unknown matrix only with high probability. In
this paper, we present a \textit{deterministic} sampling method for matrix
completion. We achieve this by choosing the sampling set as the edge set of an
asymmetric Ramanujan bigraph, and constrained nuclear norm minimization is the
recovery method. Specifically, we derive sufficient conditions under which the
unknown matrix is completed exactly with noise-free measurements, and is
approximately completed with noisy measurements, which we call ""stable""
completion.
  The conditions derived here are only sufficient and more restrictive than
random sampling. To study how close they are to being necessary, we conducted
numerical simulations on randomly generated low rank matrices, using the LPS
families of Ramanujan graphs. These simulations demonstrate two facts: (i) In
order to achieve exact completion, it appears sufficient to choose the degree
$d$ of the Ramanujan graph to be $\geq 3r$. (ii) There is a ""phase transition,""
whereby the likelihood of success suddenly drops from 100\% to 0\% if the rank
is increased by just one or two beyond a critical value. The phase transition
phenomenon is well-known and well-studied in vector recovery using
$\ell_1$-norm minimization. However, it is less studied in matrix completion
and nuclear norm minimization, and not much understood."
['stat.ML'],DisCoveR: Accurate & Efficient Discovery of Declarative Process Models,"Declarative process modeling formalisms - which capture high-level process
constraints - have seen growing interest, especially for modeling flexible
processes. This paper presents DisCoveR, an extremely efficient and accurate
declarative miner for learning Dynamic Condition Response (DCR) Graphs from
event logs. We precisely formalize the algorithm, describe a highly efficient
bit vector implementation and rigorously evaluate performance against two other
declarative miners, representing the state-of-the-art in Declare and DCR Graphs
mining. DisCoveR outperforms each of these w.r.t. a binary classification task,
achieving an average accuracy of 96.2% in the Process Discovery Contest 2019.
Due to its linear time complexity, DisCoveR also achieves run-times 1-2 orders
of magnitude below its declarative counterparts. Finally, we show how the miner
has been integrated in a state-of-the-art declarative process modeling
framework as a model recommendation tool, discuss how discovery can play an
integral part of the modeling task and report on how the integration has
improved the modeling experience of end-users."
['stat.ML'],The Effects of Randomness on the Stability of Node Embeddings,"We systematically evaluate the (in-)stability of state-of-the-art node
embedding algorithms due to randomness, i.e., the random variation of their
outcomes given identical algorithms and graphs. We apply five node embeddings
algorithms---HOPE, LINE, node2vec, SDNE, and GraphSAGE---to synthetic and
empirical graphs and assess their stability under randomness with respect to
(i) the geometry of embedding spaces as well as (ii) their performance in
downstream tasks. We find significant instabilities in the geometry of
embedding spaces independent of the centrality of a node. In the evaluation of
downstream tasks, we find that the accuracy of node classification seems to be
unaffected by random seeding while the actual classification of nodes can vary
significantly. This suggests that instability effects need to be taken into
account when working with node embeddings. Our work is relevant for researchers
and engineers interested in the effectiveness, reliability, and reproducibility
of node embedding approaches."
['stat.ML'],Best Arm Identification in Spectral Bandits,"We study best-arm identification with fixed confidence in bandit models with
graph smoothness constraint. We provide and analyze an efficient gradient
ascent algorithm to compute the sample complexity of this problem as a solution
of a non-smooth max-min problem (providing in passing a simplified analysis for
the unconstrained case). Building on this algorithm, we propose an
asymptotically optimal strategy. We furthermore illustrate by numerical
experiments both the strategy's efficiency and the impact of the smoothness
constraint on the sample complexity. Best Arm Identification (BAI) is an
important challenge in many applications ranging from parameter tuning to
clinical trials. It is now very well understood in vanilla bandit models, but
real-world problems typically involve some dependency between arms that
requires more involved models. Assuming a graph structure on the arms is an
elegant practical way to encompass this phenomenon, but this had been done so
far only for regret minimization. Addressing BAI with graph constraints
involves delicate optimization problems for which the present paper offers a
solution."
['stat.ML'],Spatial-Temporal Graph Convolutional Networks for Sign Language Recognition,"The recognition of sign language is a challenging task with an important role
in society to facilitate the communication of deaf persons. We propose a new
approach of Spatial-Temporal Graph Convolutional Network to sign language
recognition based on the human skeletal movements. The method uses graphs to
capture the signs dynamics in two dimensions, spatial and temporal, considering
the complex aspects of the language. Additionally, we present a new dataset of
human skeletons for sign language based on ASLLVD to contribute to future
related studies."
['stat.ML'],A Survey of Adversarial Learning on Graphs,"Deep learning models on graphs have achieved remarkable performance in
various graph analysis tasks, e.g., node classification, link prediction and
graph clustering. However, they expose uncertainty and unreliability against
the well-designed inputs, i.e., adversarial examples. Accordingly, a line of
studies have emerged for both attack and defense addressed in different graph
analysis tasks, leading to the arms race in graph adversarial learning.
  Despite the booming works, there still lacks a unified problem definition and
a comprehensive review. To bridge this gap, we investigate and summarize the
existing works on graph adversarial learning tasks systemically. Specifically,
we survey and unify the existing works w.r.t. attack and defense in graph
analysis tasks, and give appropriate definitions and taxonomies at the same
time. Besides, we emphasize the importance of related evaluation metrics,
investigate and summarize them comprehensively. Hopefully, our works can
provide a comprehensive overview and offer insights for the relevant
researchers. More details of our works are available at
https://github.com/gitgiter/Graph-Adversarial-Learning."
['stat.ML'],HighwayGraph: Modelling Long-distance Node Relations for Improving General Graph Neural Network,"Graph Neural Networks (GNNs) are efficient approaches to process
graph-structured data. Modelling long-distance node relations is essential for
GNN training and applications. However, conventional GNNs suffer from bad
performance in modelling long-distance node relations due to limited-layer
information propagation. Existing studies focus on building deep GNN
architectures, which face the over-smoothing issue and cannot model node
relations in particularly long distance. To address this issue, we propose to
model long-distance node relations by simply relying on shallow GNN
architectures with two solutions: (1) Implicitly modelling by learning to
predict node pair relations (2) Explicitly modelling by adding edges between
nodes that potentially have the same label. To combine our two solutions, we
propose a model-agnostic training framework named HighwayGraph, which overcomes
the challenge of insufficient labeled nodes by sampling node pairs from the
training set and adopting the self-training method. Extensive experimental
results show that our HighwayGraph achieves consistent and significant
improvements over four representative GNNs on three benchmark datasets."
['stat.ML'],Machine Learning for Exploring Spatial Affordance Patterns,"This dissertation uses supervised and unsupervised data mining techniques to
analyse office floor plans in an attempt to gain a better understanding of
their geometry-to-function relationship. This question was deemed relevant
after a background review of the state-of-the-art in automated floor-plan
generation tools showed that such tools have been prototyped since the 1960s,
but their search space is ill-informed because there are few formalisms to
describe spatial affordance. To show and evaluate the relationship of geometry
and use, data from visual graph analysis were used to train three supervised
learners and compare these to a baseline accuracy established with a ZeroR
classifier. This showed that for the office dataset examined, visual mean depth
and integration are most tightly linked to usage and that the supervised
learning algorithm J48 can correctly predict class performance on unseen
examples to up to 79.5%. The thesis also includes an evaluation of the layout
case studies with unsupervised learners, which showed that use could not be
immediately reverse-engineered based solemnly on the VGA information to achieve
a strong cluster-to-class evaluation."
['stat.ML'],Graph Neural Networks with Composite Kernels,"Learning on graph structured data has drawn increasing interest in recent
years. Frameworks like Graph Convolutional Networks (GCNs) have demonstrated
their ability to capture structural information and obtain good performance in
various tasks. In these frameworks, node aggregation schemes are typically used
to capture structural information: a node's feature vector is recursively
computed by aggregating features of its neighboring nodes. However, most of
aggregation schemes treat all connections in a graph equally, ignoring node
feature similarities. In this paper, we re-interpret node aggregation from the
perspective of kernel weighting, and present a framework to consider feature
similarity in an aggregation scheme. Specifically, we show that normalized
adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein
Space. We then propose feature aggregation as the composition of the original
neighbor-based kernel and a learnable kernel to encode feature similarities in
a feature space. We further show how the proposed method can be extended to
Graph Attention Network (GAT). Experimental results demonstrate better
performance of our proposed framework in several real-world applications."
['stat.ML'],Modeling Pharmacological Effects with Multi-Relation Unsupervised Graph Embedding,"A pharmacological effect of a drug on cells, organs and systems refers to the
specific biochemical interaction produced by a drug substance, which is called
its mechanism of action. Drug repositioning (or drug repurposing) is a
fundamental problem for the identification of new opportunities for the use of
already approved or failed drugs. In this paper, we present a method based on a
multi-relation unsupervised graph embedding model that learns latent
representations for drugs and diseases so that the distance between these
representations reveals repositioning opportunities. Once representations for
drugs and diseases are obtained we learn the likelihood of new links (that is,
new indications) between drugs and diseases. Known drug indications are used
for learning a model that predicts potential indications. Compared with
existing unsupervised graph embedding methods our method shows superior
prediction performance in terms of area under the ROC curve, and we present
examples of repositioning opportunities found on recent biomedical literature
that were also predicted by our method."
['stat.ML'],Optimal Laplacian regularization for sparse spectral community detection,"Regularization of the classical Laplacian matrices was empirically shown to
improve spectral clustering in sparse networks. It was observed that small
regularizations are preferable, but this point was left as a heuristic
argument. In this paper we formally determine a proper regularization which is
intimately related to alternative state-of-the-art spectral techniques for
sparse graphs."
"['stat.ML', 'stat.ME']",Independence Testing for Multivariate Time Series,"Complex data structures such as time series are increasingly present in
modern data science problems. A fundamental question is whether two such
time-series are statistically dependent. Many current approaches make
parametric assumptions on the random processes, only detect linear association,
require multiple tests, or forfeit power in high-dimensional, nonlinear
settings. Estimating the distribution of any test statistic under the null is
non-trivial, as the permutation test is invalid. This work juxtaposes distance
correlation (Dcorr) and multiscale graph correlation (MGC) from independence
testing literature and block permutation from time series analysis to address
these challenges. The proposed nonparametric procedure is valid and consistent,
building upon prior work by characterizing the geometry of the relationship,
estimating the time lag at which dependence is maximized, avoiding the need for
multiple testing, and exhibiting superior power in high-dimensional, low sample
size, nonlinear settings. Neural connectivity is analyzed via fMRI data,
revealing linear dependence of signals within the visual network and default
mode network, and nonlinear relationships in other networks. This work uncovers
a first-resort data analysis tool with open-source code available, directly
impacting a wide range of scientific disciplines."
['stat.ME'],"Representations, Metrics and Statistics For Shape Analysis of Elastic Graphs","Past approaches for statistical shape analysis of objects have focused mainly
on objects within the same topological classes, e.g., scalar functions,
Euclidean curves, or surfaces, etc. For objects that differ in more complex
ways, the current literature offers only topological methods. This paper
introduces a far-reaching geometric approach for analyzing shapes of graphical
objects, such as road networks, blood vessels, brain fiber tracts, etc. It
represents such objects, exhibiting differences in both geometries and
topologies, as graphs made of curves with arbitrary shapes (edges) and
connected at arbitrary junctions (nodes). To perform statistical analyses, one
needs mathematical representations, metrics and other geometrical tools, such
as geodesics, means, and covariances. This paper utilizes a quotient structure
to develop efficient algorithms for computing these quantities, leading to
useful statistical tools, including principal component analysis and analytical
statistical testing and modeling of graphical shapes. The efficacy of this
framework is demonstrated using various simulated as well as the real data from
neurons and brain arterial networks."
['stat.ML'],Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective,"Knowledge graph is a popular format for representing knowledge, with many
applications to semantic search engines, question-answering systems, and
recommender systems. Real-world knowledge graphs are usually incomplete, so
knowledge graph embedding methods, such as Canonical decomposition/Parallel
factorization (CP), DistMult, and ComplEx, have been proposed to address this
issue. These methods represent entities and relations as embedding vectors in
semantic space and predict the links between them. The embedding vectors
themselves contain rich semantic information and can be used in other
applications such as data analysis. However, mechanisms in these models and the
embedding vectors themselves vary greatly, making it difficult to understand
and compare them. Given this lack of understanding, we risk using them
ineffectively or incorrectly, particularly for complicated models, such as CP,
with two role-based embedding vectors, or the state-of-the-art ComplEx model,
with complex-valued embedding vectors. In this paper, we propose a
multi-embedding interaction mechanism as a new approach to uniting and
generalizing these models. We derive them theoretically via this mechanism and
provide empirical analyses and comparisons between them. We also propose a new
multi-embedding model based on quaternion algebra and show that it achieves
promising results using popular benchmarks. Source code is available on github
at https://github.com/tranhungnghiep/AnalyzingKGEmbeddings"
['stat.ML'],Simultaneous imputation and disease classification in incomplete medical datasets using Multigraph Geometric Matrix Completion (MGMC),"Large-scale population-based studies in medicine are a key resource towards
better diagnosis, monitoring, and treatment of diseases. They also serve as
enablers of clinical decision support systems, in particular Computer Aided
Diagnosis (CADx) using machine learning (ML). Numerous ML approaches for CADx
have been proposed in literature. However, these approaches assume full data
availability, which is not always feasible in clinical data. To account for
missing data, incomplete data samples are either removed or imputed, which
could lead to data bias and may negatively affect classification performance.
As a solution, we propose an end-to-end learning of imputation and disease
prediction of incomplete medical datasets via Multigraph Geometric Matrix
Completion (MGMC). MGMC uses multiple recurrent graph convolutional networks,
where each graph represents an independent population model based on a key
clinical meta-feature like age, sex, or cognitive function. Graph signal
aggregation from local patient neighborhoods, combined with multigraph signal
fusion via self-attention, has a regularizing effect on both matrix
reconstruction and classification performance. Our proposed approach is able to
impute class relevant features as well as perform accurate classification on
two publicly available medical datasets. We empirically show the superiority of
our proposed approach in terms of classification and imputation performance
when compared with state-of-the-art approaches. MGMC enables disease prediction
in multimodal and incomplete medical datasets. These findings could serve as
baseline for future CADx approaches which utilize incomplete datasets."
['stat.ML'],ODVICE: An Ontology-Driven Visual Analytic Tool for Interactive Cohort Extraction,"Increased availability of electronic health records (EHR) has enabled
researchers to study various medical questions. Cohort selection for the
hypothesis under investigation is one of the main consideration for EHR
analysis. For uncommon diseases, cohorts extracted from EHRs contain very
limited number of records - hampering the robustness of any analysis. Data
augmentation methods have been successfully applied in other domains to address
this issue mainly using simulated records. In this paper, we present ODVICE, a
data augmentation framework that leverages the medical concept ontology to
systematically augment records using a novel ontologically guided Monte-Carlo
graph spanning algorithm. The tool allows end users to specify a small set of
interactive controls to control the augmentation process. We analyze the
importance of ODVICE by conducting studies on MIMIC-III dataset for two
learning tasks. Our results demonstrate the predictive performance of ODVICE
augmented cohorts, showing ~30% improvement in area under the curve (AUC) over
the non-augmented dataset and other data augmentation strategies."
['stat.ML'],DeepRobust: A PyTorch Library for Adversarial Attacks and Defenses,"DeepRobust is a PyTorch adversarial learning library which aims to build a
comprehensive and easy-to-use platform to foster this research field. It
currently contains more than 10 attack algorithms and 8 defense algorithms in
image domain and 9 attack algorithms and 4 defense algorithms in graph domain,
under a variety of deep learning architectures. In this manual, we introduce
the main contents of DeepRobust with detailed instructions. The library is kept
updated and can be found at https://github.com/DSE-MSU/DeepRobust."
['stat.ML'],Fundamental Limits of Deep Graph Convolutional Networks,"Graph convolutional networks (GCNs) are a widely used method for graph
representation learning. To elucidate the capabilities and limitations of GCNs,
we investigate their power, as a function of their number of layers, to
distinguish between different random graph models (corresponding to different
class-conditional distributions in a classification problem) on the basis of
the embeddings of their sample graphs. In particular, the graph models that we
consider arise from graphons, which are the most general possible
parameterizations of infinite exchangeable graph models and which are the
central objects of study in the theory of dense graph limits. We give a precise
characterization of the set of pairs of graphons that are indistinguishable by
a GCN with nonlinear activation functions coming from a certain broad class if
its depth is at least logarithmic in the size of the sample graph. This
characterization is in terms of a degree profile closeness property. Outside
this class, a very simple GCN architecture suffices for distinguishability. We
then exhibit a concrete, infinite class of graphons arising from stochastic
block models that are well-separated in terms of cut distance and are
indistinguishable by a GCN. These results theoretically match empirical
observations of several prior works. To prove our results, we exploit a
connection to random walks on graphs. Finally, we give empirical results on
synthetic and real graph classification datasets, indicating that
indistinguishable graph distributions arise in practice."
['stat.ML'],Energy-Aware DNN Graph Optimization,"Unlike existing work in deep neural network (DNN) graphs optimization for
inference performance, we explore DNN graph optimization for energy awareness
and savings for power- and resource-constrained machine learning devices. We
present a method that allows users to optimize energy consumption or balance
between energy and inference performance for DNN graphs. This method
efficiently searches through the space of equivalent graphs, and identifies a
graph and the corresponding algorithms that incur the least cost in execution.
We implement the method and evaluate it with multiple DNN models on a GPU-based
machine. Results show that our method achieves significant energy savings,
i.e., 24% with negligible performance impact."
['stat.ML'],Community Detection Clustering via Gumbel Softmax,"Recently, in many systems such as speech recognition and visual processing,
deep learning has been widely implemented. In this research, we are exploring
the possibility of using deep learning in community detection among the graph
datasets. Graphs have gained growing traction in different fields, including
social networks, information graphs, the recommender system, and also life
sciences. In this paper, we propose a method of community detection clustering
the nodes of various graph datasets. We cluster different category datasets
that belong to Affiliation networks, Animal networks, Human contact networks,
Human social networks, Miscellaneous networks. The deep learning role in
modeling the interaction between nodes in a network allows a revolution in the
field of science relevant to graph network analysis. In this paper, we extend
the gumbel softmax approach to graph network clustering. The experimental
findings on specific graph datasets reveal that the new approach outperforms
traditional clustering significantly, which strongly shows the efficacy of deep
learning in graph community detection clustering. We do a series of experiments
on our graph clustering algorithm, using various datasets: Zachary karate club,
Highland Tribe, Train bombing, American Revolution, Dolphins, Zebra,
Windsurfers, Les Mis\'erables, Political books."
['stat.ML'],Parallel Mapper,"The construction of Mapper has emerged in the last decade as a powerful and
effective topological data analysis tool that approximates and generalizes
other topological summaries, such as the Reeb graph, the contour tree, split,
and joint trees. In this paper, we study the parallel analysis of the
construction of Mapper. We give a provably correct parallel algorithm to
execute Mapper on multiple processors and discuss the performance results that
compare our approach to a reference sequential Mapper implementation. We report
the performance experiments that demonstrate the efficiency of our method."
['stat.ML'],Ring Reservoir Neural Networks for Graphs,"Machine Learning for graphs is nowadays a research topic of consolidated
relevance. Common approaches in the field typically resort to complex deep
neural network architectures and demanding training algorithms, highlighting
the need for more efficient solutions. The class of Reservoir Computing (RC)
models can play an important role in this context, enabling to develop fruitful
graph embeddings through untrained recursive architectures. In this paper, we
study progressive simplifications to the design strategy of RC neural networks
for graphs. Our core proposal is based on shaping the organization of the
hidden neurons to follow a ring topology. Experimental results on graph
classification tasks indicate that ring-reservoirs architectures enable
particularly effective network configurations, showing consistent advantages in
terms of predictive performance."
['stat.ML'],Graph Few-shot Learning via Knowledge Transfer,"Towards the challenging problem of semi-supervised node classification, there
have been extensive studies. As a frontier, Graph Neural Networks (GNNs) have
aroused great interest recently, which update the representation of each node
by aggregating information of its neighbors. However, most GNNs have shallow
layers with a limited receptive field and may not achieve satisfactory
performance especially when the number of labeled nodes is quite small. To
address this challenge, we innovatively propose a graph few-shot learning (GFL)
algorithm that incorporates prior knowledge learned from auxiliary graphs to
improve classification accuracy on the target graph. Specifically, a
transferable metric space characterized by a node embedding and a
graph-specific prototype embedding function is shared between auxiliary graphs
and the target, facilitating the transfer of structural knowledge. Extensive
experiments and ablation studies on four real-world graph datasets demonstrate
the effectiveness of our proposed model."
['stat.ML'],MCMC assisted by Belief Propagation,"Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows expressing the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes."
['stat.ML'],Adversarial Graph Embeddings for Fair Influence Maximization over Social Networks,"Influence maximization is a widely studied topic in network science, where
the aim is to reach the maximum possible number of nodes, while only targeting
a small initial set of individuals. It has critical applications in many
fields, including viral marketing, information propagation, news dissemination,
and vaccinations. However, the objective does not usually take into account
whether the final set of influenced nodes is fair with respect to sensitive
attributes, such as race or gender. Here we address fair influence
maximization, aiming to reach minorities more equitably. We introduce
Adversarial Graph Embeddings: we co-train an auto-encoder for graph embedding
and a discriminator to discern sensitive attributes. This leads to embeddings
which are similarly distributed across sensitive attributes. We then find a
good initial set by clustering the embeddings. We believe we are the first to
use embeddings for the task of fair influence maximization. While there are
typically trade-offs between fairness and influence maximization objectives,
our experiments on synthetic and real-world datasets show that our approach
dramatically reduces disparity while remaining competitive with
state-of-the-art influence maximization methods."
['stat.ML'],VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation,"Behavior prediction in dynamic, multi-agent systems is an important problem
in the context of self-driving cars, due to the complex representations and
interactions of road components, including moving agents (e.g. pedestrians and
vehicles) and road context information (e.g. lanes, traffic lights). This paper
introduces VectorNet, a hierarchical graph neural network that first exploits
the spatial locality of individual road components represented by vectors and
then models the high-order interactions among all components. In contrast to
most recent approaches, which render trajectories of moving agents and road
context information as bird-eye images and encode them with convolutional
neural networks (ConvNets), our approach operates on a vector representation.
By operating on the vectorized high definition (HD) maps and agent
trajectories, we avoid lossy rendering and computationally intensive ConvNet
encoding steps. To further boost VectorNet's capability in learning context
features, we propose a novel auxiliary task to recover the randomly masked out
map entities and agent trajectories based on their context. We evaluate
VectorNet on our in-house behavior prediction benchmark and the recently
released Argoverse forecasting dataset. Our method achieves on par or better
performance than the competitive rendering approach on both benchmarks while
saving over 70% of the model parameters with an order of magnitude reduction in
FLOPs. It also outperforms the state of the art on the Argoverse dataset."
['stat.ML'],Importance Filtered Cross-Domain Adaptation,"In Domain Adaptation (DA), the category-relevant losses usually occupy a
dominant position, while they are usually built with hard or soft labels in
existing models. We observed that hard labels are overconfident due to hard
samples existed, and soft labels are ambiguous as too many small noisy
probabilities involved, and both of them are easily to cause negative transfer.
Besides, the category-irrelevant losses in Closed-Set DA (CSDA) paradigm fail
to work in Open-Set DA (OSDA), and they also have to be in a category-relevant
form, since target data samples are split into shared and private classes. To
this end, we propose a newly-unified DA framework (i.e., Importance Filtered
Cross-Domain Adaptation, IFCDA). Firstly, an importance filtered mechanism is
devised to generate filtered soft labels to mitigate negative transfer
desirably. Specifically, the soft labels are divided into confident and
ambiguous ones. Then, only the maximum probability in each confident label is
retained, and a threshold value is set to truncate each ambiguous label so that
only prominent probabilities are reserved. Moreover, a general graph-based
label propagation is contrived to attain soft labels in both CSDA and OSDA,
where an extra component is embedded into label vector, so that it could detect
target novel classes. Finally, the category-relevant losses in both scenarios
are reformulated using filtered soft labels, while the category-irrelevant MMD
loss in CSDA is reformulated as a form like class-wise MMD using newly-designed
importance filtered soft labels. Notably, CSDA paradigm is a special case when
all extra components are set to 0, thus the proposed approach is geared to both
CSDA and OSDA. Comprehensive experiments on benchmark cross-domain object
recognition datasets verify that the proposed approach outperforms several
state-of-the-art methods in both scenarios."
['stat.ML'],Reinforcement Learning with Feedback Graphs,"We study episodic reinforcement learning in Markov decision processes when
the agent receives additional feedback per step in the form of several
transition observations. Such additional observations are available in a range
of tasks through extended sensors or prior knowledge about the environment
(e.g., when certain actions yield similar outcome). We formalize this setting
using a feedback graph over state-action pairs and show that model-based
algorithms can leverage the additional feedback for more sample-efficient
learning. We give a regret bound that, ignoring logarithmic factors and
lower-order terms, depends only on the size of the maximum acyclic subgraph of
the feedback graph, in contrast with a polynomial dependency on the number of
states and actions in the absence of a feedback graph. Finally, we highlight
challenges when leveraging a small dominating set of the feedback graph as
compared to the bandit setting and propose a new algorithm that can use
knowledge of such a dominating set for more sample-efficient learning of a
near-optimal policy."
['stat.ML'],Plan2Vec: Unsupervised Representation Learning by Latent Plans,"In this paper we introduce plan2vec, an unsupervised representation learning
approach that is inspired by reinforcement learning. Plan2vec constructs a
weighted graph on an image dataset using near-neighbor distances, and then
extrapolates this local metric to a global embedding by distilling
path-integral over planned path. When applied to control, plan2vec offers a way
to learn goal-conditioned value estimates that are accurate over long horizons
that is both compute and sample efficient. We demonstrate the effectiveness of
plan2vec on one simulated and two challenging real-world image datasets.
Experimental results show that plan2vec successfully amortizes the planning
cost, enabling reactive planning that is linear in memory and computation
complexity rather than exhaustive over the entire state space."
['stat.ML'],"Weighted Cheeger and Buser Inequalities, with Applications to Clustering and Cutting Probability Densities","In this paper, we show how sparse or isoperimetric cuts of a probability
density function relate to Cheeger cuts of its principal eigenfunction, for
appropriate definitions of `sparse cut' and `principal eigenfunction'.
  We construct these appropriate definitions of sparse cut and principal
eigenfunction in the probability density setting. Then, we prove Cheeger and
Buser type inequalities similar to those for the normalized graph Laplacian of
Alon-Milman. We demonstrate that no such inequalities hold for most prior
definitions of sparse cut and principal eigenfunction. We apply this result to
generate novel algorithms for cutting probability densities and clustering
data, including a principled variant of spectral clustering."
['stat.ML'],Graph Spectral Feature Learning for Mixed Data of Categorical and Numerical Type,"Feature learning in the presence of a mixed type of variables, numerical and
categorical types, is an important issue for related modeling problems. For
simple neighborhood queries under mixed data space, standard practice is to
consider numerical and categorical variables separately and combining them
based on some suitable distance functions. Alternatives, such as Kernel
learning or Principal Component do not explicitly consider the inter-dependence
structure among the mixed type of variables. In this work, we propose a novel
strategy to explicitly model the probabilistic dependence structure among the
mixed type of variables by an undirected graph. Spectral decomposition of the
graph Laplacian provides the desired feature transformation. The Eigen spectrum
of the transformed feature space shows increased separability and more
prominent clusterability among the observations. The main novelty of our paper
lies in capturing interactions of the mixed feature type in an unsupervised
framework using a graphical model. We numerically validate the implications of
the feature learning strategy"
['stat.ML'],Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,"The recent developments and growing interest in neural-symbolic models has
shown that hybrid approaches can offer richer models for Artificial
Intelligence. The integration of effective relational learning and reasoning
methods is one of the key challenges in this direction, as neural learning and
symbolic reasoning offer complementary characteristics that can benefit the
development of AI systems. Relational labelling or link prediction on knowledge
graphs has become one of the main problems in deep learning-based natural
language processing research. Moreover, other fields which make use of
neural-symbolic techniques may also benefit from such research endeavours.
There have been several efforts towards the identification of missing facts
from existing ones in knowledge graphs. Two lines of research try and predict
knowledge relations between two entities by considering all known facts
connecting them or several paths of facts connecting them. We propose a
neural-symbolic graph neural network which applies learning over all the paths
by feeding the model with the embedding of the minimal subset of the knowledge
graph containing such paths. By learning to produce representations for
entities and facts corresponding to word embeddings, we show how the model can
be trained end-to-end to decode these representations and infer relations
between entities in a multitask approach. Our contribution is two-fold: a
neural-symbolic methodology leverages the resolution of relational inference in
large graphs, and we also demonstrate that such neural-symbolic model is shown
more effective than path-based approaches"
['stat.ML'],A Dynamical Mean-Field Theory for Learning in Restricted Boltzmann Machines,"We define a message-passing algorithm for computing magnetizations in
Restricted Boltzmann machines, which are Ising models on bipartite graphs
introduced as neural network models for probability distributions over spin
configurations. To model nontrivial statistical dependencies between the spins'
couplings, we assume that the rectangular coupling matrix is drawn from an
arbitrary bi-rotation invariant random matrix ensemble. Using the dynamical
functional method of statistical mechanics we exactly analyze the dynamics of
the algorithm in the large system limit. We prove the global convergence of the
algorithm under a stability criterion and compute asymptotic convergence rates
showing excellent agreement with numerical simulations."
['stat.ML'],Improving Attention Mechanism in Graph Neural Networks via Cardinality Preservation,"Graph Neural Networks (GNNs) are powerful to learn the representation of
graph-structured data. Most of the GNNs use the message-passing scheme, where
the embedding of a node is iteratively updated by aggregating the information
of its neighbors. To achieve a better expressive capability of node influences,
attention mechanism has grown to be popular to assign trainable weights to the
nodes in aggregation. Though the attention-based GNNs have achieved remarkable
results in various tasks, a clear understanding of their discriminative
capacities is missing. In this work, we present a theoretical analysis of the
representational properties of the GNN that adopts the attention mechanism as
an aggregator. Our analysis determines all cases when those attention-based
GNNs can always fail to distinguish certain distinct structures. Those cases
appear due to the ignorance of cardinality information in attention-based
aggregation. To improve the performance of attention-based GNNs, we propose
cardinality preserved attention (CPA) models that can be applied to any kind of
attention mechanisms. Our experiments on node and graph classification confirm
our theoretical analysis and show the competitive performance of our CPA
models."
['stat.ML'],On Learning Combinatorial Patterns to Assist Large-Scale Airline Crew Pairing Optimization,"Airline Crew Pairing Optimization (CPO) aims at generating a set of legal
flight sequences (crew pairings), to cover an airline's flight schedule, at
minimum cost. It is usually performed using Column Generation (CG), a
mathematical programming technique for guided search-space exploration. CG
exploits the interdependencies between the current and the preceding
CG-iteration for generating new variables (pairings) during the
optimization-search. However, with the unprecedented scale and complexity of
the emergent flight networks, it has become imperative to learn higher-order
interdependencies among the flight-connection graphs, and utilize those to
enhance the efficacy of the CPO. In first of its kind and what marks a
significant departure from the state-of-the-art, this paper proposes a novel
adaptation of the Variational Graph Auto-Encoder for learning plausible
combinatorial patterns among the flight-connection data obtained through the
search-space exploration by an Airline Crew Pairing Optimizer, AirCROP
(developed by the authors and validated by the research consortium's industrial
sponsor, GE Aviation). The resulting flight-connection predictions are combined
on-the-fly using a novel heuristic to generate new pairings for the optimizer.
The utility of the proposed approach is demonstrated on large-scale (over 4200
flights), real-world, complex flight-networks of US-based airlines,
characterized by multiple hub-and-spoke subnetworks and several crew bases."
['stat.ML'],Stochastic Neighbor Embedding of Multimodal Relational Data for Image-Text Simultaneous Visualization,"Multimodal relational data analysis has become of increasing importance in
recent years, for exploring across different domains of data, such as images
and their text tags obtained from social networking services (e.g., Flickr). A
variety of data analysis methods have been developed for visualization; to give
an example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional
feature vectors so that their similarities keep those of the observed data
vectors. However, t-SNE is designed only for a single domain of data but not
for multimodal data; this paper aims at visualizing multimodal relational data
consisting of data vectors in multiple domains with relations across these
vectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic
Neighbor Embedding (MR-SNE), that (1) first computes augmented relations, where
we observe the relations across domains and compute those within each of
domains via the observed data vectors, and (2) jointly embeds the augmented
relations to a low-dimensional space. Through visualization of Flickr and
Animal with Attributes 2 datasets, proposed MR-SNE is compared with other graph
embedding-based approaches; MR-SNE demonstrates the promising performance."
['stat.ML'],Low-Dimensional Hyperbolic Knowledge Graph Embeddings,"Knowledge graph (KG) embeddings learn low-dimensional representations of
entities and relations to predict missing facts. KGs often exhibit hierarchical
and logical patterns which must be preserved in the embedding space. For
hierarchical data, hyperbolic embedding methods have shown promise for
high-fidelity and parsimonious representations. However, existing hyperbolic
embedding methods do not account for the rich logical patterns in KGs. In this
work, we introduce a class of hyperbolic KG embedding models that
simultaneously capture hierarchical and logical patterns. Our approach combines
hyperbolic reflections and rotations with attention to model complex relational
patterns. Experimental results on standard KG benchmarks show that our method
improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in
mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that
different geometric transformations capture different types of relations while
attention-based transformations generalize to multiple relations. In high
dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR
and 57.7% on YAGO3-10."
['stat.ML'],Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control,"In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning
framework which can infer the dynamics of a physical system, given by an
ordinary differential equation (ODE), from observed state trajectories. To
achieve better generalization with fewer training samples, SymODEN incorporates
appropriate inductive bias by designing the associated computation graph in a
physics-informed manner. In particular, we enforce Hamiltonian dynamics with
control to learn the underlying dynamics in a transparent way, which can then
be leveraged to draw insight about relevant physical aspects of the system,
such as mass and potential energy. In addition, we propose a parametrization
which can enforce this Hamiltonian formalism even when the generalized
coordinate data is embedded in a high-dimensional space or we can only access
velocity data instead of generalized momentum. This framework, by offering
interpretable, physically-consistent models for physical systems, opens up new
possibilities for synthesizing model-based control strategies."
['stat.ML'],Dissipative SymODEN: Encoding Hamiltonian Dynamics with Dissipation and Control into Deep Learning,"In this work, we introduce Dissipative SymODEN, a deep learning architecture
which can infer the dynamics of a physical system with dissipation from
observed state trajectories. To improve prediction accuracy while reducing
network size, Dissipative SymODEN encodes the port-Hamiltonian dynamics with
energy dissipation and external input into the design of its computation graph
and learns the dynamics in a structured way. The learned model, by revealing
key aspects of the system, such as the inertia, dissipation, and potential
energy, paves the way for energy-based controllers."
['stat.ML'],Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships,"Embodied artificial intelligence (AI) tasks shift from tasks focusing on
internet images to active settings involving embodied agents that perceive and
act within 3D environments. In this paper, we investigate the target-driven
visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes,
whose navigation task aims to train an agent that can intelligently make a
series of decisions to arrive at a pre-specified target location from any
possible starting positions only based on egocentric views. However, most
navigation methods currently struggle against several challenging problems,
such as data efficiency, automatic obstacle avoidance, and generalization.
Generalization problem means that agent does not have the ability to transfer
navigation skills learned from previous experience to unseen targets and
scenes. To address these issues, we incorporate two designs into classic DRL
framework: attention on 3D knowledge graph (KG) and target skill extension
(TSE) module. On the one hand, our proposed method combines visual features and
3D spatial representations to learn navigation policy. On the other hand, TSE
module is used to generate sub-targets which allow agent to learn from
failures. Specifically, our 3D spatial relationships are encoded through
recently popular graph convolutional network (GCN). Considering the real world
settings, our work also considers open action and adds actionable targets into
conventional navigation situations. Those more difficult settings are applied
to test whether DRL agent really understand its task, navigating environment,
and can carry out reasoning. Our experiments, performed in the AI2-THOR, show
that our model outperforms the baselines in both SR and SPL metrics, and
improves generalization ability across targets and scenes."
['stat.ML'],Directed Graph Convolutional Network,"Graph Convolutional Networks (GCNs) have been widely used due to their
outstanding performance in processing graph-structured data. However, the
undirected graphs limit their application scope. In this paper, we extend
spectral-based graph convolution to directed graphs by using first- and
second-order proximity, which can not only retain the connection properties of
the directed graph, but also expand the receptive field of the convolution
operation. A new GCN model, called DGCN, is then designed to learn
representations on the directed graph, leveraging both the first- and
second-order proximity information. We empirically show the fact that GCNs
working only with DGCNs can encode more useful information from graph and help
achieve better performance when generalized to other models. Moreover,
extensive experiments on citation networks and co-purchase datasets demonstrate
the superiority of our model against the state-of-the-art methods."
['stat.ML'],The Immersion of Directed Multi-graphs in Embedding Fields. Generalisations,"The purpose of this paper is to outline a generalised model for representing
hybrids of relational-categorical, symbolic, perceptual-sensory and
perceptual-latent data, so as to embody, in the same architectural data layer,
representations for the input, output and latent tensors. This variety of
representation is currently used by various machine-learning models in computer
vision, NLP/NLU, reinforcement learning which allows for direct application of
cross-domain queries and functions. This is achieved by endowing a directed
Tensor-Typed Multi-Graph with at least some edge attributes which represent the
embeddings from various latent spaces, so as to define, construct and compute
new similarity and distance relationships between and across tensorial forms,
including visual, linguistic, auditory latent representations, thus stitching
the logical-categorical view of the observed universe to the
Bayesian/statistical view."
['stat.ML'],"Perturb More, Trap More: Understanding Behaviors of Graph Neural Networks","While graph neural networks (GNNs) have shown a great potential in various
tasks on graph, the lack of transparency has hindered understanding how GNNs
arrived at its predictions. Although few explainers for GNNs are explored, the
consideration of local fidelity, indicating how the model behaves around an
instance should be predicted, is neglected. In this paper, we first propose a
novel post-hoc framework based on local fidelity for any trained GNNs - TraP2,
which can generate a high-fidelity explanation. Considering that both relevant
graph structure and important features inside each node need to be highlighted,
a three-layer architecture in TraP2 is designed: i) interpretation domain are
defined by Translation layer in advance; ii) local predictive behavior of GNNs
being explained are probed and monitored by Perturbation layer, in which
multiple perturbations for graph structure and feature-level are conducted in
interpretation domain; iii) high faithful explanations are generated by fitting
the local decision boundary through Paraphrase layer. Finally, TraP2 is
evaluated on six benchmark datasets based on five desired attributions:
accuracy, fidelity, decisiveness, insight and inspiration, which achieves
$10.2\%$ higher explanation accuracy than the state-of-the-art methods."
['stat.ML'],Learning Compositional Koopman Operators for Model-Based Control,"Finding an embedding space for a linear approximation of a nonlinear
dynamical system enables efficient system identification and control synthesis.
The Koopman operator theory lays the foundation for identifying the
nonlinear-to-linear coordinate transformations with data-driven methods.
Recently, researchers have proposed to use deep neural networks as a more
expressive class of basis functions for calculating the Koopman operators.
These approaches, however, assume a fixed dimensional state space; they are
therefore not applicable to scenarios with a variable number of objects. In
this paper, we propose to learn compositional Koopman operators, using graph
neural networks to encode the state into object-centric embeddings and using a
block-wise linear transition matrix to regularize the shared structure across
objects. The learned dynamics can quickly adapt to new environments of unknown
physical parameters and produce control signals to achieve a specified goal.
Our experiments on manipulating ropes and controlling soft robots show that the
proposed method has better efficiency and generalization ability than existing
baselines."
['stat.ML'],Representation Learning for Dynamic Graphs: A Survey,"Graphs arise naturally in many real-world applications including social
networks, recommender systems, ontologies, biology, and computational finance.
Traditionally, machine learning models for graphs have been mostly designed for
static graphs. However, many applications involve evolving graphs. This
introduces important challenges for learning and inference since nodes,
attributes, and edges change over time. In this survey, we review the recent
advances in representation learning for dynamic graphs, including dynamic
knowledge graphs. We describe existing models from an encoder-decoder
perspective, categorize these encoders and decoders based on the techniques
they employ, and analyze the approaches in each category. We also review
several prominent applications and widely used datasets and highlight
directions for future research."
['stat.ML'],Data-Driven Construction of Data Center Graph of Things for Anomaly Detection,"Data center (DC) contains both IT devices and facility equipment, and the
operation of a DC requires a high-quality monitoring (anomaly detection)
system. There are lots of sensors in computer rooms for the DC monitoring
system, and they are inherently related. This work proposes a data-driven
pipeline (ts2graph) to build a DC graph of things (sensor graph) from the time
series measurements of sensors. The sensor graph is an undirected weighted
property graph, where sensors are the nodes, sensor features are the node
properties, and sensor connections are the edges. The sensor node property is
defined by features that characterize the sensor events (behaviors), instead of
the original time series. The sensor connection (edge weight) is defined by the
probability of concurrent events between two sensors. A graph of things
prototype is constructed from the sensor time series of a real data center, and
it successfully reveals meaningful relationships between the sensors. To
demonstrate the use of the DC sensor graph for anomaly detection, we compare
the performance of graph neural network (GNN) and existing standard methods on
synthetic anomaly data. GNN outperforms existing algorithms by a factor of 2 to
3 (in terms of precision and F1 score), because it takes into account the
topology relationship between DC sensors. We expect that the DC sensor graph
can serve as the infrastructure for the DC monitoring system since it
represents the sensor relationships."
['stat.ML'],Compressed Sensing Using Binary Matrices of Nearly Optimal Dimensions,"In this paper, we study the problem of compressed sensing using binary
measurement matrices and $\ell_1$-norm minimization (basis pursuit) as the
recovery algorithm. We derive new upper and lower bounds on the number of
measurements to achieve robust sparse recovery with binary matrices. We
establish sufficient conditions for a column-regular binary matrix to satisfy
the robust null space property (RNSP) and show that the associated sufficient
conditions % sparsity bounds for robust sparse recovery obtained using the RNSP
are better by a factor of $(3 \sqrt{3})/2 \approx 2.6$ compared to the
sufficient conditions obtained using the restricted isometry property (RIP).
Next we derive universal \textit{lower} bounds on the number of measurements
that any binary matrix needs to have in order to satisfy the weaker sufficient
condition based on the RNSP and show that bipartite graphs of girth six are
optimal. Then we display two classes of binary matrices, namely parity check
matrices of array codes and Euler squares, which have girth six and are nearly
optimal in the sense of almost satisfying the lower bound. In principle,
randomly generated Gaussian measurement matrices are ""order-optimal"". So we
compare the phase transition behavior of the basis pursuit formulation using
binary array codes and Gaussian matrices and show that (i) there is essentially
no difference between the phase transition boundaries in the two cases and (ii)
the CPU time of basis pursuit with binary matrices is hundreds of times faster
than with Gaussian matrices and the storage requirements are less. Therefore it
is suggested that binary matrices are a viable alternative to Gaussian matrices
for compressed sensing using basis pursuit. \end{abstract}"
['stat.ML'],A Deeper Look at the Unsupervised Learning of Disentangled Representations in $β$-VAE from the Perspective of Core Object Recognition,"The ability to recognize objects despite there being differences in
appearance, known as Core Object Recognition, forms a critical part of human
perception. While it is understood that the brain accomplishes Core Object
Recognition through feedforward, hierarchical computations through the visual
stream, the underlying algorithms that allow for invariant representations to
form downstream is still not well understood. (DiCarlo et al., 2012) Various
computational perceptual models have been built to attempt and tackle the
object identification task in an artificial perceptual setting. Artificial
Neural Networks, computational graphs consisting of weighted edges and
mathematical operations at vertices, are loosely inspired by neural networks in
the brain and have proven effective at various visual perceptual tasks,
including object characterization and identification. (Pinto et al., 2008)
(DiCarlo et al., 2012) For many data analysis tasks, learning representations
where each dimension is statistically independent and thus disentangled from
the others is useful. If the underlying generative factors of the data are also
statistically independent, Bayesian inference of latent variables can form
disentangled representations. This thesis constitutes a research project
exploring a generalization of the Variational Autoencoder (VAE), $\beta$-VAE,
that aims to learn disentangled representations using variational inference.
$\beta$-VAE incorporates the hyperparameter $\beta$, and enforces conditional
independence of its bottleneck neurons, which is in general not compatible with
the statistical independence of latent variables. This text examines this
architecture, and provides analytical and numerical arguments, with the goal of
demonstrating that this incompatibility leads to a non-monotonic inference
performance in $\beta$-VAE with a finite optimal $\beta$."
['stat.ML'],Sensor selection on graphs via data-driven node sub-sampling in network time series,"This paper is concerned by the problem of selecting an optimal sampling set
of sensors over a network of time series for the purpose of signal recovery at
non-observed sensors with a minimal reconstruction error. The problem is
motivated by applications where time-dependent graph signals are collected over
redundant networks. In this setting, one may wish to only use a subset of
sensors to predict data streams over the whole collection of nodes in the
underlying graph. A typical application is the possibility to reduce the power
consumption in a network of sensors that may have limited battery supplies. We
propose and compare various data-driven strategies to turn off a fixed number
of sensors or equivalently to select a sampling set of nodes. We also relate
our approach to the existing literature on sensor selection from multivariate
data with a (possibly) underlying graph structure. Our methodology combines
tools from multivariate time series analysis, graph signal processing,
statistical learning in high-dimension and deep learning. To illustrate the
performances of our approach, we report numerical experiments on the analysis
of real data from bike sharing networks in different cities."
['stat.ML'],Variational Graph Recurrent Neural Networks,"Representation learning over graph structured data has been mostly studied in
static graph settings while efforts for modeling dynamic graphs are still
scant. In this paper, we develop a novel hierarchical variational model that
introduces additional latent random variables to jointly model the hidden
states of a graph recurrent neural network (GRNN) to capture both topology and
node attribute changes in dynamic graphs. We argue that the use of high-level
latent random variables in this variational GRNN (VGRNN) can better capture
potential variability observed in dynamic graphs as well as the uncertainty of
node latent representation. With semi-implicit variational inference developed
for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian
latent representations can further help dynamic graph analytic tasks. Our
experiments with multiple real-world dynamic graph datasets demonstrate that
SI-VGRNN and VGRNN consistently outperform the existing baseline and
state-of-the-art methods by a significant margin in dynamic link prediction."
['stat.ML'],Semi-Implicit Graph Variational Auto-Encoders,"Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand
the flexibility of variational graph auto-encoders (VGAE) to model graph data.
SIG-VAE employs a hierarchical variational framework to enable neighboring node
sharing for better generative modeling of graph dependency structure, together
with a Bernoulli-Poisson link decoder. Not only does this hierarchical
construction provide a more flexible generative graph model to better capture
real-world graph properties, but also does SIG-VAE naturally lead to
semi-implicit hierarchical variational inference that allows faithful modeling
of implicit posteriors of given graph data, which may exhibit heavy tails,
multiple modes, skewness, and rich dependency structures. Compared to VGAE, the
derived graph latent representations by SIG-VAE are more interpretable, due to
more expressive generative model and more faithful inference enabled by the
flexible semi-implicit construction. Extensive experiments with a variety of
graph data show that SIG-VAE significantly outperforms state-of-the-art methods
on several different graph analytic tasks."
['stat.ML'],"Flow-based Algorithms for Improving Clusters: A Unifying Framework, Software, and Performance","Clustering points in a vector space or nodes in a graph is a ubiquitous
primitive in statistical data analysis, and it is commonly used for exploratory
data analysis. In practice, it is often of interest to ""refine"" or ""improve"" a
given cluster that has been obtained by some other method. In this survey, we
focus on principled algorithms for this cluster improvement problem. Many such
cluster improvement algorithms are flow-based methods, by which we mean that
operationally they require the solution of a sequence of maximum flow problems
on a (typically implicitly) modified data graph. These cluster improvement
algorithms are powerful, both in theory and in practice, but they have not been
widely adopted for problems such as community detection, local graph
clustering, semi-supervised learning, etc. Possible reasons for this are: the
steep learning curve for these algorithms; the lack of efficient and easy to
use software; and the lack of detailed numerical experiments on real-world data
that demonstrate their usefulness. Our objective here is to address these
issues. To do so, we guide the reader through the whole process of
understanding how to implement and apply these powerful algorithms. We present
a unifying fractional programming optimization framework that permits us to
distill out in a simple way the crucial components of all these algorithms. It
also makes apparent similarities and differences between related methods.
Viewing these cluster improvement algorithms via a fractional programming
framework suggests directions for future algorithm development. Finally, we
develop efficient implementations of these algorithms in our
LocalGraphClustering python package, and we perform extensive numerical
experiments to demonstrate the performance of these methods on social networks
and image-based data graphs."
"['stat.ML', 'stat.ME']",High-dimensional Gaussian graphical model for network-linked data,"Graphical models are commonly used to represent conditional dependence
relationships between variables. There are multiple methods available for
exploring them from high-dimensional data, but almost all of them rely on the
assumption that the observations are independent and identically distributed.
At the same time, observations connected by a network are becoming increasingly
common, and tend to violate these assumptions. Here we develop a Gaussian
graphical model for observations connected by a network with potentially
different mean vectors, varying smoothly over the network. We propose an
efficient estimation algorithm and demonstrate its effectiveness on both
simulated and real data, obtaining meaningful and interpretable results on a
statistics coauthorship network. We also prove that our method estimates both
the inverse covariance matrix and the corresponding graph structure correctly
under the assumption of network “cohesion”, which refers to the empirically
observed phenomenon of network neighbors sharing similar traits."
['stat.ML'],"Strong Consistency, Graph Laplacians, and the Stochastic Block Model","Spectral clustering has become one of the most popular algorithms in data
clustering and community detection. We study the performance of classical
two-step spectral clustering via the graph Laplacian to learn the stochastic
block model. Our aim is to answer the following question: when is spectral
clustering via the graph Laplacian able to achieve strong consistency, i.e.,
the exact recovery of the underlying hidden communities? Our work provides an
entrywise analysis (an $\ell_{\infty}$-norm perturbation bound) of the Fielder
eigenvector of both the unnormalized and the normalized Laplacian associated
with the adjacency matrix sampled from the stochastic block model. We prove
that spectral clustering is able to achieve exact recovery of the planted
community structure under conditions that match the information-theoretic
limits."
['stat.ML'],Graph-Partitioning-Based Diffusion Convolutional Recurrent Neural Network for Large-Scale Traffic Forecasting,"Traffic forecasting approaches are critical to developing adaptive strategies
for mobility. Traffic patterns have complex spatial and temporal dependencies
that make accurate forecasting on large highway networks a challenging task.
Recently, diffusion convolutional recurrent neural networks (DCRNNs) have
achieved state-of-the-art results in traffic forecasting by capturing the
spatiotemporal dynamics of the traffic. Despite the promising results, however,
applying DCRNNs for large highway networks still remains elusive because of
computational and memory bottlenecks. We present an approach for implementing a
DCRNN for a large highway network that overcomes these limitations. Our
approach uses a graph-partitioning method to decompose a large highway network
into smaller networks and trains them independently. We demonstrate the
efficacy of the graph-partitioning-based DCRNN approach to model the traffic on
a large California highway network with 11,160 sensor locations. We develop an
overlapping nodes approach for the graph-partitioning-based DCRNN to include
sensor locations from partitions that are geographically close to a given
partition. Furthermore, we demonstrate that the DCRNN model can be used to
forecast the speed and flow simultaneously and that the forecasted values
preserve fundamental traffic flow dynamics. Our approach to developing DCRNN
models that represent large highway networks can be a potential core capability
in advanced highway traffic monitoring systems, where a trained DCRNN model
forecasting traffic at all sensor locations can be used to adjust traffic
management strategies proactively based on anticipated future conditions."
['stat.ML'],Transfer Learning with Graph Neural Networks for Short-Term Highway Traffic Forecasting,"Highway traffic modeling and forecasting approaches are critical for
intelligent transportation systems. Recently, deep-learning-based traffic
forecasting methods have emerged as state of the art for a wide range of
traffic forecasting tasks. However, these methods require a large amount of
training data, which needs to be collected over a significant period of time.
This can present a number of challenges for the development and deployment of
data-driven learning methods for highway networks that suffer from lack of
historical data. A promising approach to address this issue is transfer
learning, where a model trained on one part of the highway network can be
adapted for a different part of the highway network. We focus on diffusion
convolutional recurrent neural network (DCRNN), a state-of-the-art graph neural
network for highway network forecasting. It models the complex spatial and
temporal dynamics of the highway network using a graph-based diffusion
convolution operation within a recurrent neural network. DCRNN cannot perform
transfer learning, however, because it learns location-specific traffic
patterns, which cannot be used for unseen regions of the network. To that end,
we develop a new transfer learning approach for DCRNN, where a single model
trained on data-rich regions of the highway network can be used to forecast
traffic on unseen regions of the highway network. We evaluate the ability of
our approach to forecast the traffic on the entire California highway network
with one year of time series data. We show that TL-DCRNN can learn from several
regions of the California highway network and forecast the traffic on the
unseen regions of the network with high accuracy. Moreover, we demonstrate that
TL-DCRNN can learn from San Francisco region traffic data and can forecast
traffic on the Los Angeles region and vice versa."
['stat.ML'],Binarized Graph Neural Network,"Recently, there have been some breakthroughs in graph analysis by applying
the graph neural networks (GNNs) following a neighborhood aggregation scheme,
which demonstrate outstanding performance in many tasks. However, we observe
that the parameters of the network and the embedding of nodes are represented
in real-valued matrices in existing GNN-based graph embedding approaches which
may limit the efficiency and scalability of these models. It is well-known that
binary vector is usually much more space and time efficient than the
real-valued vector. This motivates us to develop a binarized graph neural
network to learn the binary representations of the nodes with binary network
parameters following the GNN-based paradigm. Our proposed method can be
seamlessly integrated into the existing GNN-based embedding approaches to
binarize the model parameters and learn the compact embedding. Extensive
experiments indicate that the proposed binarized graph neural network, namely
BGN, is orders of magnitude more efficient in terms of both time and space
while matching the state-of-the-art performance."
['stat.ML'],Hierarchical Generation of Molecular Graphs using Structural Motifs,"Graph generation techniques are increasingly being adopted for drug
discovery. Previous graph generation approaches have utilized relatively small
molecular building blocks such as atoms or simple cycles, limiting their
effectiveness to smaller molecules. Indeed, as we demonstrate, their
performance degrades significantly for larger molecules. In this paper, we
propose a new hierarchical graph encoder-decoder that employs significantly
larger and more flexible graph motifs as basic building blocks. Our encoder
produces a multi-resolution representation for each molecule in a
fine-to-coarse fashion, from atoms to connected motifs. Each level integrates
the encoding of constituents below with the graph at that level. Our
autoregressive coarse-to-fine decoder adds one motif at a time, interleaving
the decision of selecting a new motif with the process of resolving its
attachments to the emerging molecule. We evaluate our model on multiple
molecule generation tasks, including polymers, and show that our model
significantly outperforms previous state-of-the-art baselines."
['stat.ML'],A Lagrangian Approach to Information Propagation in Graph Neural Networks,"In many real world applications, data are characterized by a complex
structure, that can be naturally encoded as a graph. In the last years, the
popularity of deep learning techniques has renewed the interest in neural
models able to process complex patterns. In particular, inspired by the Graph
Neural Network (GNN) model, different architectures have been proposed to
extend the original GNN scheme. GNNs exploit a set of state variables, each
assigned to a graph node, and a diffusion mechanism of the states among
neighbor nodes, to implement an iterative procedure to compute the fixed point
of the (learnable) state transition function. In this paper, we propose a novel
approach to the state computation and the learning algorithm for GNNs, based on
a constraint optimisation task solved in the Lagrangian framework. The state
convergence procedure is implicitly expressed by the constraint satisfaction
mechanism and does not require a separate iterative phase for each epoch of the
learning procedure. In fact, the computational structure is based on the search
for saddle points of the Lagrangian in the adjoint space composed of weights,
neural outputs (node states), and Lagrange multipliers. The proposed approach
is compared experimentally with other popular models for processing graphs."
['stat.ML'],Correlated Variational Auto-Encoders,"Variational Auto-Encoders (VAEs) are capable of learning latent
representations for high dimensional data. However, due to the i.i.d.
assumption, VAEs only optimize the singleton variational distributions and fail
to account for the correlations between data points, which might be crucial for
learning latent representations from dataset where a priori we know
correlations exist. We propose Correlated Variational Auto-Encoders (CVAEs)
that can take the correlation structure into consideration when learning latent
representations with VAEs. CVAEs apply a prior based on the correlation
structure. To address the intractability introduced by the correlated prior, we
develop an approximation by average of a set of tractable lower bounds over all
maximal acyclic subgraphs of the undirected correlation graph. Experimental
results on matching and link prediction on public benchmark rating datasets and
spectral clustering on a synthetic dataset show the effectiveness of the
proposed method over baseline algorithms."
['stat.ML'],"MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models","Dense, discrete Graphical Models with pairwise potentials are a powerful
class of models which are employed in state-of-the-art computer vision and
bio-imaging applications. This work introduces a new MAP-solver, based on the
popular Dual Block-Coordinate Ascent principle. Surprisingly, by making a small
change to the low-performing solver, the Max Product Linear Programming (MPLP)
algorithm, we derive the new solver MPLP++ that significantly outperforms all
existing solvers by a large margin, including the state-of-the-art solver
Tree-Reweighted Sequential (TRWS) message-passing algorithm. Additionally, our
solver is highly parallel, in contrast to TRWS, which gives a further boost in
performance with the proposed GPU and multi-thread CPU implementations. We
verify the superiority of our algorithm on dense problems from publicly
available benchmarks, as well, as a new benchmark for 6D Object Pose
estimation. We also provide an ablation study with respect to graph density."
['stat.ML'],Taxonomy of Dual Block-Coordinate Ascent Methods for Discrete Energy Minimization,"We consider the maximum-a-posteriori inference problem in discrete graphical
models and study solvers based on the dual block-coordinate ascent rule. We map
all existing solvers in a single framework, allowing for a better understanding
of their design principles. We theoretically show that some block-optimizing
updates are sub-optimal and how to strictly improve them. On a wide range of
problem instances of varying graph connectivity, we study the performance of
existing solvers as well as new variants that can be obtained within the
framework. As a result of this exploration we build a new state-of-the art
solver, performing uniformly better on the whole range of test instances."
['stat.ML'],Controlling Computation versus Quality for Neural Sequence Models,"Most neural networks utilize the same amount of compute for every example
independent of the inherent complexity of the input. Further, methods that
adapt the amount of computation to the example focus on finding a fixed
inference-time computational graph per example, ignoring any external
computational budgets or varying inference time limitations. In this work, we
utilize conditional computation to make neural sequence models (Transformer)
more efficient and computation-aware during inference. We first modify the
Transformer architecture, making each set of operations conditionally
executable depending on the output of a learned control network. We then train
this model in a multi-task setting, where each task corresponds to a particular
computation budget. This allows us to train a single model that can be
controlled to operate on different points of the computation-quality trade-off
curve, depending on the available computation budget at inference time. We
evaluate our approach on two tasks: (i) WMT English-French Translation and (ii)
Unsupervised representation learning (BERT). Our experiments demonstrate that
the proposed Conditional Computation Transformer (CCT) is competitive with
vanilla Transformers when allowed to utilize its full computational budget,
while improving significantly over computationally equivalent baselines when
operating on smaller computational budgets."
['stat.ML'],Hcore-Init: Neural Network Initialization based on Graph Degeneracy,"Neural networks are the pinnacle of Artificial Intelligence, as in recent
years we witnessed many novel architectures, learning and optimization
techniques for deep learning. Capitalizing on the fact that neural networks
inherently constitute multipartite graphs among neuron layers, we aim to
analyze directly their structure to extract meaningful information that can
improve the learning process. To our knowledge graph mining techniques for
enhancing learning in neural networks have not been thoroughly investigated. In
this paper we propose an adapted version of the k-core structure for the
complete weighted multipartite graph extracted from a deep learning
architecture. As a multipartite graph is a combination of bipartite graphs,
that are in turn the incidence graphs of hypergraphs, we design k-hypercore
decomposition, the hypergraph analogue of k-core degeneracy. We applied
k-hypercore to several neural network architectures, more specifically to
convolutional neural networks and multilayer perceptrons for image recognition
tasks after a very short pretraining. Then we used the information provided by
the hypercore numbers of the neurons to re-initialize the weights of the neural
network, thus biasing the gradient optimization scheme. Extensive experiments
proved that k-hypercore outperforms the state-of-the-art initialization
methods."
['stat.ML'],Identification Methods With Arbitrary Interventional Distributions as Inputs,"Causal inference quantifies cause-effect relationships by estimating
counterfactual parameters from data. This entails using \emph{identification
theory} to establish a link between counterfactual parameters of interest and
distributions from which data is available. A line of work characterized
non-parametric identification for a wide variety of causal parameters in terms
of the \emph{observed data distribution}. More recently, identification results
have been extended to settings where experimental data from interventional
distributions is also available. In this paper, we use Single World
Intervention Graphs and a nested factorization of models associated with mixed
graphs to give a very simple view of existing identification theory for
experimental data. We use this view to yield general identification algorithms
for settings where the input distributions consist of an arbitrary set of
observational and experimental distributions, including marginal and
conditional distributions. We show that for problems where inputs are
interventional marginal distributions of a certain type (ancestral marginals),
our algorithm is complete."
['stat.ML'],Mask Combination of Multi-layer Graphs for Global Structure Inference,"Structure inference is an important task for network data processing and
analysis in data science. In recent years, quite a few approaches have been
developed to learn the graph structure underlying a set of observations
captured in a data space. Although real-world data is often acquired in
settings where relationships are influenced by a priori known rules, such
domain knowledge is still not well exploited in structure inference problems.
In this paper, we identify the structure of signals defined in a data space
whose inner relationships are encoded by multi-layer graphs. We aim at properly
exploiting the information originating from each layer to infer the global
structure underlying the signals. We thus present a novel method for combining
the multiple graphs into a global graph using mask matrices, which are
estimated through an optimization problem that accommodates the multi-layer
graph information and a signal representation model. The proposed mask
combination method also estimates the contribution of each graph layer in the
structure of signals. The experiments conducted both on synthetic and
real-world data suggest that integrating the multi-layer graph representation
of the data in the structure inference framework enhances the learning
procedure considerably by adapting to the quality and the quantity of the input
data."
['stat.ML'],MxPool: Multiplex Pooling for Hierarchical Graph Representation Learning,"How to utilize deep learning methods for graph classification tasks has
attracted considerable research attention in the past few years. Regarding
graph classification tasks, the graphs to be classified may have various graph
sizes (i.e., different number of nodes and edges) and have various graph
properties (e.g., average node degree, diameter, and clustering coefficient).
The diverse property of graphs has imposed significant challenges on existing
graph learning techniques since diverse graphs have different best-fit
hyperparameters. It is difficult to learn graph features from a set of diverse
graphs by a unified graph neural network. This motivates us to use a multiplex
structure in a diverse way and utilize a priori properties of graphs to guide
the learning. In this paper, we propose MxPool, which concurrently uses
multiple graph convolution/pooling networks to build a hierarchical learning
structure for graph representation learning tasks. Our experiments on numerous
graph classification benchmarks show that our MxPool has superiority over other
state-of-the-art graph representation learning methods."
['stat.ML'],In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL,"In-database machine learning has been very popular, almost being a cliche.
However, can we do it the other way around? In this work, we say ""yes"" by
applying plain old SQL to deep learning, in a sense implementing deep learning
algorithms with SQL. Most deep learning frameworks, as well as generic machine
learning ones, share a de facto standard of multidimensional array operations,
underneath fancier infrastructure such as automatic differentiation. As SQL
tables can be regarded as generalisations of (multi-dimensional) arrays, we
have found a way to express common deep learning operations in SQL, encouraging
a different way of thinking and thus potentially novel models. In particular,
one of the latest trend in deep learning was the introduction of sparsity in
the name of graph convolutional networks, whereas we take sparsity almost for
granted in the database world. As both databases and machine learning involve
transformation of datasets, we hope this work can inspire further works
utilizing the large body of existing wisdom, algorithms and technologies in the
database field to advance the state of the art in machine learning, rather than
merely integerating machine learning into databases."
"['stat.ML', 'stat.CO']","Vertex Nomination, Consistent Estimation, and Adversarial Modification","Given a pair of graphs $G_1$ and $G_2$ and a vertex set of interest in $G_1$,
the vertex nomination (VN) problem seeks to find the corresponding vertices of
interest in $G_2$ (if they exist) and produce a rank list of the vertices in
$G_2$, with the corresponding vertices of interest in $G_2$ concentrating,
ideally, at the top of the rank list. In this paper, we define and derive the
analogue of Bayes optimality for VN with multiple vertices of interest, and we
define the notion of maximal consistency classes in vertex nomination. This
theory forms the foundation for a novel VN adversarial contamination model, and
we demonstrate with real and simulated data that there are VN schemes that
perform effectively in the uncontaminated setting, and adversarial network
contamination adversely impacts the performance of our VN scheme. We further
define a network regularization method for mitigating the impact of the
adversarial contamination, and we demonstrate the effectiveness of
regularization in both real and synthetic data."
['stat.ML'],Gumbel-softmax-based Optimization: A Simple General Framework for Optimization Problems on Graphs,"In computer science, there exist a large number of optimization problems
defined on graphs, that is to find a best node state configuration or a network
structure such that the designed objective function is optimized under some
constraints. However, these problems are notorious for their hardness to solve
because most of them are NP-hard or NP-complete. Although traditional general
methods such as simulated annealing (SA), genetic algorithms (GA) and so forth
have been devised to these hard problems, their accuracy and time consumption
are not satisfying in practice. In this work, we proposed a simple, fast, and
general algorithm framework based on advanced automatic differentiation
technique empowered by deep learning frameworks. By introducing Gumbel-softmax
technique, we can optimize the objective function directly by gradient descent
algorithm regardless of the discrete nature of variables. We also introduce
evolution strategy to parallel version of our algorithm. We test our algorithm
on three representative optimization problems on graph including modularity
optimization from network science, Sherrington-Kirkpatrick (SK) model from
statistical physics, maximum independent set (MIS) and minimum vertex cover
(MVC) problem from combinatorial optimization on graph. High-quality solutions
can be obtained with much less time consuming compared to traditional
approaches."
['stat.ML'],Contrastive Examples for Addressing the Tyranny of the Majority,"Computer vision algorithms, e.g. for face recognition, favour groups of
individuals that are better represented in the training data. This happens
because of the generalization that classifiers have to make. It is simpler to
fit the majority groups as this fit is more important to overall error. We
propose to create a balanced training dataset, consisting of the original
dataset plus new data points in which the group memberships are intervened,
minorities become majorities and vice versa. We show that current generative
adversarial networks are a powerful tool for learning these data points, called
contrastive examples. We experiment with the equalized odds bias measure on
tabular data as well as image data (CelebA and Diversity in Faces datasets).
Contrastive examples allow us to expose correlations between group membership
and other seemingly neutral features. Whenever a causal graph is available, we
can put those contrastive examples in the perspective of counterfactuals."
['stat.ML'],Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies,"We propose and address a novel few-shot RL problem, where a task is
characterized by a subtask graph which describes a set of subtasks and their
dependencies that are unknown to the agent. The agent needs to quickly adapt to
the task over few episodes during adaptation phase to maximize the return in
the test phase. Instead of directly learning a meta-policy, we develop a
Meta-learner with Subtask Graph Inference(MSGI), which infers the latent
parameter of the task by interacting with the environment and maximizes the
return given the latent parameter. To facilitate learning, we adopt an
intrinsic reward inspired by upper confidence bound (UCB) that encourages
efficient exploration. Our experiment results on two grid-world domains and
StarCraft II environments show that the proposed method is able to accurately
infer the latent task parameter, and to adapt more efficiently than existing
meta RL and hierarchical RL methods."
['stat.ML'],Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits,"Probabilistic circuits (PCs) are a promising avenue for probabilistic
modeling, as they permit a wide range of exact and efficient inference
routines. Recent ``deep-learning-style'' implementations of PCs strive for a
better scalability, but are still difficult to train on real-world data, due to
their sparsely connected computational graphs. In this paper, we propose Einsum
Networks (EiNets), a novel implementation design for PCs, improving prior art
in several regards. At their core, EiNets combine a large number of arithmetic
operations in a single monolithic einsum-operation, leading to speedups and
memory savings of up to two orders of magnitude, in comparison to previous
implementations. As an algorithmic contribution, we show that the
implementation of Expectation-Maximization (EM) can be simplified for PCs, by
leveraging automatic differentiation. Furthermore, we demonstrate that EiNets
scale well to datasets which were previously out of reach, such as SVHN and
CelebA, and that they can be used as faithful generative image models."
['stat.ML'],Exploiting Interpretable Patterns for Flow Prediction in Dockless Bike Sharing Systems,"Unlike the traditional dock-based systems, dockless bike-sharing systems are
more convenient for users in terms of flexibility. However, the flexibility of
these dockless systems comes at the cost of management and operation
complexity. Indeed, the imbalanced and dynamic use of bikes leads to mandatory
rebalancing operations, which impose a critical need for effective bike traffic
flow prediction. While efforts have been made in developing traffic flow
prediction models, existing approaches lack interpretability, and thus have
limited value in practical deployment. To this end, we propose an Interpretable
Bike Flow Prediction (IBFP) framework, which can provide effective bike flow
prediction with interpretable traffic patterns. Specifically, by dividing the
urban area into regions according to flow density, we first model the
spatio-temporal bike flows between regions with graph regularized sparse
representation, where graph Laplacian is used as a smooth operator to preserve
the commonalities of the periodic data structure. Then, we extract traffic
patterns from bike flows using subspace clustering with sparse representation
to construct interpretable base matrices. Moreover, the bike flows can be
predicted with the interpretable base matrices and learned parameters. Finally,
experimental results on real-world data show the advantages of the IBFP method
for flow prediction in dockless bike sharing systems. In addition, the
interpretability of our flow pattern exploitation is further illustrated
through a case study where IBFP provides valuable insights into bike flow
analysis."
['stat.ML'],Learning Conserved Networks from Flows,"A challenging problem in complex networks is the network reconstruction
problem from data. This work deals with a class of networks denoted as
conserved networks, in which a flow associated with every edge and the flows
are conserved at all non-source and non-sink nodes. We propose a novel
polynomial time algorithm to reconstruct conserved networks from flow data by
exploiting graph theoretic properties of conserved networks combined with
learning techniques. We prove that exact network reconstruction is possible for
arborescence networks. We also extend the methodology for reconstructing
networks from noisy data and explore the reconstruction performance on
arborescence networks with different structural characteristics."
['stat.ML'],Unsupervised Representation Learning with Minimax Distance Measures,"We investigate the use of Minimax distances to extract in a nonparametric way
the features that capture the unknown underlying patterns and structures in the
data. We develop a general-purpose and computationally efficient framework to
employ Minimax distances with many machine learning methods that perform on
numerical data. We study both computing the pairwise Minimax distances for all
pairs of objects and as well as computing the Minimax distances of all the
objects to/from a fixed (test) object.
  We first efficiently compute the pairwise Minimax distances between the
objects, using the equivalence of Minimax distances over a graph and over a
minimum spanning tree constructed on that. Then, we perform an embedding of the
pairwise Minimax distances into a new vector space, such that their squared
Euclidean distances in the new space equal to the pairwise Minimax distances in
the original space. We also study the case of having multiple pairwise Minimax
matrices, instead of a single one. Thereby, we propose an embedding via first
summing up the centered matrices and then performing an eigenvalue
decomposition to obtain the relevant features.
  In the following, we study computing Minimax distances from a fixed (test)
object which can be used for instance in K-nearest neighbor search. Similar to
the case of all-pair pairwise Minimax distances, we develop an efficient and
general-purpose algorithm that is applicable with any arbitrary base distance
measure. Moreover, we investigate in detail the edges selected by the Minimax
distances and thereby explore the ability of Minimax distances in detecting
outlier objects.
  Finally, for each setting, we perform several experiments to demonstrate the
effectiveness of our framework."
['stat.ML'],Tensor Decompositions for temporal knowledge base completion,"Most algorithms for representation learning and link prediction in relational
data have been designed for static data. However, the data they are applied to
usually evolves with time, such as friend graphs in social networks or user
interactions with items in recommender systems. This is also the case for
knowledge bases, which contain facts such as (US, has president, B. Obama,
[2009-2017]) that are valid only at certain points in time. For the problem of
link prediction under temporal constraints, i.e., answering queries such as
(US, has president, ?, 2012), we propose a solution inspired by the canonical
decomposition of tensors of order 4. We introduce new regularization schemes
and present an extension of ComplEx (Trouillon et al., 2016) that achieves
state-of-the-art performance. Additionally, we propose a new dataset for
knowledge base completion constructed from Wikidata, larger than previous
benchmarks by an order of magnitude, as a new reference for evaluating temporal
and non-temporal link prediction methods."
['stat.ML'],Regression and Singular Value Decomposition in Dynamic Graphs,"Most of real-world graphs are {\em dynamic}, i.e., they change over time.
However, while problems such as regression and Singular Value Decomposition
(SVD) have been studied for {\em static} graphs, they have not been
investigated for {\em dynamic} graphs, yet. In this paper, we introduce,
motivate and study regression and SVD over dynamic graphs. First, we present
the notion of {\em update-efficient matrix embedding} that defines the
conditions sufficient for a matrix embedding to be used for the dynamic graph
regression problem (under $l_2$ norm). We prove that given an $n \times m$
update-efficient matrix embedding (e.g., adjacency matrix), after an update
operation in the graph, the optimal solution of the graph regression problem
for the revised graph can be computed in $O(nm)$ time. We also study dynamic
graph regression under least absolute deviation. Then, we characterize a class
of matrix embeddings that can be used to efficiently update SVD of a dynamic
graph. For adjacency matrix and Laplacian matrix, we study those graph update
operations for which SVD (and low rank approximation) can be updated
efficiently."
['stat.ML'],Heuristics for Link Prediction in Multiplex Networks,"Link prediction, or the inference of future or missing connections between
entities, is a well-studied problem in network analysis. A multitude of
heuristics exist for link prediction in ordinary networks with a single type of
connection. However, link prediction in multiplex networks, or networks with
multiple types of connections, is not a well understood problem. We propose a
novel general framework and three families of heuristics for multiplex network
link prediction that are simple, interpretable, and take advantage of the rich
connection type correlation structure that exists in many real world networks.
We further derive a theoretical threshold for determining when to use a
different connection type based on the number of links that overlap with an
Erdos-Renyi random graph. Through experiments with simulated and real world
scientific collaboration, transportation and global trade networks, we
demonstrate that the proposed heuristics show increased performance with the
richness of connection type correlation structure and significantly outperform
their baseline heuristics for ordinary networks with a single connection type."
['stat.ML'],Graph Highway Networks,"Graph Convolution Networks (GCN) are widely used in learning graph
representations due to their effectiveness and efficiency. However, they suffer
from the notorious over-smoothing problem, in which the learned representations
of densely connected nodes converge to alike vectors when many (>3) graph
convolutional layers are stacked. In this paper, we argue that
there-normalization trick used in GCN leads to overly homogeneous information
propagation, which is the source of over-smoothing. To address this problem, we
propose Graph Highway Networks(GHNet) which utilize gating units to
automatically balance the trade-off between homogeneity and heterogeneity in
the GCN learning process. The gating units serve as direct highways to maintain
heterogeneous information from the node itself after feature propagation. This
design enables GHNet to achieve much larger receptive fields per node without
over-smoothing and thus access to more of the graph connectivity information.
Experimental results on benchmark datasets demonstrate the superior performance
of GHNet over GCN and related models."
['stat.ML'],HopGAT: Hop-aware Supervision Graph Attention Networks for Sparsely Labeled Graphs,"Due to the cost of labeling nodes, classifying a node in a sparsely labeled
graph while maintaining the prediction accuracy deserves attention. The key
point is how the algorithm learns sufficient information from more neighbors
with different hop distances. This study first proposes a hop-aware attention
supervision mechanism for the node classification task. A simulated annealing
learning strategy is then adopted to balance two learning tasks, node
classification and the hop-aware attention coefficients, along the training
timeline. Compared with state-of-the-art models, the experimental results
proved the superior effectiveness of the proposed Hop-aware Supervision Graph
Attention Networks (HopGAT) model. Especially, for the protein-protein
interaction network, in a 40% labeled graph, the performance loss is only 3.9%,
from 98.5% to 94.6%, compared to the fully labeled graph. Extensive experiments
also demonstrate the effectiveness of supervised attention coefficient and
learning strategies."
['stat.ML'],Deep Learning and Open Set Malware Classification: A Survey,"As the Internet is growing rapidly these years, the variant of malicious
software, which often referred to as malware, has become one of the major and
serious threats to Internet users. The dramatic increase of malware has led to
a research area of not only using cutting edge machine learning techniques
classify malware into their known families, moreover, recognize the unknown
ones, which can be related to Open Set Recognition (OSR) problem in machine
learning. Recent machine learning works have shed light on Open Set Recognition
(OSR) from different scenarios. Under the situation of missing unknown training
samples, the OSR system should not only correctly classify the known classes,
but also recognize the unknown class. This survey provides an overview of
different deep learning techniques, a discussion of OSR and graph
representation solutions and an introduction of malware classification systems."
['stat.ML'],Improving Expressivity of Graph Neural Networks,"We propose a Graph Neural Network with greater expressive power than commonly
used GNNs - not constrained to only differentiate between graphs that
Weisfeiler-Lehman test recognizes to be non-isomorphic. We use a graph
attention network with expanding attention window that aggregates information
from nodes exponentially far away. We also use partially random initial
embeddings, allowing differentiation between nodes that would otherwise look
the same. This could cause problem with a traditional dropout mechanism,
therefore we use a ""head dropout"", randomly ignoring some attention heads
rather than some dimensions of the embedding."
['stat.ML'],A Graph Convolutional Network Composition Framework for Semi-supervised Classification,"Graph convolutional networks (GCNs) have gained popularity due to high
performance achievable on several downstream tasks including node
classification. Several architectural variants of these networks have been
proposed and investigated with experimental studies in the literature.
Motivated by a recent work on simplifying GCNs, we study the problem of
designing other variants and propose a framework to compose networks using
building blocks of GCN. The framework offers flexibility to compose and
evaluate different networks using feature and/or label propagation networks,
linear or non-linear networks, with each composition having different
computational complexity. We conduct a detailed experimental study on several
benchmark datasets with many variants and present observations from our
evaluation. Our empirical experimental results suggest that several newly
composed variants are useful alternatives to consider because they are as
competitive as, or better than the original GCN."
['stat.ML'],The general theory of permutation equivarant neural networks and higher order graph variational encoders,"Previous work on symmetric group equivariant neural networks generally only
considered the case where the group acts by permuting the elements of a single
vector. In this paper we derive formulae for general permutation equivariant
layers, including the case where the layer acts on matrices by permuting their
rows and columns simultaneously. This case arises naturally in graph learning
and relation learning applications. As a specific case of higher order
permutation equivariant networks, we present a second order graph variational
encoder, and show that the latent distribution of equivariant generative models
must be exchangeable. We demonstrate the efficacy of this architecture on the
tasks of link prediction in citation graphs and molecular graph generation."
['stat.ML'],GraphGen: A Scalable Approach to Domain-agnostic Labeled Graph Generation,"Graph generative models have been extensively studied in the data mining
literature. While traditional techniques are based on generating structures
that adhere to a pre-decided distribution, recent techniques have shifted
towards learning this distribution directly from the data. While learning-based
approaches have imparted significant improvement in quality, some limitations
remain to be addressed. First, learning graph distributions introduces
additional computational overhead, which limits their scalability to large
graph databases. Second, many techniques only learn the structure and do not
address the need to also learn node and edge labels, which encode important
semantic information and influence the structure itself. Third, existing
techniques often incorporate domain-specific rules and lack generalizability.
Fourth, the experimentation of existing techniques is not comprehensive enough
due to either using weak evaluation metrics or focusing primarily on synthetic
or small datasets. In this work, we develop a domain-agnostic technique called
GraphGen to overcome all of these limitations. GraphGen converts graphs to
sequences using minimum DFS codes. Minimum DFS codes are canonical labels and
capture the graph structure precisely along with the label information. The
complex joint distributions between structure and semantic labels are learned
through a novel LSTM architecture. Extensive experiments on million-sized, real
graph datasets show GraphGen to be 4 times faster on average than
state-of-the-art techniques while being significantly better in quality across
a comprehensive set of 11 different metrics. Our code is released at
https://github.com/idea-iitd/graphgen."
['stat.ML'],Robust spectral clustering using LASSO regularization,"Cluster structure detection is a fundamental task for the analysis of graphs,
in order to understand and to visualize their functional characteristics. Among
the different cluster structure detection methods, spectral clustering is
currently one of the most widely used due to its speed and simplicity. Yet,
there are few theoretical guarantee to recover the underlying partitions of the
graph for general models. This paper therefore presents a variant of spectral
clustering, called 1-spectral clustering, performed on a new random model
closely related to stochastic block model. Its goal is to promote a sparse
eigenbasis solution of a 1 minimization problem revealing the natural structure
of the graph. The effectiveness and the robustness to small noise perturbations
of our technique is confirmed through a collection of simulated and real data
examples."
['stat.ML'],Consistent and Complementary Graph Regularized Multi-view Subspace Clustering,"This study investigates the problem of multi-view clustering, where multiple
views contain consistent information and each view also includes complementary
information. Exploration of all information is crucial for good multi-view
clustering. However, most traditional methods blindly or crudely combine
multiple views for clustering and are unable to fully exploit the valuable
information. Therefore, we propose a method that involves consistent and
complementary graph-regularized multi-view subspace clustering (GRMSC), which
simultaneously integrates a consistent graph regularizer with a complementary
graph regularizer into the objective function. In particular, the consistent
graph regularizer learns the intrinsic affinity relationship of data points
shared by all views. The complementary graph regularizer investigates the
specific information of multiple views. It is noteworthy that the consistent
and complementary regularizers are formulated by two different graphs
constructed from the first-order proximity and second-order proximity of
multiple views, respectively. The objective function is optimized by the
augmented Lagrangian multiplier method in order to achieve multi-view
clustering. Extensive experiments on six benchmark datasets serve to validate
the effectiveness of the proposed method over other state-of-the-art multi-view
clustering methods."
['stat.ML'],Graph Prolongation Convolutional Networks: Explicitly Multiscale Machine Learning on Graphs with Applications to Modeling of Cytoskeleton,"We define a novel type of ensemble Graph Convolutional Network (GCN) model.
Using optimized linear projection operators to map between spatial scales of
graph, this ensemble model learns to aggregate information from each scale for
its final prediction. We calculate these linear projection operators as the
infima of an objective function relating the structure matrices used for each
GCN. Equipped with these projections, our model (a Graph
Prolongation-Convolutional Network) outperforms other GCN ensemble models at
predicting the potential energy of monomer subunits in a coarse-grained
mechanochemical simulation of microtubule bending. We demonstrate these
performance gains by measuring an estimate of the FLOPs spent to train each
model, as well as wall-clock time. Because our model learns at multiple scales,
it is possible to train at each scale according to a predetermined schedule of
coarse vs. fine training. We examine several such schedules adapted from the
Algebraic Multigrid (AMG) literature, and quantify the computational benefit of
each. We also compare this model to another model which features an optimized
coarsening of the input graph. Finally, we derive backpropagation rules for the
input of our network model with respect to its output, and discuss how our
method may be extended to very large graphs."
['stat.ML'],Detecting Communities in Heterogeneous Multi-Relational Networks:A Message Passing based Approach,"Community is a common characteristic of networks including social networks,
biological networks, computer and information networks, to name a few.
Community detection is a basic step for exploring and analysing these network
data. Typically, homogenous network is a type of networks which consists of
only one type of objects with one type of links connecting them. There has been
a large body of developments in models and algorithms to detect communities
over it. However, real-world networks naturally exhibit heterogeneous qualities
appearing as multiple types of objects with multi-relational links connecting
them. Those heterogeneous information could facilitate the community detection
for its constituent homogeneous networks, but has not been fully explored. In
this paper, we exploit heterogeneous multi-relational networks (HMRNet) and
propose an efficient message passing based algorithm to simultaneously detect
communities for all homogeneous networks. Specifically, an HMRNet is
reorganized into a hierarchical structure with homogeneous networks as its
layers and heterogeneous links connecting them. To detect communities in such
an HMRNet, the problem is formulated as a maximum a posterior (MAP) over a
factor graph. Finally a message passing based algorithm is derived to find a
best solution of the MAP problem. Evaluation on both synthetic and real-world
networks confirms the effectiveness of the proposed method."
['stat.ML'],Let's Agree to Degree: Comparing Graph Convolutional Networks in the Message-Passing Framework,"In this paper we cast neural networks defined on graphs as message-passing
neural networks (MPNNs) in order to study the distinguishing power of different
classes of such models. We are interested in whether certain architectures are
able to tell vertices apart based on the feature labels given as input with the
graph. We consider two variants of MPNNS: anonymous MPNNs whose message
functions depend only on the labels of vertices involved; and degree-aware
MPNNs in which message functions can additionally use information regarding the
degree of vertices. The former class covers a popular formalisms for computing
functions on graphs: graph neural networks (GNN). The latter covers the
so-called graph convolutional networks (GCNs), a recently introduced variant of
GNNs by Kipf and Welling. We obtain lower and upper bounds on the
distinguishing power of MPNNs in terms of the distinguishing power of the
Weisfeiler-Lehman (WL) algorithm. Our results imply that (i) the distinguishing
power of GCNs is bounded by the WL algorithm, but that they are one step ahead;
(ii) the WL algorithm cannot be simulated by ""plain vanilla"" GCNs but the
addition of a trade-off parameter between features of the vertex and those of
its neighbours (as proposed by Kipf and Welling themselves) resolves this
problem."
['stat.ML'],Forecast Network-Wide Traffic States for Multiple Steps Ahead: A Deep Learning Approach Considering Dynamic Non-Local Spatial Correlation and Non-Stationary Temporal Dependency,"Obtaining accurate information about future traffic flows of all links in a
traffic network is of great importance for traffic management and control
applications. This research studies two particular problems in traffic
forecasting: (1) capture the dynamic and non-local spatial correlation between
traffic links and (2) model the dynamics of temporal dependency for accurate
multiple steps ahead predictions. To address these issues, we propose a deep
learning framework named Spatial-Temporal Sequence to Sequence model
(STSeq2Seq). This model builds on sequence to sequence (seq2seq) architecture
to capture temporal feature and relies on graph convolution for aggregating
spatial information. Moreover, STSeq2Seq defines and constructs pattern-aware
adjacency matrices (PAMs) based on pair-wise similarity of the recent traffic
patterns on traffic links and integrate it into graph convolution operation. It
also deploys a novel seq2sesq architecture which couples a convolutional
encoder and a recurrent decoder with attention mechanism for dynamic modeling
of long-range dependence between different time steps. We conduct extensive
experiments using two publicly-available large-scale traffic datasets and
compare STSeq2Seq with other baseline models. The numerical results demonstrate
that the proposed model achieves state-of-the-art forecasting performance in
terms of various error measures. The ablation study verifies the effectiveness
of PAMs in capturing dynamic non-local spatial correlation and the superiority
of proposed seq2seq architecture in modeling non-stationary temporal dependency
for multiple steps ahead prediction. Furthermore, qualitative analysis is
conducted on PAMs as well as the attention weights for model interpretation."
['stat.ML'],GraphChallenge.org Sparse Deep Neural Network Performance,"The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to
developing new solutions for analyzing graphs and sparse data. Sparse AI
analytics present unique scalability difficulties. The Sparse Deep Neural
Network (DNN) Challenge draws upon prior challenges from machine learning, high
performance computing, and visual analytics to create a challenge that is
reflective of emerging sparse AI systems. The sparse DNN challenge is based on
a mathematically well-defined DNN inference computation and can be implemented
in any programming environment. In 2019 several sparse DNN challenge
submissions were received from a wide range of authors and organizations. This
paper presents a performance analysis of the best performers of these
submissions. These submissions show that their state-of-the-art sparse DNN
execution time, $T_{\rm DNN}$, is a strong function of the number of DNN
operations performed, $N_{\rm op}$. The sparse DNN challenge provides a clear
picture of current sparse DNN systems and underscores the need for new
innovations to achieve high performance on very large sparse DNNs."
['stat.ML'],DeepMap: Learning Deep Representations for Graph Classification,"Graph-structured data arise in many scenarios. A fundamental problem is to
quantify the similarities of graphs for tasks such as classification. Graph
kernels are positive-semidefinite functions that decompose graphs into
substructures and compare them. One problem in the effective implementation of
this idea is that the substructures are not independent, which leads to
high-dimensional feature space. In addition, graph kernels cannot capture the
high-order complex interactions between vertices. To mitigate these two
problems, we propose a framework called DeepMap to learn deep representations
for graph feature maps. The learnt deep representation for a graph is a dense
and low-dimensional vector that captures complex high-order interactions in a
vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to
arbitrary graphs by aligning vertices across graphs and building the receptive
field for each vertex. We empirically validate DeepMap on various graph
classification benchmarks and demonstrate that it achieves state-of-the-art
performance."
['stat.ML'],Attribute2vec: Deep Network Embedding Through Multi-Filtering GCN,"We present a multi-filtering Graph Convolution Neural Network (GCN) framework
for network embedding task. It uses multiple local GCN filters to do feature
extraction in every propagation layer. We show this approach could capture
different important aspects of node features against the existing attribute
embedding based method. We also show that with multi-filtering GCN approach, we
can achieve significant improvement against baseline methods when training data
is limited. We also perform many empirical experiments and demonstrate the
benefit of using multiple filters against single filter as well as most current
existing network embedding methods for both the link prediction and node
classification tasks."
['stat.ML'],Towards Interpretable Sparse Graph Representation Learning with Laplacian Pooling,"Recent work in graph neural networks (GNNs) has led to improvements in
molecular activity and property prediction tasks. Unfortunately, GNNs often
fail to capture the relative importance of interactions between molecular
substructures, in part due to the absence of efficient intermediate pooling
steps. To address these issues, we propose LaPool (Laplacian Pooling), a novel,
data-driven, and interpretable hierarchical graph pooling method that takes
into account both node features and graph structure to improve molecular
representation. We benchmark LaPool on molecular graph prediction and
understanding tasks and show that it outperforms recent GNNs. Interestingly,
LaPool also remains competitive on non-molecular tasks. Both quantitative and
qualitative assessments are done to demonstrate LaPool's improved
interpretability and highlight its potential benefits in drug design. Finally,
we demonstrate LaPool's utility for the generation of valid and novel molecules
by incorporating it into an adversarial autoencoder."
['stat.ML'],SGAS: Sequential Greedy Architecture Search,"Architecture design has become a crucial component of successful deep
learning. Recent progress in automatic neural architecture search (NAS) shows a
lot of promise. However, discovered architectures often fail to generalize in
the final evaluation. Architectures with a higher validation accuracy during
the search phase may perform worse in the evaluation. Aiming to alleviate this
common issue, we introduce sequential greedy architecture search (SGAS), an
efficient method for neural architecture search. By dividing the search
procedure into sub-problems, SGAS chooses and prunes candidate operations in a
greedy fashion. We apply SGAS to search architectures for Convolutional Neural
Networks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments
show that SGAS is able to find state-of-the-art architectures for tasks such as
image classification, point cloud classification and node classification in
protein-protein interaction graphs with minimal computational cost. Please
visit https://www.deepgcns.org/auto/sgas for more information about SGAS."
['stat.ML'],Unsupervised Feature Learning Architecture with Multi-clustering Integration RBM,"In this paper, we present a novel unsupervised feature learning architecture,
which consists of a multi-clustering integration module and a variant of RBM
termed multi-clustering integration RBM (MIRBM). In the multi-clustering
integration module, we apply three unsupervised K-means, affinity propagation
and spectral clustering algorithms to obtain three different clustering
partitions (CPs) without any background knowledge or label. Then, an unanimous
voting strategy is used to generate a local clustering partition (LCP). The
novel MIRBM model is a core feature encoding part of the proposed unsupervised
feature learning architecture. The novelty of it is that the LCP as an
unsupervised guidance is integrated into one step contrastive divergence (CD1)
learning to guide the distribution of the hidden layer features. For the
instance in the same LCP cluster, the hidden and reconstructed hidden layer
features of the MIRBM model in the proposed architecture tend to constrict
together in the training process. Meanwhile, each LCP center tends to disperse
from each other as much as possible in the hidden and reconstructed hidden
layer during training. The experiments demonstrate that the proposed
unsupervised feature learning architecture has more powerful feature
representation and generalization capability than the state-of-the-art graph
regularized RBM (GraphRBM) for clustering tasks in the Microsoft Research Asia
Multimedia (MSRA-MM)2.0 dataset."
['stat.ML'],Synchronizing Probability Measures on Rotations via Optimal Transport,"We introduce a new paradigm, $\textit{measure synchronization}$, for
synchronizing graphs with measure-valued edges. We formulate this problem as
maximization of the cycle-consistency in the space of probability measures over
relative rotations. In particular, we aim at estimating marginal distributions
of absolute orientations by synchronizing the $\textit{conditional}$ ones,
which are defined on the Riemannian manifold of quaternions. Such graph
optimization on distributions-on-manifolds enables a natural treatment of
multimodal hypotheses, ambiguities and uncertainties arising in many computer
vision applications such as SLAM, SfM, and object pose estimation. We first
formally define the problem as a generalization of the classical rotation graph
synchronization, where in our case the vertices denote probability measures
over rotations. We then measure the quality of the synchronization by using
Sinkhorn divergences, which reduces to other popular metrics such as
Wasserstein distance or the maximum mean discrepancy as limit cases. We propose
a nonparametric Riemannian particle optimization approach to solve the problem.
Even though the problem is non-convex, by drawing a connection to the recently
proposed sparse optimization methods, we show that the proposed algorithm
converges to the global optimum in a special case of the problem under certain
conditions. Our qualitative and quantitative experiments show the validity of
our approach and we bring in new perspectives to the study of synchronization."
['stat.ML'],Drug-disease Graph: Predicting Adverse Drug Reaction Signals via Graph Neural Network with Clinical Data,"Adverse Drug Reaction (ADR) is a significant public health concern
world-wide. Numerous graph-based methods have been applied to biomedical graphs
for predicting ADRs in pre-marketing phases. ADR detection in post-market
surveillance is no less important than pre-marketing assessment, and ADR
detection with large-scale clinical data have attracted much attention in
recent years. However, there are not many studies considering graph structures
from clinical data for detecting an ADR signal, which is a pair of a
prescription and a diagnosis that might be a potential ADR. In this study, we
develop a novel graph-based framework for ADR signal detection using healthcare
claims data. We construct a Drug-disease graph with nodes representing the
medical codes. The edges are given as the relationships between two codes,
computed using the data. We apply Graph Neural Network to predict ADR signals,
using labels from the Side Effect Resource database. The model shows improved
AUROC and AUPRC performance of 0.795 and 0.775, compared to other algorithms,
showing that it successfully learns node representations expressive of those
relationships. Furthermore, our model predicts ADR pairs that do not exist in
the established ADR database, showing its capability to supplement the ADR
database."
['stat.ML'],Business Process Variant Analysis based on Mutual Fingerprints of Event Logs,"Comparing business process variants using event logs is a common use case in
process mining. Existing techniques for process variant analysis detect
statistically-significant differences between variants at the level of
individual entities (such as process activities) and their relationships (e.g.
directly-follows relations between activities). This may lead to a
proliferation of differences due to the low level of granularity in which such
differences are captured. This paper presents a novel approach to detect
statistically-significant differences between variants at the level of entire
process traces (i.e. sequences of directly-follows relations). The cornerstone
of this approach is a technique to learn a directly follows graph called mutual
fingerprint from the event logs of the two variants. A mutual fingerprint is a
lossless encoding of a set of traces and their duration using discrete wavelet
transformation. This structure facilitates the understanding of statistical
differences along the control-flow and performance dimensions. The approach has
been evaluated using real-life event logs against two baselines. The results
show that at a trace level, the baselines cannot always reveal the differences
discovered by our approach, or can detect spurious differences."
['stat.ML'],A theory of independent mechanisms for extrapolation in generative models,"Deep generative models reproduce complex empirical data but cannot
extrapolate to novel environments. An intuitive idea to promote extrapolation
capabilities is to enforce the architecture to have the modular structure of a
causal graphical model, where one can intervene on each module independently of
the others in the graph. We develop a framework to formalize this intuition,
using the principle of Independent Causal Mechanisms, and show how
over-parameterization of generative neural networks can hinder extrapolation
capabilities. Our experiments on the generation of human faces shows successive
layers of a generator architecture implement independent mechanisms to some
extent, allowing meaningful extrapolations. Finally, we illustrate that
independence of mechanisms may be enforced during training to improve
extrapolation."
['stat.ML'],Exact marginal inference in Latent Dirichlet Allocation,"Assume we have potential ""causes"" $z\in Z$, which produce ""events"" $w$ with
known probabilities $\beta(w|z)$. We observe $w_1,w_2,...,w_n$, what can we say
about the distribution of the causes? A Bayesian estimate will assume a prior
on distributions on $Z$ (we assume a Dirichlet prior) and calculate a
posterior. An average over that posterior then gives a distribution on $Z$,
which estimates how much each cause $z$ contributed to our observations. This
is the setting of Latent Dirichlet Allocation, which can be applied e.g. to
topics ""producing"" words in a document. In this setting usually the number of
observed words is large, but the number of potential topics is small. We are
here interested in applications with many potential ""causes"" (e.g. locations on
the globe), but only a few observations. We show that the exact Bayesian
estimate can be computed in linear time (and constant space) in $|Z|$ for a
given upper bound on $n$ with a surprisingly simple formula. We generalize this
algorithm to the case of sparse probabilities $\beta(w|z)$, in which we only
need to assume that the tree width of an ""interaction graph"" on the
observations is limited. On the other hand we also show that without such
limitation the problem is NP-hard."
"['stat.ML', 'stat.TH']",A Decentralized Policy with Logarithmic Regret for a Class of Multi-Agent Multi-Armed Bandit Problems with Option Unavailability Constraints and Stochastic Communication Protocols,"This paper considers a multi-armed bandit (MAB) problem in which multiple
mobile agents receive rewards by sampling from a collection of spatially
dispersed stochastic processes, called bandits. The goal is to formulate a
decentralized policy for each agent, in order to maximize the total cumulative
reward over all agents, subject to option availability and inter-agent
communication constraints. The problem formulation is motivated by applications
in which a team of autonomous mobile robots cooperates to accomplish an
exploration and exploitation task in an uncertain environment. Bandit locations
are represented by vertices of the spatial graph. At any time, an agent's
option consist of sampling the bandit at its current location, or traveling
along an edge of the spatial graph to a new bandit location. Communication
constraints are described by a directed, non-stationary, stochastic
communication graph. At any time, agents may receive data only from their
communication graph in-neighbors. For the case of a single agent on a fully
connected spatial graph, it is known that the expected regret for any optimal
policy is necessarily bounded below by a function that grows as the logarithm
of time. A class of policies called upper confidence bound (UCB) algorithms
asymptotically achieve logarithmic regret for the classical MAB problem. In
this paper, we propose a UCB-based decentralized motion and option selection
policy and a non-stationary stochastic communication protocol that guarantee
logarithmic regret. To our knowledge, this is the first such decentralized
policy for non-fully connected spatial graphs with communication constraints.
When the spatial graph is fully connected and the communication graph is
stationary, our decentralized algorithm matches or exceeds the best reported
prior results from the literature."
['stat.ML'],Gossip and Attend: Context-Sensitive Graph Representation Learning,"Graph representation learning (GRL) is a powerful technique for learning
low-dimensional vector representation of high-dimensional and often sparse
graphs. Most studies explore the structure and metadata associated with the
graph using random walks and employ an unsupervised or semi-supervised learning
schemes. Learning in these methods is context-free, resulting in only a single
representation per node. Recently studies have argued on the adequacy of a
single representation and proposed context-sensitive approaches, which are
capable of extracting multiple node representations for different contexts.
This proved to be highly effective in applications such as link prediction and
ranking.
  However, most of these methods rely on additional textual features that
require complex and expensive RNNs or CNNs to capture high-level features or
rely on a community detection algorithm to identify multiple contexts of a
node.
  In this study we show that in-order to extract high-quality context-sensitive
node representations it is not needed to rely on supplementary node features,
nor to employ computationally heavy and complex models. We propose GOAT, a
context-sensitive algorithm inspired by gossip communication and a mutual
attention mechanism simply over the structure of the graph. We show the
efficacy of GOAT using 6 real-world datasets on link prediction and node
clustering tasks and compare it against 12 popular and state-of-the-art (SOTA)
baselines. GOAT consistently outperforms them and achieves up to 12% and 19%
gain over the best performing methods on link prediction and clustering tasks,
respectively."
['stat.ML'],Stochastic Flows and Geometric Optimization on the Orthogonal Group,"We present a new class of stochastic, geometrically-driven optimization
algorithms on the orthogonal group $O(d)$ and naturally reductive homogeneous
manifolds obtained from the action of the rotation group $SO(d)$. We
theoretically and experimentally demonstrate that our methods can be applied in
various fields of machine learning including deep, convolutional and recurrent
neural networks, reinforcement learning, normalizing flows and metric learning.
We show an intriguing connection between efficient stochastic optimization on
the orthogonal group and graph theory (e.g. matching problem, partition
functions over graphs, graph-coloring). We leverage the theory of Lie groups
and provide theoretical results for the designed class of algorithms. We
demonstrate broad applicability of our methods by showing strong performance on
the seemingly unrelated tasks of learning world models to obtain stable
policies for the most difficult $\mathrm{Humanoid}$ agent from
$\mathrm{OpenAI}$ $\mathrm{Gym}$ and improving convolutional neural networks."
['stat.ML'],OCmst: One-class Novelty Detection using Convolutional Neural Network and Minimum Spanning Trees,"We present a novel model called One Class Minimum Spanning Tree (OCmst) for
novelty detection problem that uses a Convolutional Neural Network (CNN) as
deep feature extractor and graph-based model based on Minimum Spanning Tree
(MST). In a novelty detection scenario, the training data is no polluted by
outliers (abnormal class) and the goal is to recognize if a test instance
belongs to the normal class or to the abnormal class. Our approach uses the
deep features from CNN to feed a pair of MSTs built starting from each test
instance. To cut down the computational time we use a parameter $\gamma$ to
specify the size of the MST's starting to the neighbours from the test
instance. To prove the effectiveness of the proposed approach we conducted
experiments on two publicly available datasets, well-known in literature and we
achieved the state-of-the-art results on CIFAR10 dataset."
['stat.ML'],Secure Metric Learning via Differential Pairwise Privacy,"Distance Metric Learning (DML) has drawn much attention over the last two
decades. A number of previous works have shown that it performs well in
measuring the similarities of individuals given a set of correctly labeled
pairwise data by domain experts. These important and precisely-labeled pairwise
data are often highly sensitive in real world (e.g., patients similarity). This
paper studies, for the first time, how pairwise information can be leaked to
attackers during distance metric learning, and develops differential pairwise
privacy (DPP), generalizing the definition of standard differential privacy,
for secure metric learning. Unlike traditional differential privacy which only
applies to independent samples, thus cannot be used for pairwise data, DPP
successfully deals with this problem by reformulating the worst case.
Specifically, given the pairwise data, we reveal all the involved correlations
among pairs in the constructed undirected graph. DPP is then formalized that
defines what kind of DML algorithm is private to preserve pairwise data. After
that, a case study employing the contrastive loss is exhibited to clarify the
details of implementing a DPP-DML algorithm. Particularly, the sensitivity
reduction technique is proposed to enhance the utility of the output distance
metric. Experiments both on a toy dataset and benchmarks demonstrate that the
proposed scheme achieves pairwise data privacy without compromising the output
performance much (Accuracy declines less than 0.01 throughout all benchmark
datasets when the privacy budget is set at 4)."
['stat.ML'],Stochastic Proximal Gradient Algorithm with Minibatches. Application to Large Scale Learning Models,"Stochastic optimization lies at the core of most statistical learning models.
The recent great development of stochastic algorithmic tools focused
significantly onto proximal gradient iterations, in order to find an efficient
approach for nonsmooth (composite) population risk functions. The complexity of
finding optimal predictors by minimizing regularized risk is largely understood
for simple regularizations such as $\ell_1/\ell_2$ norms. However, more complex
properties desired for the predictor necessitates highly difficult regularizers
as used in grouped lasso or graph trend filtering. In this chapter we develop
and analyze minibatch variants of stochastic proximal gradient algorithm for
general composite objective functions with stochastic nonsmooth components. We
provide iteration complexity for constant and variable stepsize policies
obtaining that, for minibatch size $N$, after
$\mathcal{O}(\frac{1}{N\epsilon})$ iterations $\epsilon-$suboptimality is
attained in expected quadratic distance to optimal solution. The numerical
tests on $\ell_2-$regularized SVMs and parametric sparse representation
problems confirm the theoretical behaviour and surpasses minibatch SGD
performance."
['stat.ML'],Topological Data Analysis in Text Classification: Extracting Features with Additive Information,"While the strength of Topological Data Analysis has been explored in many
studies on high dimensional numeric data, it is still a challenging task to
apply it to text. As the primary goal in topological data analysis is to define
and quantify the shapes in numeric data, defining shapes in the text is much
more challenging, even though the geometries of vector spaces and conceptual
spaces are clearly relevant for information retrieval and semantics. In this
paper, we examine two different methods of extraction of topological features
from text, using as the underlying representations of words the two most
popular methods, namely word embeddings and TF-IDF vectors. To extract
topological features from the word embedding space, we interpret the embedding
of a text document as high dimensional time series, and we analyze the topology
of the underlying graph where the vertices correspond to different embedding
dimensions. For topological data analysis with the TF-IDF representations, we
analyze the topology of the graph whose vertices come from the TF-IDF vectors
of different blocks in the textual document. In both cases, we apply
homological persistence to reveal the geometric structures under different
distance resolutions. Our results show that these topological features carry
some exclusive information that is not captured by conventional text mining
methods. In our experiments we observe adding topological features to the
conventional features in ensemble models improves the classification results
(up to 5\%). On the other hand, as expected, topological features by themselves
may be not sufficient for effective classification. It is an open problem to
see whether TDA features from word embeddings might be sufficient, as they seem
to perform within a range of few points from top results obtained with a linear
support vector classifier."
['stat.ML'],Unsupervised Attributed Multiplex Network Embedding,"Nodes in a multiplex network are connected by multiple types of relations.
However, most existing network embedding methods assume that only a single type
of relation exists between nodes. Even for those that consider the multiplexity
of a network, they overlook node attributes, resort to node labels for
training, and fail to model the global properties of a graph. We present a
simple yet effective unsupervised network embedding method for attributed
multiplex network called DMGI, inspired by Deep Graph Infomax (DGI) that
maximizes the mutual information between local patches of a graph, and the
global representation of the entire graph. We devise a systematic way to
jointly integrate the node embeddings from multiple graphs by introducing 1)
the consensus regularization framework that minimizes the disagreements among
the relation-type specific node embeddings, and 2) the universal discriminator
that discriminates true samples regardless of the relation types. We also show
that the attention mechanism infers the importance of each relation type, and
thus can be useful for filtering unnecessary relation types as a preprocessing
step. Extensive experiments on various downstream tasks demonstrate that DMGI
outperforms the state-of-the-art methods, even though DMGI is fully
unsupervised."
['stat.ML'],On the Duality between Network Flows and Network Lasso,"Many applications generate data with an intrinsic network structure such as
time series data, image data or social network data. The network Lasso (nLasso)
has been proposed recently as a method for joint clustering and optimization of
machine learning models for networked data. The nLasso extends the Lasso from
sparse linear models to clustered graph signals. This paper explores the
duality of nLasso and network flow optimization. We show that, in a very
precise sense, nLasso is equivalent to a minimum-cost flow problem on the data
network structure. Our main technical result is a concise characterization of
nLasso solutions via existence of certain network flows. The main conceptual
result is a useful link between nLasso methods and basic graph algorithms such
as clustering or maximum flow."
['stat.ML'],Semiparametric Inference For Causal Effects In Graphical Models With Hidden Variables,"The last decade witnessed the development of algorithms that completely solve
the identifiability problem for causal effects in hidden variable causal models
associated with directed acyclic graphs. However, much of this machinery
remains underutilized in practice owing to the complexity of estimating
identifying functionals yielded by these algorithms. In this paper, we provide
simple graphical criteria and semiparametric estimators that bridge the gap
between identification and estimation for causal effects involving a single
treatment and a single outcome. First, we provide influence function based
doubly robust estimators that cover a significant subset of hidden variable
causal models where the effect is identifiable. We further characterize an
important subset of this class for which we demonstrate how to derive the
estimator with the lowest asymptotic variance, i.e., one that achieves the
semiparametric efficiency bound. Finally, we provide semiparametric estimators
for any single treatment causal effect parameter identified via the
aforementioned algorithms. The resulting estimators resemble influence function
based estimators that are sequentially reweighted, and exhibit a partial double
robustness property, provided the parts of the likelihood corresponding to a
set of weight models are correctly specified. Our methods are easy to implement
and we demonstrate their utility through simulations."
['stat.ML'],The impossibility of low rank representations for triangle-rich complex networks,"The study of complex networks is a significant development in modern science,
and has enriched the social sciences, biology, physics, and computer science.
Models and algorithms for such networks are pervasive in our society, and
impact human behavior via social networks, search engines, and recommender
systems to name a few. A widely used algorithmic technique for modeling such
complex networks is to construct a low-dimensional Euclidean embedding of the
vertices of the network, where proximity of vertices is interpreted as the
likelihood of an edge. Contrary to the common view, we argue that such graph
embeddings do not}capture salient properties of complex networks. The two
properties we focus on are low degree and large clustering coefficients, which
have been widely established to be empirically true for real-world networks. We
mathematically prove that any embedding (that uses dot products to measure
similarity) that can successfully create these two properties must have rank
nearly linear in the number of vertices. Among other implications, this
establishes that popular embedding techniques such as Singular Value
Decomposition and node2vec fail to capture significant structural aspects of
real-world complex networks. Furthermore, we empirically study a number of
different embedding techniques based on dot product, and show that they all
fail to capture the triangle structure."
['stat.ML'],"word2vec, node2vec, graph2vec, X2vec: Towards a Theory of Vector Embeddings of Structured Data","Vector representations of graphs and relational structures, whether
hand-crafted feature vectors or learned representations, enable us to apply
standard data analysis and machine learning techniques to the structures. A
wide range of methods for generating such embeddings have been studied in the
machine learning and knowledge representation literature. However, vector
embeddings have received relatively little attention from a theoretical point
of view.
  Starting with a survey of embedding techniques that have been used in
practice, in this paper we propose two theoretical approaches that we see as
central for understanding the foundations of vector embeddings. We draw
connections between the various approaches and suggest directions for future
research."
['stat.ML'],$Π-$nets: Deep Polynomial Neural Networks,"Deep Convolutional Neural Networks (DCNNs) is currently the method of choice
both for generative, as well as for discriminative learning in computer vision
and machine learning. The success of DCNNs can be attributed to the careful
selection of their building blocks (e.g., residual blocks, rectifiers,
sophisticated normalization schemes, to mention but a few). In this paper, we
propose $\Pi$-Nets, a new class of DCNNs. $\Pi$-Nets are polynomial neural
networks, i.e., the output is a high-order polynomial of the input. $\Pi$-Nets
can be implemented using special kind of skip connections and their parameters
can be represented via high-order tensors. We empirically demonstrate that
$\Pi$-Nets have better representation power than standard DCNNs and they even
produce good results without the use of non-linear activation functions in a
large battery of tasks and signals, i.e., images, graphs, and audio. When used
in conjunction with activation functions, $\Pi$-Nets produce state-of-the-art
results in challenging tasks, such as image generation. Lastly, our framework
elucidates why recent generative models, such as StyleGAN, improve upon their
predecessors, e.g., ProGAN."
['stat.ML'],Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs,"We consider the fully decentralized machine learning scenario where many
users with personal datasets collaborate to learn models through local
peer-to-peer exchanges, without a central coordinator. We propose to train
personalized models that leverage a collaboration graph describing the
relationships between user personal tasks, which we learn jointly with the
models. Our fully decentralized optimization procedure alternates between
training nonlinear models given the graph in a greedy boosting manner, and
updating the collaboration graph (with controlled sparsity) given the models.
Throughout the process, users exchange messages only with a small number of
peers (their direct neighbors when updating the models, and a few random users
when updating the graph), ensuring that the procedure naturally scales with the
number of users. Overall, our approach is communication-efficient and avoids
exchanging personal data. We provide an extensive analysis of the convergence
rate, memory and communication complexity of our approach, and demonstrate its
benefits compared to competing techniques on synthetic and real datasets."
['stat.ML'],A Survey of Deep Learning for Scientific Discovery,"Over the past few years, we have seen fundamental breakthroughs in core
problems in machine learning, largely driven by advances in deep neural
networks. At the same time, the amount of data collected in a wide array of
scientific domains is dramatically increasing in both size and complexity.
Taken together, this suggests many exciting opportunities for deep learning
applications in scientific settings. But a significant challenge to this is
simply knowing where to start. The sheer breadth and diversity of different
deep learning techniques makes it difficult to determine what scientific
problems might be most amenable to these methods, or which specific combination
of methods might offer the most promising first approach. In this survey, we
focus on addressing this central issue, providing an overview of many widely
used deep learning models, spanning visual, sequential and graph structured
data, associated tasks and different training methods, along with techniques to
use deep learning with less data and better interpret these complex models ---
two central considerations for many scientific use cases. We also include
overviews of the full design process, implementation tips, and links to a
plethora of tutorials, research summaries and open-sourced deep learning
pipelines and pretrained models, developed by the community. We hope that this
survey will help accelerate the use of deep learning across different
scientific domains."
['stat.ML'],Bridging the Gap Between Spectral and Spatial Domains in Graph Neural Networks,"This paper aims at revisiting Graph Convolutional Neural Networks by bridging
the gap between spectral and spatial design of graph convolutions. We
theoretically demonstrate some equivalence of the graph convolution process
regardless it is designed in the spatial or the spectral domain. The obtained
general framework allows to lead a spectral analysis of the most popular
ConvGNNs, explaining their performance and showing their limits. Moreover, the
proposed framework is used to design new convolutions in spectral domain with a
custom frequency profile while applying them in the spatial domain. We also
propose a generalization of the depthwise separable convolution framework for
graph convolutional networks, what allows to decrease the total number of
trainable parameters by keeping the capacity of the model. To the best of our
knowledge, such a framework has never been used in the GNNs literature. Our
proposals are evaluated on both transductive and inductive graph learning
problems. Obtained results show the relevance of the proposed method and
provide one of the first experimental evidence of transferability of spectral
filter coefficients from one graph to another. Our source codes are publicly
available at: https://github.com/balcilar/Spectral-Designed-Graph-Convolutions"
['stat.ML'],Incorporating User's Preference into Attributed Graph Clustering,"Graph clustering has been studied extensively on both plain graphs and
attributed graphs. However, all these methods need to partition the whole graph
to find cluster structures. Sometimes, based on domain knowledge, people may
have information about a specific target region in the graph and only want to
find a single cluster concentrated on this local region. Such a task is called
local clustering. In contrast to global clustering, local clustering aims to
find only one cluster that is concentrating on the given seed vertex (and also
on the designated attributes for attributed graphs). Currently, very few
methods can deal with this kind of task. To this end, we propose two quality
measures for a local cluster: Graph Unimodality (GU) and Attribute Unimodality
(AU). The former measures the homogeneity of the graph structure while the
latter measures the homogeneity of the subspace that is composed of the
designated attributes. We call their linear combination as Compactness.
Further, we propose LOCLU to optimize the Compactness score. The local cluster
detected by LOCLU concentrates on the region of interest, provides efficient
information flow in the graph and exhibits a unimodal data distribution in the
subspace of the designated attributes."
['stat.ML'],Thresholding Graph Bandits with GrAPL,"In this paper, we introduce a new online decision making paradigm that we
call Thresholding Graph Bandits. The main goal is to efficiently identify a
subset of arms in a multi-armed bandit problem whose means are above a
specified threshold. While traditionally in such problems, the arms are assumed
to be independent, in our paradigm we further suppose that we have access to
the similarity between the arms in the form of a graph, allowing us gain
information about the arm means in fewer samples. Such settings play a key role
in a wide range of modern decision making problems where rapid decisions need
to be made in spite of the large number of options available at each time. We
present GrAPL, a novel algorithm for the thresholding graph bandit problem. We
demonstrate theoretically that this algorithm is effective in taking advantage
of the graph structure when available and the reward function homophily (that
strongly connected arms have similar rewards) when favorable. We confirm these
theoretical findings via experiments on both synthetic and real data."
['stat.ML'],Dynamic Origin-Destination Matrix Prediction with Line Graph Neural Networks and Kalman Filter,"Modern intelligent transportation systems provide data that allow real-time
dynamic demand prediction, which is essential for planning and operations. The
main challenge of prediction of dynamic Origin-Destination (O-D) demand
matrices is that demands cannot be directly measured by traffic sensors;
instead, they have to be inferred from aggregate traffic flow data on traffic
links. Specifically, spatial correlation, congestion and time dependent factors
need to be considered in general transportation networks. In this paper we
propose a novel O-D prediction framework combining heterogeneous prediction in
graph neural networks and Kalman filter to recognize spatial and temporal
patterns simultaneously. The underlying road network topology is converted into
a corresponding line graph in the newly designed Fusion Line Graph
Convolutional Networks (FL-GCNs), which provide a general framework of
predicting spatial-temporal O-D flows from link information. Data from New
Jersey Turnpike network are used to evaluate the proposed model. The results
show that our proposed approach yields the best performance under various
prediction scenarios. In addition, the advantage of combining deep neural
networks and Kalman filter is demonstrated."
['stat.ML'],Mutual Information Maximization in Graph Neural Networks,"A variety of graph neural networks (GNNs) frameworks for representation
learning on graphs have been recently developed. These frameworks rely on
aggregation and iteration scheme to learn the representation of nodes. However,
information between nodes is inevitably lost in the scheme during learning. In
order to reduce the loss, we extend the GNNs frameworks by exploring the
aggregation and iteration scheme in the methodology of mutual information. We
propose a new approach of enlarging the normal neighborhood in the aggregation
of GNNs, which aims at maximizing mutual information. Based on a series of
experiments conducted on several benchmark datasets, we show that the proposed
approach improves the state-of-the-art performance for four types of graph
tasks, including supervised and semi-supervised graph classification, graph
link prediction and graph edge generation and classification."
"['stat.ML', 'stat.ME']",Learning Sparse Nonparametric DAGs,"We develop a framework for learning sparse nonparametric directed acyclic
graphs (DAGs) from data. Our approach is based on a recent algebraic
characterization of DAGs that led to a fully continuous program for score-based
learning of DAG models parametrized by a linear structural equation model
(SEM). We extend this algebraic characterization to nonparametric SEM by
leveraging nonparametric sparsity based on partial derivatives, resulting in a
continuous optimization problem that can be applied to a variety of
nonparametric and semiparametric models including GLMs, additive noise models,
and index models as special cases. Unlike existing approaches that require
specific modeling choices, loss functions, or algorithms, we present a
completely general framework that can be applied to general nonlinear models
(e.g. without additive noise), general differentiable loss functions, and
generic black-box optimization routines. The code is available at
https://github.com/xunzheng/notears."
['stat.ML'],ProGraML: Graph-based Deep Learning for Program Optimization and Analysis,"The increasing complexity of computing systems places a tremendous burden on
optimizing compilers, requiring ever more accurate and aggressive
optimizations. Machine learning offers significant benefits for constructing
optimization heuristics but there remains a gap between what state-of-the-art
methods achieve and the performance of an optimal heuristic. Closing this gap
requires improvements in two key areas: a representation that accurately
captures the semantics of programs, and a model architecture with sufficient
expressiveness to reason about this representation.
  We introduce ProGraML - Program Graphs for Machine Learning - a novel
graph-based program representation using a low level, language agnostic, and
portable format; and machine learning models capable of performing complex
downstream tasks over these graphs. The ProGraML representation is a directed
attributed multigraph that captures control, data, and call relations, and
summarizes instruction and operand types and ordering. Message Passing Neural
Networks propagate information through this structured representation, enabling
whole-program or per-vertex classification tasks.
  ProGraML provides a general-purpose program representation that equips
learnable models to perform the types of program analysis that are fundamental
to optimization. To this end, we evaluate the performance of our approach first
on a suite of traditional compiler analysis tasks: control flow reachability,
dominator trees, data dependencies, variable liveness, and common subexpression
detection. On a benchmark dataset of 250k LLVM-IR files covering six source
programming languages, ProGraML achieves an average 94.0 F1 score,
significantly outperforming the state-of-the-art approaches. We then apply our
approach to two high-level tasks - heterogeneous device mapping and program
classification - setting new state-of-the-art performance in both."
['stat.ML'],Diffusion-based Deep Active Learning,"The remarkable performance of deep neural networks depends on the
availability of massive labeled data. To alleviate the load of data annotation,
active deep learning aims to select a minimal set of training points to be
labelled which yields maximal model accuracy. Most existing approaches
implement either an `exploration'-type selection criterion, which aims at
exploring the joint distribution of data and labels, or a `refinement'-type
criterion which aims at localizing the detected decision boundaries. We propose
a versatile and efficient criterion that automatically switches from
exploration to refinement when the distribution has been sufficiently mapped.
Our criterion relies on a process of diffusing the existing label information
over a graph constructed from the hidden representation of the data set as
provided by the neural network. This graph representation captures the
intrinsic geometry of the approximated labeling function. The diffusion-based
criterion is shown to be advantageous as it outperforms existing criteria for
deep active learning."
['stat.ML'],Incremental and Decremental Fuzzy Bounded Twin Support Vector Machine,"In this paper we present an incremental variant of the Twin Support Vector
Machine (TWSVM) called Fuzzy Bounded Twin Support Vector Machine (FBTWSVM) to
deal with large datasets and learning from data streams. We combine the TWSVM
with a fuzzy membership function, so that each input has a different
contribution to each hyperplane in a binary classifier. To solve the pair of
quadratic programming problems (QPPs) we use a dual coordinate descent
algorithm with a shrinking strategy, and to obtain a robust classification with
a fast training we propose the use of a Fourier Gaussian approximation function
with our linear FBTWSVM. Inspired by the shrinking technique, the incremental
algorithm re-utilizes part of the training method with some heuristics, while
the decremental procedure is based on a scored window. The FBTWSVM is also
extended for multi-class problems by combining binary classifiers using a
Directed Acyclic Graph (DAG) approach. Moreover, we analyzed the theoretical
foundations properties of the proposed approach and its extension, and the
experimental results on benchmark datasets indicate that the FBTWSVM has a fast
training and retraining process while maintaining a robust classification
performance."
['stat.ML'],GraKeL: A Graph Kernel Library in Python,"The problem of accurately measuring the similarity between graphs is at the
core of many applications in a variety of disciplines. Graph kernels have
recently emerged as a promising approach to this problem. There are now many
kernels, each focusing on different structural aspects of graphs. Here, we
present GraKeL, a library that unifies several graph kernels into a common
framework. The library is written in Python and adheres to the scikit-learn
interface. It is simple to use and can be naturally combined with
scikit-learn's modules to build a complete machine learning pipeline for tasks
such as graph classification and clustering. The code is BSD licensed and is
available at: https://github.com/ysig/GraKeL ."
['stat.ML'],Data Mapping and Finite Difference Learning,"Restricted Boltzmann machine (RBM) is a two-layer neural network constructed
as a probabilistic model and its training is to maximize a product of
probabilities by the contrastive divergence (CD) scheme. In this paper a data
mapping is proposed to describe the relationship between the visible and hidden
layers and the training is to minimize a squared error on the visible layer by
a finite difference learning. This paper presents three new properties in using
the RBM: 1) nodes on the visible and hidden layers can take real-valued matrix
data without a probabilistic interpretation; 2) the famous CD1 is a finite
difference approximation of the gradient descent; 3) the activation can take
non-sigmoid functions such as identity, relu and softsign. The data mapping
provides a unified framework on the dimensionality reduction, the feature
extraction and the data representation pioneered and developed by Hinton and
his colleagues. As an approximation of the gradient descent, the finite
difference learning is applicable to both directed and undirected graphs.
Numerical experiments are performed to verify these new properties on the very
low dimensionality reduction, the collinearity of timer series data and the use
of flexible activations."
['stat.ML'],Bandits with Feedback Graphs and Switching Costs,"We study the adversarial multi-armed bandit problem where partial
observations are available and where, in addition to the loss incurred for each
action, a \emph{switching cost} is incurred for shifting to a new action. All
previously known results incur a factor proportional to the independence number
of the feedback graph. We give a new algorithm whose regret guarantee depends
only on the domination number of the graph. We further supplement that result
with a lower bound. Finally, we also give a new algorithm with improved policy
regret bounds when partial counterfactual feedback is available."
['stat.ML'],Autoencoding Undirected Molecular Graphs With Neural Networks,"Discrete structure rules for validating molecular structures are usually
limited to fulfillment of the octet rule or similar simple deterministic
heuristics. We propose a model, inspired by language modeling from natural
language processing, with the ability to learn from a collection of undirected
molecular graphs, enabling fitting of any underlying structure rule present in
the collection. We introduce an adaption to the popular Transformer model,
which can learn relationships between atoms and bonds. To our knowledge, the
Transformer adaption is the first model that is trained to solve the
unsupervised task of recovering partially observed molecules. In this work, we
assess how different degrees of information impact performance w.r.t. to
fitting the QM9 dataset, which conforms to the octet rule, and to fitting the
ZINC dataset, which contains hypervalent molecules and ions requiring the model
to learn a more complex structure rule. More specifically, we test a full
discrete graph with bond order information, a full discrete graph with only
connectivity, a bag-of-neighbors, a bag-of-atoms, and a count-based unigram
statistics. These results provide encouraging evidence that neural networks,
even when only connectivity is available, can learn arbitrary molecular
structure rules specific to a dataset, as the Transformer adaption surpasses a
strong octet rule baseline on the ZINC dataset."
['stat.ML'],Probabilistic Dual Network Architecture Search on Graphs,"We present the first differentiable Network Architecture Search (NAS) for
Graph Neural Networks (GNNs). GNNs show promising performance on a wide range
of tasks, but require a large amount of architecture engineering. First, graphs
are inherently a non-Euclidean and sophisticated data structure, leading to
poor adaptivity of GNN architectures across different datasets. Second, a
typical graph block contains numerous different components, such as aggregation
and attention, generating a large combinatorial search space. To counter these
problems, we propose a Probabilistic Dual Network Architecture Search (PDNAS)
framework for GNNs. PDNAS not only optimises the operations within a single
graph block (micro-architecture), but also considers how these blocks should be
connected to each other (macro-architecture). The dual architecture (micro- and
marco-architectures) optimisation allows PDNAS to find deeper GNNs on diverse
datasets with better performance compared to other graph NAS methods. Moreover,
we use a fully gradient-based search approach to update architectural
parameters, making it the first differentiable graph NAS method. PDNAS
outperforms existing hand-designed GNNs and NAS results, for example, on the
PPI dataset, PDNAS beats its best competitors by 1.67 and 0.17 in F1 scores."
['stat.ML'],Graph Laplacian mixture model,"Graph learning methods have recently been receiving increasing interest as
means to infer structure in datasets. Most of the recent approaches focus on
different relationships between a graph and data sample distributions, mostly
in settings where all available data relate to the same graph. This is,
however, not always the case, as data is often available in mixed form,
yielding the need for methods that are able to cope with mixture data and learn
multiple graphs. We propose a novel generative model that represents a
collection of distinct data which naturally live on different graphs. We assume
the mapping of data to graphs is not known and investigate the problem of
jointly clustering a set of data and learning a graph for each of the clusters.
Experiments demonstrate promising performance in data clustering and multiple
graph inference, and show desirable properties in terms of interpretability and
coping with high dimensionality on weather and traffic data, as well as digit
classification."
['stat.ML'],A unified framework for spectral clustering in sparse graphs,"This article considers spectral community detection in the regime of sparse
networks with heterogeneous degree distributions, for which we devise an
algorithm to efficiently retrieve communities. Specifically, we demonstrate
that a conveniently parametrized form of regularized Laplacian matrix can be
used to perform spectral clustering in sparse networks, without suffering from
its degree heterogeneity. Besides, we exhibit important connections between
this proposed matrix and the now popular non-backtracking matrix, the
Bethe-Hessian matrix, as well as the standard Laplacian matrix. Interestingly,
as opposed to competitive methods, our proposed improved parametrization
inherently accounts for the hardness of the classification problem. These
findings are summarized under the form of an algorithm capable of both
estimating the number of communities and achieving high-quality community
reconstruction."
['stat.ML'],UGRWO-Sampling: A modified random walk under-sampling approach based on graphs to imbalanced data classification,"In this paper, we propose a new RWO-Sampling (Random Walk Over-Sampling)
based on graphs for imbalanced datasets. In this method, two figures based on
under-sampling and over-sampling methods are introduced to keep the proximity
information, which is robust to noises and outliers. After the construction of
the first graph on minority class, RWO-Sampling will be implemented on selected
samples, and the rest of them will remain unchanged. The second graph is
constructed for the majority class, and the samples in a low-density area
(outliers) are removed. In the proposed method, examples of the majority class
in a high-density area are selected, and the rest of them are eliminated.
Furthermore, utilizing RWO-sampling, the boundary of minority class is
increased though, the outliers are not raised. This method is tested, and the
number of evaluation measures is compared to previous methods on nine
continuous attribute datasets with different over-sampling rates. The
experimental results were an indicator of the high efficiency and flexibility
of the proposed method for the classification of imbalanced data."
['stat.ML'],Joint Event Extraction along Shortest Dependency Paths using Graph Convolutional Networks,"Event extraction (EE) is one of the core information extraction tasks, whose
purpose is to automatically identify and extract information about incidents
and their actors from texts. This may be beneficial to several domains such as
knowledge bases, question answering, information retrieval and summarization
tasks, to name a few. The problem of extracting event information from texts is
longstanding and usually relies on elaborately designed lexical and syntactic
features, which, however, take a large amount of human effort and lack
generalization. More recently, deep neural network approaches have been adopted
as a means to learn underlying features automatically. However, existing
networks do not make full use of syntactic features, which play a fundamental
role in capturing very long-range dependencies. Also, most approaches extract
each argument of an event separately without considering associations between
arguments which ultimately leads to low efficiency, especially in sentences
with multiple events. To address the two above-referred problems, we propose a
novel joint event extraction framework that aims to extract multiple event
triggers and arguments simultaneously by introducing shortest dependency path
(SDP) in the dependency graph. We do this by eliminating irrelevant words in
the sentence, thus capturing long-range dependencies. Also, an attention-based
graph convolutional network is proposed, to carry syntactically related
information along the shortest paths between argument candidates that captures
and aggregates the latent associations between arguments; a problem that has
been overlooked by most of the literature. Our results show a substantial
improvement over state-of-the-art methods."
['stat.ML'],A comprehensive study on the prediction reliability of graph neural networks for virtual screening,"Prediction models based on deep neural networks are increasingly gaining
attention for fast and accurate virtual screening systems. For decision makings
in virtual screening, researchers find it useful to interpret an output of
classification system as probability, since such interpretation allows them to
filter out more desirable compounds. However, probabilistic interpretation
cannot be correct for models that hold over-parameterization problems or
inappropriate regularizations, leading to unreliable prediction and decision
making. In this regard, we concern the reliability of neural prediction models
on molecular properties, especially when models are trained with sparse data
points and imbalanced distributions. This work aims to propose guidelines for
training reliable models, we thus provide methodological details and ablation
studies on the following train principles. We investigate the effects of model
architectures, regularization methods, and loss functions on the prediction
performance and reliability of classification results. Moreover, we evaluate
prediction reliability of models on virtual screening scenario. Our result
highlights that correct choice of regularization and inference methods is
evidently important to achieve high success rate, especially in data imbalanced
situation. All experiments were performed under a single unified model
implementation to alleviate external randomness in model training and to enable
precise comparison of results."
['stat.ML'],Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human Motion Prediction,"We propose novel dynamic multiscale graph neural networks (DMGNN) to predict
3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale
graph to comprehensively model the internal relations of a human body for
motion feature learning. This multiscale graph is adaptive during training and
dynamic across network layers. Based on this graph, we propose a multiscale
graph computational unit (MGCU) to extract features at individual scales and
fuse features across scales. The entire model is action-category-agnostic and
follows an encoder-decoder framework. The encoder consists of a sequence of
MGCUs to learn motion features. The decoder uses a proposed graph-based gate
recurrent unit to generate future poses. Extensive experiments show that the
proposed DMGNN outperforms state-of-the-art methods in both short and long-term
predictions on the datasets of Human 3.6M and CMU Mocap. We further investigate
the learned multiscale graphs for the interpretability. The codes could be
downloaded from https://github.com/limaosen0/DMGNN."
['stat.ML'],Poincaré Wasserstein Autoencoder,"This work presents a reformulation of the recently proposed Wasserstein
autoencoder framework on a non-Euclidean manifold, the Poincar\'e ball model of
the hyperbolic space. By assuming the latent space to be hyperbolic, we can use
its intrinsic hierarchy to impose structure on the learned latent space
representations. We demonstrate the model in the visual domain to analyze some
of its properties and show competitive results on a graph link prediction task."
['stat.ML'],ActiLabel: A Combinatorial Transfer Learning Framework for Activity Recognition,"Sensor-based human activity recognition has become a critical component of
many emerging applications ranging from behavioral medicine to gaming. However,
an unprecedented increase in the diversity of sensor devices in the
Internet-of-Things era has limited the adoption of activity recognition models
for use across different domains. We propose ActiLabel a combinatorial
framework that learns structural similarities among the events in an arbitrary
domain and those of a different domain. The structural similarities are
captured through a graph model, referred to as the it dependency graph, which
abstracts details of activity patterns in low-level signal and feature space.
The activity labels are then autonomously learned by finding an optimal tiered
mapping between the dependency graphs. Extensive experiments based on three
public datasets demonstrate the superiority of ActiLabel over state-of-the-art
transfer learning and deep learning methods."
['stat.ML'],Tensor Graph Convolutional Networks for Multi-relational and Robust Learning,"The era of ""data deluge"" has sparked renewed interest in graph-based learning
methods and their widespread applications ranging from sociology and biology to
transportation and communications. In this context of graph-aware methods, the
present paper introduces a tensor-graph convolutional network (TGCN) for
scalable semi-supervised learning (SSL) from data associated with a collection
of graphs, that are represented by a tensor. Key aspects of the novel TGCN
architecture are the dynamic adaptation to different relations in the tensor
graph via learnable weights, and the consideration of graph-based regularizers
to promote smoothness and alleviate over-parameterization. The ultimate goal is
to design a powerful learning architecture able to: discover complex and highly
nonlinear data associations, combine (and select) multiple types of relations,
scale gracefully with the graph size, and remain robust to perturbations on the
graph edges. The proposed architecture is relevant not only in applications
where the nodes are naturally involved in different relations (e.g., a
multi-relational graph capturing family, friendship and work relations in a
social network), but also in robust learning setups where the graph entails a
certain level of uncertainty, and the different tensor slabs correspond to
different versions (realizations) of the nominal graph. Numerical tests
showcase that the proposed architecture achieves markedly improved performance
relative to standard GCNs, copes with state-of-the-art adversarial attacks, and
leads to remarkable SSL performance over protein-to-protein interaction
networks."
['stat.ML'],Evaluating Logical Generalization in Graph Neural Networks,"Recent research has highlighted the role of relational inductive biases in
building learning agents that can generalize and reason in a compositional
manner. However, while relational learning algorithms such as graph neural
networks (GNNs) show promise, we do not understand how effectively these
approaches can adapt to new tasks. In this work, we study the task of logical
generalization using GNNs by designing a benchmark suite grounded in
first-order logic. Our benchmark suite, GraphLog, requires that learning
algorithms perform rule induction in different synthetic logics, represented as
knowledge graphs. GraphLog consists of relation prediction tasks on 57 distinct
logical domains. We use GraphLog to evaluate GNNs in three different setups:
single-task supervised learning, multi-task pretraining, and continual
learning. Unlike previous benchmarks, our approach allows us to precisely
control the logical relationship between the different tasks. We find that the
ability for models to generalize and adapt is strongly determined by the
diversity of the logical rules they encounter during training, and our results
highlight new challenges for the design of GNN models. We publicly release the
dataset and code used to generate and interact with the dataset at
https://www.cs.mcgill.ca/~ksinha4/graphlog."
['stat.ML'],Heterogeneous-Temporal Graph Convolutional Networks: Make the Community Detection Much Better,"Community detection has long been an important yet challenging task to
analyze complex networks with a focus on detecting topological structures of
graph data. Essentially, real-world graph data contains various features, node
and edge types which dynamically vary over time, and this invalidates most
existing community detection approaches. To cope with these issues, this paper
proposes the heterogeneous-temporal graph convolutional networks (HTGCN) to
detect communities from hetergeneous and temporal graphs. Particularly, we
first design a heterogeneous GCN component to acquire feature representations
for each heterogeneous graph at each time step. Then, a residual compressed
aggregation component is proposed to represent ""dynamic"" features for ""varying""
communities, which are then aggregated with ""static"" features extracted from
current graph. Extensive experiments are evaluated on two real-world datasets,
i.e., DBLP and IMDB. The promising results demonstrate that the proposed HTGCN
is superior to both benchmark and the state-of-the-art approaches, e.g., GCN,
GAT, GNN, LGNN, HAN and STAR, with respect to a number of evaluation criteria."
['stat.ML'],Deep Learning on Graphs: A Survey,"Deep learning has been shown to be successful in a number of domains, ranging
from acoustics, images, to natural language processing. However, applying deep
learning to the ubiquitous graph data is non-trivial because of the unique
characteristics of graphs. Recently, substantial research efforts have been
devoted to applying deep learning methods to graphs, resulting in beneficial
advances in graph analysis techniques. In this survey, we comprehensively
review the different types of deep learning methods on graphs. We divide the
existing methods into five categories based on their model architectures and
training strategies: graph recurrent neural networks, graph convolutional
networks, graph autoencoders, graph reinforcement learning, and graph
adversarial methods. We then provide a comprehensive overview of these methods
in a systematic manner mainly by following their development history. We also
analyze the differences and compositions of different methods. Finally, we
briefly outline the applications in which they have been used and discuss
potential future research directions."
['stat.ML'],Heterogeneous Relational Reasoning in Knowledge Graphs with Reinforcement Learning,"Path-based relational reasoning over knowledge graphs has become increasingly
popular due to a variety of downstream applications such as question answering
in dialogue systems, fact prediction, and recommender systems. In recent years,
reinforcement learning (RL) has provided solutions that are more interpretable
and explainable than other deep learning models. However, these solutions still
face several challenges, including large action space for the RL agent and
accurate representation of entity neighborhood structure. We address these
problems by introducing a type-enhanced RL agent that uses the local
neighborhood information for efficient path-based reasoning over knowledge
graphs. Our solution uses graph neural network (GNN) for encoding the
neighborhood information and utilizes entity types to prune the action space.
Experiments on real-world dataset show that our method outperforms
state-of-the-art RL methods and discovers more novel paths during the training
procedure."
['stat.ML'],Wasserstein-based Graph Alignment,"We propose a novel method for comparing non-aligned graphs of different
sizes, based on the Wasserstein distance between graph signal distributions
induced by the respective graph Laplacian matrices. Specifically, we cast a new
formulation for the one-to-many graph alignment problem, which aims at matching
a node in the smaller graph with one or more nodes in the larger graph. By
integrating optimal transport in our graph comparison framework, we generate
both a structurally-meaningful graph distance, and a signal transportation plan
that models the structure of graph data. The resulting alignment problem is
solved with stochastic gradient descent, where we use a novel Dykstra operator
to ensure that the solution is a one-to-many (soft) assignment matrix. We
demonstrate the performance of our novel framework on graph alignment and graph
classification, and we show that our method leads to significant improvements
with respect to the state-of-the-art algorithms for each of these tasks."
['stat.ML'],Topological Effects on Attacks Against Vertex Classification,"Vertex classification is vulnerable to perturbations of both graph topology
and vertex attributes, as shown in recent research. As in other machine
learning domains, concerns about robustness to adversarial manipulation can
prevent potential users from adopting proposed methods when the consequence of
action is very high. This paper considers two topological characteristics of
graphs and explores the way these features affect the amount the adversary must
perturb the graph in order to be successful. We show that, if certain vertices
are included in the training set, it is possible to substantially an
adversary's required perturbation budget. On four citation datasets, we
demonstrate that if the training set includes high degree vertices or vertices
that ensure all unlabeled nodes have neighbors in the training set, we show
that the adversary's budget often increases by a substantial factor---often a
factor of 2 or more---over random training for the Nettack poisoning attack.
Even for especially easy targets (those that are misclassified after just one
or two perturbations), the degradation of performance is much slower, assigning
much lower probabilities to the incorrect classes. In addition, we demonstrate
that this robustness either persists when recently proposed defenses are
applied, or is competitive with the resulting performance improvement for the
defender."
['stat.ML'],DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,"\emph{Over-fitting} and \emph{over-smoothing} are two main obstacles of
developing deep Graph Convolutional Networks (GCNs) for node classification. In
particular, over-fitting weakens the generalization ability on small dataset,
while over-smoothing impedes model training by isolating output representations
from the input features with the increase in network depth. This paper proposes
DropEdge, a novel and flexible technique to alleviate both issues. At its core,
DropEdge randomly removes a certain number of edges from the input graph at
each training epoch, acting like a data augmenter and also a message passing
reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces
the convergence speed of over-smoothing or relieves the information loss caused
by it. More importantly, our DropEdge is a general skill that can be equipped
with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for
enhanced performance. Extensive experiments on several benchmarks verify that
DropEdge consistently improves the performance on a variety of both shallow and
deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically
visualized and validated as well. Codes are released
on~\url{https://github.com/DropEdge/DropEdge}."
['stat.ML'],Industrial Scale Privacy Preserving Deep Neural Network,"Deep Neural Network (DNN) has been showing great potential in kinds of
real-world applications such as fraud detection and distress prediction.
Meanwhile, data isolation has become a serious problem currently, i.e.,
different parties cannot share data with each other. To solve this issue, most
research leverages cryptographic techniques to train secure DNN models for
multi-parties without compromising their private data. Although such methods
have strong security guarantee, they are difficult to scale to deep networks
and large datasets due to its high communication and computation complexities.
To solve the scalability of the existing secure Deep Neural Network (DNN) in
data isolation scenarios, in this paper, we propose an industrial scale privacy
preserving neural network learning paradigm, which is secure against
semi-honest adversaries. Our main idea is to split the computation graph of DNN
into two parts, i.e., the computations related to private data are performed by
each party using cryptographic techniques, and the rest computations are done
by a neutral server with high computation ability. We also present a defender
mechanism for further privacy protection. We conduct experiments on real-world
fraud detection dataset and financial distress prediction dataset, the
encouraging results demonstrate the practicalness of our proposal."
['stat.ML'],Leveraging Communication Topologies Between Learning Agents in Deep Reinforcement Learning,"A common technique to improve learning performance in deep reinforcement
learning (DRL) and many other machine learning algorithms is to run multiple
learning agents in parallel. A neglected component in the development of these
algorithms has been how best to arrange the learning agents involved to improve
distributed search. Here we draw upon results from the networked optimization
literatures suggesting that arranging learning agents in communication networks
other than fully connected topologies (the implicit way agents are commonly
arranged in) can improve learning. We explore the relative performance of four
popular families of graphs and observe that one such family (Erdos-Renyi random
graphs) empirically outperforms the de facto fully-connected communication
topology across several DRL benchmark tasks. Additionally, we observe that 1000
learning agents arranged in an Erdos-Renyi graph can perform as well as 3000
agents arranged in the standard fully-connected topology, showing the large
learning improvement possible when carefully designing the topology over which
agents communicate. We complement these empirical results with a theoretical
investigation of why our alternate topologies perform better. Overall, our work
suggests that distributed machine learning algorithms could be made more
effective if the communication topology between learning agents was optimized."
['stat.ML'],Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs,"A common approach to define convolutions on meshes is to interpret them as a
graph and apply graph convolutional networks (GCNs). Such GCNs utilize
isotropic kernels and are therefore insensitive to the relative orientation of
vertices and thus to the geometry of the mesh as a whole. We propose Gauge
Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge
equivariant kernels. Since the resulting features carry orientation
information, we introduce a geometric message passing scheme defined by
parallel transporting features over mesh edges. Our experiments validate the
significantly improved expressivity of the proposed model over conventional
GCNs and other methods."
['stat.ML'],A Comparative Study for Unsupervised Network Representation Learning,"There has been appreciable progress in unsupervised network representation
learning (UNRL) approaches over graphs recently with flexible random-walk
approaches, new optimization objectives and deep architectures. However, there
is no common ground for systematic comparison of embeddings to understand their
behavior for different graphs and tasks. In this paper we theoretically group
different approaches under a unifying framework and empirically investigate the
effectiveness of different network representation methods. In particular, we
argue that most of the UNRL approaches either explicitly or implicit model and
exploit context information of a node. Consequently, we propose a framework
that casts a variety of approaches -- random walk based, matrix factorization
and deep learning based -- into a unified context-based optimization function.
We systematically group the methods based on their similarities and
differences. We study the differences among these methods in detail which we
later use to explain their performance differences (on downstream tasks). We
conduct a large-scale empirical study considering 9 popular and recent UNRL
techniques and 11 real-world datasets with varying structural properties and
two common tasks -- node classification and link prediction. We find that there
is no single method that is a clear winner and that the choice of a suitable
method is dictated by certain properties of the embedding methods, task and
structural properties of the underlying graph. In addition we also report the
common pitfalls in evaluation of UNRL methods and come up with suggestions for
experimental design and interpretation of results."
['stat.ML'],Gaussian Graphical Model exploration and selection in high dimension low sample size setting,"Gaussian Graphical Models (GGM) are often used to describe the conditional
correlations between the components of a random vector. In this article, we
compare two families of GGM inference methods: nodewise edge selection and
penalised likelihood maximisation. We demonstrate on synthetic data that, when
the sample size is small, the two methods produce graphs with either too few or
too many edges when compared to the real one. As a result, we propose a
composite procedure that explores a family of graphs with an nodewise numerical
scheme and selects a candidate among them with an overall likelihood criterion.
We demonstrate that, when the number of observations is small, this selection
method yields graphs closer to the truth and corresponding to distributions
with better KL divergence with regards to the real distribution than the other
two. Finally, we show the interest of our algorithm on two concrete cases:
first on brain imaging data, then on biological nephrology data. In both cases
our results are more in line with current knowledge in each field."
['stat.ML'],Learning Execution through Neural Code Fusion,"As the performance of computer systems stagnates due to the end of Moore's
Law, there is a need for new models that can understand and optimize the
execution of general purpose code. While there is a growing body of work on
using Graph Neural Networks (GNNs) to learn representations of source code,
these representations do not understand how code dynamically executes. In this
work, we propose a new approach to use GNNs to learn fused representations of
general source code and its execution. Our approach defines a multi-task GNN
over low-level representations of source code and program state (i.e., assembly
code and dynamic memory states), converting complex source code constructs and
complex data structures into a simpler, more uniform format. We show that this
leads to improved performance over similar methods that do not use execution
and it opens the door to applying GNN models to new tasks that would not be
feasible from static code alone. As an illustration of this, we apply the new
model to challenging dynamic tasks (branch prediction and prefetching) from the
SPEC CPU benchmark suite, outperforming the state-of-the-art by 26% and 45%
respectively. Moreover, we use the learned fused graph embeddings to
demonstrate transfer learning with high performance on an indirectly related
task (algorithm classification)."
['stat.ML'],Graph Metric Learning via Gershgorin Disc Alignment,"We propose a fast general projection-free metric learning framework, where
the minimization objective $\min_{\textbf{M} \in \mathcal{S}} Q(\textbf{M})$ is
a convex differentiable function of the metric matrix $\textbf{M}$, and
$\textbf{M}$ resides in the set $\mathcal{S}$ of generalized graph Laplacian
matrices for connected graphs with positive edge weights and node degrees.
Unlike low-rank metric matrices common in the literature, $\mathcal{S}$
includes the important positive-diagonal-only matrices as a special case in the
limit. The key idea for fast optimization is to rewrite the positive definite
cone constraint in $\mathcal{S}$ as signal-adaptive linear constraints via
Gershgorin disc alignment, so that the alternating optimization of the diagonal
and off-diagonal terms in $\textbf{M}$ can be solved efficiently as linear
programs via Frank-Wolfe iterations. We prove that the Gershgorin discs can be
aligned perfectly using the first eigenvector $\textbf{v}$ of $\textbf{M}$,
which we update iteratively using Locally Optimal Block Preconditioned
Conjugate Gradient (LOBPCG) with warm start as diagonal / off-diagonal terms
are optimized. Experiments show that our efficiently computed graph metric
matrices outperform metrics learned using competing methods in terms of
classification tasks."
['stat.ML'],Consistency of semi-supervised learning algorithms on graphs: Probit and one-hot methods,"Graph-based semi-supervised learning is the problem of propagating labels
from a small number of labelled data points to a larger set of unlabelled data.
This paper is concerned with the consistency of optimization-based techniques
for such problems, in the limit where the labels have small noise and the
underlying unlabelled data is well clustered. We study graph-based probit for
binary classification, and a natural generalization of this method to
multi-class classification using one-hot encoding. The resulting objective
function to be optimized comprises the sum of a quadratic form defined through
a rational function of the graph Laplacian, involving only the unlabelled data,
and a fidelity term involving only the labelled data. The consistency analysis
sheds light on the choice of the rational function defining the optimization."
['stat.ML'],AdarGCN: Adaptive Aggregation GCN for Few-Shot Learning,"Existing few-shot learning (FSL) methods assume that there exist sufficient
training samples from source classes for knowledge transfer to target classes
with few training samples. However, this assumption is often invalid,
especially when it comes to fine-grained recognition. In this work, we define a
new FSL setting termed few-shot fewshot learning (FSFSL), under which both the
source and target classes have limited training samples. To overcome the source
class data scarcity problem, a natural option is to crawl images from the web
with class names as search keywords. However, the crawled images are inevitably
corrupted by large amount of noise (irrelevant images) and thus may harm the
performance. To address this problem, we propose a graph convolutional network
(GCN)-based label denoising (LDN) method to remove the irrelevant images.
Further, with the cleaned web images as well as the original clean training
images, we propose a GCN-based FSL method. For both the LDN and FSL tasks, a
novel adaptive aggregation GCN (AdarGCN) model is proposed, which differs from
existing GCN models in that adaptive aggregation is performed based on a
multi-head multi-level aggregation module. With AdarGCN, how much and how far
information carried by each graph node is propagated in the graph structure can
be determined automatically, therefore alleviating the effects of both noisy
and outlying training samples. Extensive experiments show the superior
performance of our AdarGCN under both the new FSFSL and the conventional FSL
settings."
['stat.ML'],PersLay: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures,"Persistence diagrams, the most common descriptors of Topological Data
Analysis, encode topological properties of data and have already proved pivotal
in many different applications of data science. However, since the (metric)
space of persistence diagrams is not Hilbert, they end up being difficult
inputs for most Machine Learning techniques. To address this concern, several
vectorization methods have been put forward that embed persistence diagrams
into either finite-dimensional Euclidean space or (implicit) infinite
dimensional Hilbert space with kernels. In this work, we focus on persistence
diagrams built on top of graphs. Relying on extended persistence theory and the
so-called heat kernel signature, we show how graphs can be encoded by
(extended) persistence diagrams in a provably stable way. We then propose a
general and versatile framework for learning vectorizations of persistence
diagrams, which encompasses most of the vectorization techniques used in the
literature. We finally showcase the experimental strength of our setup by
achieving competitive scores on classification tasks on real-life graph
datasets."
"['stat.ML', 'stat.TH']",Learning Some Popular Gaussian Graphical Models without Condition Number Bounds,"Gaussian Graphical Models (GGMs) have wide-ranging applications in machine
learning and the natural and social sciences. In most of the settings in which
they are applied, the number of observed samples is much smaller than the
dimension and they are assumed to be sparse. While there are a variety of
algorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph
structure with a logarithmic number of samples, they assume various conditions
that require the precision matrix to be in some sense well-conditioned.
  Here we give the first polynomial-time algorithms for learning attractive
GGMs and walk-summable GGMs with a logarithmic number of samples without any
such assumptions. In particular, our algorithms can tolerate strong
dependencies among the variables. Our result for structure recovery in
walk-summable GGMs is derived from a more general result for efficient sparse
linear regression in walk-summable models without any norm dependencies. We
complement our results with experiments showing that many existing algorithms
fail even in some simple settings where there are long dependency chains,
whereas ours do not."
['stat.ML'],Solving NP-Hard Problems on Graphs with Extended AlphaGo Zero,"There have been increasing challenges to solve combinatorial optimization
problems by machine learning. Khalil et al. proposed an end-to-end
reinforcement learning framework, S2V-DQN, which automatically learns graph
embeddings to construct solutions to a wide range of problems. To improve the
generalization ability of their Q-learning method, we propose a novel learning
strategy based on AlphaGo Zero which is a Go engine that achieved a superhuman
level without the domain knowledge of the game. Our framework is redesigned for
combinatorial problems, where the final reward might take any real number
instead of a binary response, win/lose. In experiments conducted for five kinds
of NP-hard problems including {\sc MinimumVertexCover} and {\sc MaxCut}, our
method is shown to generalize better to various graphs than S2V-DQN.
Furthermore, our method can be combined with recently-developed graph neural
network (GNN) models such as the \emph{Graph Isomorphism Network}, resulting in
even better performance. This experiment also gives an interesting insight into
a suitable choice of GNN models for each task."
['stat.ML'],Neural Operator: Graph Kernel Network for Partial Differential Equations,"The classical development of neural networks has been primarily for mappings
between a finite-dimensional Euclidean space and a set of classes, or between
two finite-dimensional Euclidean spaces. The purpose of this work is to
generalize neural networks so that they can learn mappings between
infinite-dimensional spaces (operators). The key innovation in our work is that
a single set of network parameters, within a carefully designed network
architecture, may be used to describe mappings between infinite-dimensional
spaces and between different finite-dimensional approximations of those spaces.
We formulate approximation of the infinite-dimensional mapping by composing
nonlinear activation functions and a class of integral operators. The kernel
integration is computed by message passing on graph networks. This approach has
substantial practical consequences which we will illustrate in the context of
mappings between input data to partial differential equations (PDEs) and their
solutions. In this context, such learned networks can generalize among
different approximation methods for the PDE (such as finite difference or
finite element methods) and among approximations corresponding to different
underlying levels of resolution and discretization. Experiments confirm that
the proposed graph kernel network does have the desired properties and show
competitive performance compared to the state of the art solvers."
['stat.ML'],Directional Message Passing for Molecular Graphs,"Graph neural networks have recently achieved great successes in predicting
quantum mechanical properties of molecules. These models represent a molecule
as a graph using only the distance between atoms (nodes). They do not, however,
consider the spatial direction from one atom to another, despite directional
information playing a central role in empirical potentials for molecules, e.g.
in angular potentials. To alleviate this limitation we propose directional
message passing, in which we embed the messages passed between atoms instead of
the atoms themselves. Each message is associated with a direction in coordinate
space. These directional message embeddings are rotationally equivariant since
the associated directions rotate with the molecule. We propose a message
passing scheme analogous to belief propagation, which uses the directional
information by transforming messages based on the angle between them.
Additionally, we use spherical Bessel functions and spherical harmonics to
construct theoretically well-founded, orthogonal representations that achieve
better performance than the currently prevalent Gaussian radial basis
representations while using fewer than 1/4 of the parameters. We leverage these
innovations to construct the directional message passing neural network
(DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by
31% on QM9. Our implementation is available online."
['stat.ML'],Factorized Graph Representations for Semi-Supervised Learning from Sparse Data,"Node classification is an important problem in graph data management. It is
commonly solved by various label propagation methods that work iteratively
starting from a few labeled seed nodes. For graphs with arbitrary
compatibilities between classes, these methods crucially depend on knowing the
compatibility matrix that must be provided by either domain experts or
heuristics. Can we instead directly estimate the correct compatibilities from a
sparsely labeled graph in a principled and scalable way? We answer this
question affirmatively and suggest a method called distant compatibility
estimation that works even on extremely sparsely labeled graphs (e.g., 1 in
10,000 nodes is labeled) in a fraction of the time it later takes to label the
remaining nodes. Our approach first creates multiple factorized graph
representations (with size independent of the graph) and then performs
estimation on these smaller graph sketches. We define algebraic amplification
as the more general idea of leveraging algebraic properties of an algorithm's
update equations to amplify sparse signals. We show that our estimator is by
orders of magnitude faster than an alternative approach and that the end-to-end
classification accuracy is comparable to using gold standard compatibilities.
This makes it a cheap preprocessing step for any existing label propagation
method and removes the current dependence on heuristics."
['stat.ML'],Cross-GCN: Enhancing Graph Convolutional Network with $k$-Order Feature Interactions,"Graph Convolutional Network (GCN) is an emerging technique that performs
learning and reasoning on graph data. It operates feature learning on the graph
structure, through aggregating the features of the neighbor nodes to obtain the
embedding of each target node. Owing to the strong representation power, recent
research shows that GCN achieves state-of-the-art performance on several tasks
such as recommendation and linked document classification.
  Despite its effectiveness, we argue that existing designs of GCN forgo
modeling cross features, making GCN less effective for tasks or data where
cross features are important. Although neural network can approximate any
continuous function, including the multiplication operator for modeling feature
crosses, it can be rather inefficient to do so (i.e., wasting many parameters
at the risk of overfitting) if there is no explicit design.
  To this end, we design a new operator named Cross-feature Graph Convolution,
which explicitly models the arbitrary-order cross features with complexity
linear to feature dimension and order size. We term our proposed architecture
as Cross-GCN, and conduct experiments on three graphs to validate its
effectiveness. Extensive analysis validates the utility of explicitly modeling
cross features in GCN, especially for feature learning at lower layers."
['stat.ML'],"Convolution, attention and structure embedding","Deep neural networks are composed of layers of parametrised linear operations
intertwined with non linear activations. In basic models, such as the
multi-layer perceptron, a linear layer operates on a simple input vector
embedding of the instance being processed, and produces an output vector
embedding by straight multiplication by a matrix parameter. In more complex
models, the input and output are structured and their embeddings are higher
order tensors. The parameter of each linear operation must then be controlled
so as not to explode with the complexity of the structures involved. This is
essentially the role of convolution models, which exist in many flavours
dependent on the type of structure they deal with (grids, networks, time series
etc.). We present here a unified framework which aims at capturing the essence
of these diverse models, allowing a systematic analysis of their properties and
their mutual enrichment. We also show that attention models naturally fit in
the same framework: attention is convolution in which the structure itself is
adaptive, and learnt, instead of being given a priori."
['stat.ML'],Semi-supervised Learning Meets Factorization: Learning to Recommend with Chain Graph Model,"Recently latent factor model (LFM) has been drawing much attention in
recommender systems due to its good performance and scalability. However,
existing LFMs predict missing values in a user-item rating matrix only based on
the known ones, and thus the sparsity of the rating matrix always limits their
performance. Meanwhile, semi-supervised learning (SSL) provides an effective
way to alleviate the label (i.e., rating) sparsity problem by performing label
propagation, which is mainly based on the smoothness insight on affinity
graphs. However, graph-based SSL suffers serious scalability and graph
unreliable problems when directly being applied to do recommendation. In this
paper, we propose a novel probabilistic chain graph model (CGM) to marry SSL
with LFM. The proposed CGM is a combination of Bayesian network and Markov
random field. The Bayesian network is used to model the rating generation and
regression procedures, and the Markov random field is used to model the
confidence-aware smoothness constraint between the generated ratings.
Experimental results show that our proposed CGM significantly outperforms the
state-of-the-art approaches in terms of four evaluation metrics, and with a
larger performance margin when data sparsity increases."
['stat.ML'],Graphon Pooling in Graph Neural Networks,"Graph neural networks (GNNs) have been used effectively in different
applications involving the processing of signals on irregular structures
modeled by graphs. Relying on the use of shift-invariant graph filters, GNNs
extend the operation of convolution to graphs. However, the operations of
pooling and sampling are still not clearly defined and the approaches proposed
in the literature either modify the graph structure in a way that does not
preserve its spectral properties, or require defining a policy for selecting
which nodes to keep. In this work, we propose a new strategy for pooling and
sampling on GNNs using graphons which preserves the spectral properties of the
graph. To do so, we consider the graph layers in a GNN as elements of a
sequence of graphs that converge to a graphon. In this way we have no ambiguity
in the node labeling when mapping signals from one layer to the other and a
spectral representation that is consistent throughout the layers. We evaluate
this strategy in a synthetic and a real-world numerical experiment where we
show that graphon pooling GNNs are less prone to overfitting and improve upon
other pooling techniques, especially when the dimensionality reduction ratios
between layers is large."
['stat.ML'],Self-Supervised Graph Representation Learning via Global Context Prediction,"To take full advantage of fast-growing unlabeled networked data, this paper
introduces a novel self-supervised strategy for graph representation learning
by exploiting natural supervision provided by the data itself. Inspired by
human social behavior, we assume that the global context of each node is
composed of all nodes in the graph since two arbitrary entities in a connected
network could interact with each other via paths of varying length. Based on
this, we investigate whether the global context can be a source of free and
effective supervisory signals for learning useful node representations.
Specifically, we randomly select pairs of nodes in a graph and train a
well-designed neural net to predict the contextual position of one node
relative to the other. Our underlying hypothesis is that the representations
learned from such within-graph context would capture the global topology of the
graph and finely characterize the similarity and differentiation between nodes,
which is conducive to various downstream learning tasks. Extensive benchmark
experiments including node classification, clustering, and link prediction
demonstrate that our approach outperforms many state-of-the-art unsupervised
methods and sometimes even exceeds the performance of supervised counterparts."
['stat.ML'],Differentiable Causal Backdoor Discovery,"Discovering the causal effect of a decision is critical to nearly all forms
of decision-making. In particular, it is a key quantity in drug development, in
crafting government policy, and when implementing a real-world machine learning
system. Given only observational data, confounders often obscure the true
causal effect. Luckily, in some cases, it is possible to recover the causal
effect by using certain observed variables to adjust for the effects of
confounders. However, without access to the true causal model, finding this
adjustment requires brute-force search. In this work, we present an algorithm
that exploits auxiliary variables, similar to instruments, in order to find an
appropriate adjustment by a gradient-based optimization method. We demonstrate
that it outperforms practical alternatives in estimating the true causal
effect, without knowledge of the full causal graph."
['stat.ML'],Learn to Generate Time Series Conditioned Graphs with Generative Adversarial Nets,"Deep learning based approaches have been utilized to model and generate
graphs subjected to different distributions recently. However, they are
typically unsupervised learning based and unconditioned generative models or
simply conditioned on the graph-level contexts, which are not associated with
rich semantic node-level contexts. Differently, in this paper, we are
interested in a novel problem named Time Series Conditioned Graph Generation:
given an input multivariate time series, we aim to infer a target relation
graph modeling the underlying interrelationships between time series with each
node corresponding to each time series. For example, we can study the
interrelationships between genes in a gene regulatory network of a certain
disease conditioned on their gene expression data recorded as time series. To
achieve this, we propose a novel Time Series conditioned Graph
Generation-Generative Adversarial Networks (TSGG-GAN) to handle challenges of
rich node-level context structures conditioning and measuring similarities
directly between graphs and time series. Extensive experiments on synthetic and
real-word gene regulatory networks datasets demonstrate the effectiveness and
generalizability of the proposed TSGG-GAN."
['stat.ML'],Learning Context-aware Task Reasoning for Efficient Meta-reinforcement Learning,"Despite recent success of deep network-based Reinforcement Learning (RL), it
remains elusive to achieve human-level efficiency in learning novel tasks.
While previous efforts attempt to address this challenge using meta-learning
strategies, they typically suffer from sampling inefficiency with on-policy RL
algorithms or meta-overfitting with off-policy learning. In this work, we
propose a novel meta-RL strategy to address those limitations. In particular,
we decompose the meta-RL problem into three sub-tasks, task-exploration,
task-inference and task-fulfillment, instantiated with two deep network agents
and a task encoder. During meta-training, our method learns a task-conditioned
actor network for task-fulfillment, an explorer network with a self-supervised
reward shaping that encourages task-informative experiences in
task-exploration, and a context-aware graph-based task encoder for task
inference. We validate our approach with extensive experiments on several
public benchmarks and the results show that our algorithm effectively performs
exploration for task inference, improves sample efficiency during both training
and testing, and mitigates the meta-overfitting problem."
['stat.ML'],Heterogeneous Graph Transformer,"Recent years have witnessed the emerging success of graph neural networks
(GNNs) for modeling structured data. However, most GNNs are designed for
homogeneous graphs, in which all nodes and edges belong to the same types,
making them infeasible to represent heterogeneous structures. In this paper, we
present the Heterogeneous Graph Transformer (HGT) architecture for modeling
Web-scale heterogeneous graphs. To model heterogeneity, we design node- and
edge-type dependent parameters to characterize the heterogeneous attention over
each edge, empowering HGT to maintain dedicated representations for different
types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce
the relative temporal encoding technique into HGT, which is able to capture the
dynamic structural dependency with arbitrary durations. To handle Web-scale
graph data, we design the heterogeneous mini-batch graph sampling
algorithm---HGSampling---for efficient and scalable training. Extensive
experiments on the Open Academic Graph of 179 million nodes and 2 billion edges
show that the proposed HGT model consistently outperforms all the
state-of-the-art GNN baselines by 9%--21% on various downstream tasks."
['stat.ML'],Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees,"Graph representation learning is a ubiquitous task in machine learning where
the goal is to embed each vertex into a low-dimensional vector space. We
consider the bipartite graph and formalize its representation learning problem
as a statistical estimation problem of parameters in a semiparametric
exponential family distribution. The bipartite graph is assumed to be generated
by a semiparametric exponential family distribution, whose parametric component
is given by the proximity of outputs of two one-layer neural networks, while
nonparametric (nuisance) component is the base measure. Neural networks take
high-dimensional features as inputs and output embedding vectors. In this
setting, the representation learning problem is equivalent to recovering the
weight matrices. The main challenges of estimation arise from the nonlinearity
of activation functions and the nonparametric nuisance component of the
distribution. To overcome these challenges, we propose a pseudo-likelihood
objective based on the rank-order decomposition technique and focus on its
local geometry. We show that the proposed objective is strongly convex in a
neighborhood around the ground truth, so that a gradient descent-based method
achieves linear convergence rate. Moreover, we prove that the sample complexity
of the problem is linear in dimensions (up to logarithmic factors), which is
consistent with parametric Gaussian models. However, our estimator is robust to
any model misspecification within the exponential family, which is validated in
extensive experiments."
['stat.ML'],EvoNet: A Neural Network for Predicting the Evolution of Dynamic Graphs,"Neural networks for structured data like graphs have been studied extensively
in recent years. To date, the bulk of research activity has focused mainly on
static graphs. However, most real-world networks are dynamic since their
topology tends to change over time. Predicting the evolution of dynamic graphs
is a task of high significance in the area of graph mining. Despite its
practical importance, the task has not been explored in depth so far, mainly
due to its challenging nature. In this paper, we propose a model that predicts
the evolution of dynamic graphs. Specifically, we use a graph neural network
along with a recurrent architecture to capture the temporal evolution patterns
of dynamic graphs. Then, we employ a generative model which predicts the
topology of the graph at the next time step and constructs a graph instance
that corresponds to that topology. We evaluate the proposed model on several
artificial datasets following common network evolving dynamics, as well as on
real-world datasets. Results demonstrate the effectiveness of the proposed
model."
['stat.ML'],Permutation Invariant Graph Generation via Score-Based Generative Modeling,"Learning generative models for graph-structured data is challenging because
graphs are discrete, combinatorial, and the underlying data distribution is
invariant to the ordering of nodes. However, most of the existing generative
models for graphs are not invariant to the chosen ordering, which might lead to
an undesirable bias in the learned distribution. To address this difficulty, we
propose a permutation invariant approach to modeling graphs, using the recent
framework of score-based generative modeling. In particular, we design a
permutation equivariant, multi-channel graph neural network to model the
gradient of the data distribution at the input graph (a.k.a., the score
function). This permutation equivariant model of gradients implicitly defines a
permutation invariant distribution for graphs. We train this graph neural
network with score matching and sample from it with annealed Langevin dynamics.
In our experiments, we first demonstrate the capacity of this new architecture
in learning discrete graph algorithms. For graph generation, we find that our
learning approach achieves better or comparable results to existing models on
benchmark datasets."
['stat.ML'],Meta-Graph: Few Shot Link Prediction via Meta Learning,"We consider the task of few shot link prediction on graphs. The goal is to
learn from a distribution over graphs so that a model is able to quickly infer
missing edges in a new graph after a small amount of training. We show that
current link prediction methods are generally ill-equipped to handle this task.
They cannot effectively transfer learned knowledge from one graph to another
and are unable to effectively learn from sparse samples of edges. To address
this challenge, we introduce a new gradient-based meta learning framework,
Meta-Graph. Our framework leverages higher-order gradients along with a learned
graph signature function that conditionally generates a graph neural network
initialization. Using a novel set of few shot link prediction benchmarks, we
show that Meta-Graph can learn to quickly adapt to a new graph using only a
small sample of true edges, enabling not only fast adaptation but also improved
results at convergence."
['stat.ML'],SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,"Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity computation, such as Graph Edit Distance (GED) and Maximum
Common Subgraph (MCS), is the core operation of graph similarity search and
many other applications, but very costly to compute in practice. Inspired by
the recent success of neural network approaches to several graph applications,
such as node or graph classification, we propose a novel neural network based
approach to address this classic yet challenging graph problem, aiming to
alleviate the computational burden while preserving a good performance.
  The proposed approach, called SimGNN, combines two strategies. First, we
design a learnable embedding function that maps every graph into a vector,
which provides a global summary of a graph. A novel attention mechanism is
proposed to emphasize the important nodes with respect to a specific similarity
metric. Second, we design a pairwise node comparison method to supplement the
graph-level embeddings with fine-grained node-level information. Our model
achieves better generalization on unseen graphs, and in the worst case runs in
quadratic time with respect to the number of nodes in two graphs. Taking GED
computation as an example, experimental results on three real graph datasets
demonstrate the effectiveness and efficiency of our approach. Specifically, our
model achieves smaller error rate and great time reduction compared against a
series of baselines, including several approximation algorithms on GED
computation, and many existing graph neural network based models. To the best
of our knowledge, we are among the first to adopt neural networks to explicitly
model the similarity between two graphs, and provide a new direction for future
research on graph similarity computation and graph similarity search."
['stat.ML'],Ego-based Entropy Measures for Structural Representations,"In complex networks, nodes that share similar structural characteristics
often exhibit similar roles (e.g type of users in a social network or the
hierarchical position of employees in a company). In order to leverage this
relationship, a growing literature proposed latent representations that
identify structurally equivalent nodes. However, most of the existing methods
require high time and space complexity. In this paper, we propose VNEstruct, a
simple approach for generating low-dimensional structural node embeddings, that
is both time efficient and robust to perturbations of the graph structure. The
proposed approach focuses on the local neighborhood of each node and employs
the Von Neumann entropy, an information-theoretic tool, to extract features
that capture the neighborhood's topology. Moreover, on graph classification
tasks, we suggest the utilization of the generated structural embeddings for
the transformation of an attributed graph structure into a set of augmented
node attributes. Empirically, we observe that the proposed approach exhibits
robustness on structural role identification tasks and state-of-the-art
performance on graph classification tasks, while maintaining very high
computational speed."
['stat.ML'],From Node Embedding To Community Embedding : A Hyperbolic Approach,"Detecting communities on graphs has received significant interest in recent
literature. Current state-of-the-art community embedding approach called
\textit{ComE} tackles this problem by coupling graph embedding with community
detection. Considering the success of hyperbolic representations of
graph-structured data in last years, an ongoing challenge is to set up a
hyperbolic approach for the community detection problem. The present paper
meets this challenge by introducing a Riemannian equivalent of \textit{ComE}.
Our proposed approach combines hyperbolic embeddings with Riemannian K-means or
Riemannian mixture models to perform community detection. We illustrate the
usefulness of this framework through several experiments on real-world social
networks and comparisons with \textit{ComE} and recent hyperbolic-based
classification approaches."
['stat.ML'],Towards a predictive spatio-temporal representation of brain data,"The characterisation of the brain as a ""connectome"", in which the connections
are represented by correlational values across timeseries and as summary
measures derived from graph theory analyses, has been very popular in the last
years. However, although this representation has advanced our understanding of
the brain function, it may represent an oversimplified model. This is because
the typical fMRI datasets are constituted by complex and highly heterogeneous
timeseries that vary across space (i.e., location of brain regions). We compare
various modelling techniques from deep learning and geometric deep learning to
pave the way for future research in effectively leveraging the rich spatial and
temporal domains of typical fMRI datasets, as well as of other similar
datasets. As a proof-of-concept, we compare our approaches in the homogeneous
and publicly available Human Connectome Project (HCP) dataset on a supervised
binary classification task. We hope that our methodological advances relative
to previous ""connectomic"" measures can ultimately be clinically and
computationally relevant by leading to a more nuanced understanding of the
brain dynamics in health and disease. Such understanding of the brain can
fundamentally reduce the constant specialised clinical expertise in order to
accurately understand brain variability."
['stat.ML'],Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings,"Answering complex logical queries on large-scale incomplete knowledge graphs
(KGs) is a fundamental yet challenging task. Recently, a promising approach to
this problem has been to embed KG entities as well as the query into a vector
space such that entities that answer the query are embedded close to the query.
However, prior work models queries as single points in the vector space, which
is problematic because a complex query represents a potentially large set of
its answer entities, but it is unclear how such a set can be represented as a
single point. Furthermore, prior work can only handle queries that use
conjunctions ($\wedge$) and existential quantifiers ($\exists$). Handling
queries with logical disjunctions ($\vee$) remains an open problem. Here we
propose query2box, an embedding-based framework for reasoning over arbitrary
queries with $\wedge$, $\vee$, and $\exists$ operators in massive and
incomplete KGs. Our main insight is that queries can be embedded as boxes
(i.e., hyper-rectangles), where a set of points inside the box corresponds to a
set of answer entities of the query. We show that conjunctions can be naturally
represented as intersections of boxes and also prove a negative result that
handling disjunctions would require embedding with dimension proportional to
the number of KG entities. However, we show that by transforming queries into a
Disjunctive Normal Form, query2box is capable of handling arbitrary logical
queries with $\wedge$, $\vee$, $\exists$ in a scalable manner. We demonstrate
the effectiveness of query2box on three large KGs and show that query2box
achieves up to 25% relative improvement over the state of the art."
['stat.ML'],Rep the Set: Neural Networks for Learning Set Representations,"In several domains, data objects can be decomposed into sets of simpler
objects. It is then natural to represent each object as the set of its
components or parts. Many conventional machine learning algorithms are unable
to process this kind of representations, since sets may vary in cardinality and
elements lack a meaningful ordering. In this paper, we present a new neural
network architecture, called RepSet, that can handle examples that are
represented as sets of vectors. The proposed model computes the correspondences
between an input set and some hidden sets by solving a series of network flow
problems. This representation is then fed to a standard neural network
architecture to produce the output. The architecture allows end-to-end
gradient-based learning. We demonstrate RepSet on classification tasks,
including text categorization, and graph classification, and we show that the
proposed neural network achieves performance better or comparable to
state-of-the-art algorithms."
['stat.ML'],A Deep Generative Model for Fragment-Based Molecule Generation,"Molecule generation is a challenging open problem in cheminformatics.
Currently, deep generative approaches addressing the challenge belong to two
broad categories, differing in how molecules are represented. One approach
encodes molecular graphs as strings of text, and learns their corresponding
character-based language model. Another, more expressive, approach operates
directly on the molecular graph. In this work, we address two limitations of
the former: generation of invalid and duplicate molecules. To improve validity
rates, we develop a language model for small molecular substructures called
fragments, loosely inspired by the well-known paradigm of Fragment-Based Drug
Design. In other words, we generate molecules fragment by fragment, instead of
atom by atom. To improve uniqueness rates, we present a frequency-based masking
strategy that helps generate molecules with infrequent fragments. We show
experimentally that our model largely outperforms other language model-based
competitors, reaching state-of-the-art performances typical of graph-based
approaches. Moreover, generated molecules display molecular properties similar
to those in the training sample, even in absence of explicit task-specific
supervision."
['stat.ML'],AutoSF: Searching Scoring Functions for Knowledge Graph Embedding,"Scoring functions (SFs), which measure the plausibility of triplets in
knowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which
target at capturing different kinds of relations in KGs, have been designed by
humans in recent years. However, as relations can exhibit complex patterns that
are hard to infer before training, none of them can consistently perform better
than others on existing benchmark data sets. In this paper, inspired by the
recent success of automated machine learning (AutoML), we propose to
automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques.
However, it is non-trivial to explore domain-specific information here to make
AutoSF efficient and effective. We firstly identify a unified representation
over popularly used SFs, which helps to set up a search space for AutoSF. Then,
we propose a greedy algorithm to search in such a space efficiently. The
algorithm is further sped up by a filter and a predictor, which can avoid
repeatedly training SFs with same expressive ability and help removing bad
candidates during the search before model training. Finally, we perform
extensive experiments on benchmark data sets. Results on link prediction and
triplets classification show that the searched SFs by AutoSF, are KG dependent,
new to the literature, and outperform the state-of-the-art SFs designed by
humans."
['stat.ML'],Advances in Collaborative Filtering and Ranking,"In this dissertation, we cover some recent advances in collaborative
filtering and ranking. In chapter 1, we give a brief introduction of the
history and the current landscape of collaborative filtering and ranking;
chapter 2 we first talk about pointwise collaborative filtering problem with
graph information, and how our proposed new method can encode very deep graph
information which helps four existing graph collaborative filtering algorithms;
chapter 3 is on the pairwise approach for collaborative ranking and how we
speed up the algorithm to near-linear time complexity; chapter 4 is on the new
listwise approach for collaborative ranking and how the listwise approach is a
better choice of loss for both explicit and implicit feedback over pointwise
and pairwise loss; chapter 5 is about the new regularization technique
Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it
is both theoretically sound and empirically effectively for 6 different tasks
across recommendation and natural language processing; chapter 6 is how we
introduce personalization for the state-of-the-art sequential recommendation
model with the help of SSE, which plays an important role in preventing our
personalized model from overfitting to the training data; chapter 7, we
summarize what we have achieved so far and predict what the future directions
can be; chapter 8 is the appendix to all the chapters."
['stat.ML'],Heterogeneous Graph Neural Networks for Malicious Account Detection,"We present, GEM, the first heterogeneous graph neural network approach for
detecting malicious accounts at Alipay, one of the world's leading mobile
cashless payment platform. Our approach, inspired from a connected subgraph
approach, adaptively learns discriminative embeddings from heterogeneous
account-device graphs based on two fundamental weaknesses of attackers, i.e.
device aggregation and activity aggregation. For the heterogeneous graph
consists of various types of nodes, we propose an attention mechanism to learn
the importance of different types of nodes, while using the sum operator for
modeling the aggregation patterns of nodes in each type. Experiments show that
our approaches consistently perform promising results compared with competitive
methods over time."
['stat.ML'],Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures,"We propose to study the problem of few shot graph classification in graph
neural networks (GNNs) to recognize unseen classes, given limited labeled graph
examples. Despite several interesting GNN variants being proposed recently for
node and graph classification tasks, when faced with scarce labeled examples in
the few shot setting, these GNNs exhibit significant loss in classification
performance. Here, we present an approach where a probability measure is
assigned to each graph based on the spectrum of the graphs normalized
Laplacian. This enables us to accordingly cluster the graph base labels
associated with each graph into super classes, where the Lp Wasserstein
distance serves as our underlying distance metric. Subsequently, a super graph
constructed based on the super classes is then fed to our proposed GNN
framework which exploits the latent inter class relationships made explicit by
the super graph to achieve better class label separation among the graphs. We
conduct exhaustive empirical evaluations of our proposed method and show that
it outperforms both the adaptation of state of the art graph classification
methods to few shot scenario and our naive baseline GNNs. Additionally, we also
extend and study the behavior of our method to semi supervised and active
learning scenarios."
['stat.ML'],Semi-supervised Anomaly Detection on Attributed Graphs,"We propose a simple yet effective method for detecting anomalous instances on
an attribute graph with label information of a small number of instances.
Although with standard anomaly detection methods it is usually assumed that
instances are independent and identically distributed, in many real-world
applications, instances are often explicitly connected with each other,
resulting in so-called attributed graphs. The proposed method embeds nodes
(instances) on the attributed graph in the latent space by taking into account
their attributes as well as the graph structure based on graph convolutional
networks (GCNs). To learn node embeddings specialized for anomaly detection, in
which there is a class imbalance due to the rarity of anomalies, the parameters
of a GCN are trained to minimize the volume of a hypersphere that encloses the
node embeddings of normal instances while embedding anomalous ones outside the
hypersphere. This enables us to detect anomalies by simply calculating the
distances between the node embeddings and hypersphere center. The proposed
method can effectively propagate label information on a small amount of nodes
to unlabeled ones by taking into account the node's attributes, graph
structure, and class imbalance. In experiments with five real-world attributed
graph datasets, we demonstrate that the proposed method achieves better
performance than various existing anomaly detection methods."
['stat.ML'],GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation,"Molecular graph generation is a fundamental problem for drug discovery and
has been attracting growing attention. The problem is challenging since it
requires not only generating chemically valid molecular structures but also
optimizing their chemical properties in the meantime. Inspired by the recent
progress in deep generative models, in this paper we propose a flow-based
autoregressive model for graph generation called GraphAF. GraphAF combines the
advantages of both autoregressive and flow-based approaches and enjoys: (1)
high model flexibility for data density estimation; (2) efficient parallel
computation for training; (3) an iterative sampling process, which allows
leveraging chemical domain knowledge for valency checking. Experimental results
show that GraphAF is able to generate 68% chemically valid molecules even
without chemical knowledge rules and 100% valid molecules with chemical rules.
The training process of GraphAF is two times faster than the existing
state-of-the-art approach GCPN. After fine-tuning the model for goal-directed
property optimization with reinforcement learning, GraphAF achieves
state-of-the-art performance on both chemical property optimization and
constrained property optimization."
['stat.ML'],Kernel Bi-Linear Modeling for Reconstructing Data on Manifolds: The Dynamic-MRI Case,"This paper establishes a kernel-based framework for reconstructing data on
manifolds, tailored to fit the dynamic-(d)MRI-data recovery problem. The
proposed methodology exploits simple tangent-space geometries of manifolds in
reproducing kernel Hilbert spaces and follows classical kernel-approximation
arguments to form the data-recovery task as a bi-linear inverse problem.
Departing from mainstream approaches, the proposed methodology uses no training
data, employs no graph Laplacian matrix to penalize the optimization task, uses
no costly (kernel) pre-imaging step to map feature points back to the input
space, and utilizes complex-valued kernel functions to account for k-space
data. The framework is validated on synthetically generated dMRI data, where
comparisons against state-of-the-art schemes highlight the rich potential of
the proposed approach in data-recovery problems."
"['stat.ML', 'stat.TH']",On the Sample Complexity of Learning Sum-Product Networks,"Sum-Product Networks (SPNs) can be regarded as a form of deep graphical
models that compactly represent deeply factored and mixed distributions. An SPN
is a rooted directed acyclic graph (DAG) consisting of a set of leaves
(corresponding to base distributions), a set of sum nodes (which represent
mixtures of their children distributions) and a set of product nodes
(representing the products of its children distributions).
  In this work, we initiate the study of the sample complexity of PAC-learning
the set of distributions that correspond to SPNs. We show that the sample
complexity of learning tree structured SPNs with the usual type of leaves
(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)
with the number of parameters of the SPN. More specifically, we show that the
class of distributions that corresponds to tree structured Gaussian SPNs with
$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned
within Total Variation error $\epsilon$ using at most
$\widetilde{O}(\frac{ed^2+k}{\epsilon^2})$ samples. A similar result holds for
tree structured SPNs with discrete leaves.
  We obtain the upper bounds based on the recently proposed notion of
distribution compression schemes. More specifically, we show that if a (base)
class of distributions $\mathcal{F}$ admits an ""efficient"" compression, then
the class of tree structured SPNs with leaves from $\mathcal{F}$ also admits an
efficient compression."
['stat.ML'],Transferring Robustness for Graph Neural Network Against Poisoning Attacks,"Graph neural networks (GNNs) are widely used in many applications. However,
their robustness against adversarial attacks is criticized. Prior studies show
that using unnoticeable modifications on graph topology or nodal features can
significantly reduce the performances of GNNs. It is very challenging to design
robust graph neural networks against poisoning attack and several efforts have
been taken. Existing work aims at reducing the negative impact from adversarial
edges only with the poisoned graph, which is sub-optimal since they fail to
discriminate adversarial edges from normal ones. On the other hand, clean
graphs from similar domains as the target poisoned graph are usually available
in the real world. By perturbing these clean graphs, we create supervised
knowledge to train the ability to detect adversarial edges so that the
robustness of GNNs is elevated. However, such potential for clean graphs is
neglected by existing work. To this end, we investigate a novel problem of
improving the robustness of GNNs against poisoning attacks by exploring clean
graphs. Specifically, we propose PA-GNN, which relies on a penalized
aggregation mechanism that directly restrict the negative impact of adversarial
edges by assigning them lower attention coefficients. To optimize PA-GNN for a
poisoned graph, we design a meta-optimization algorithm that trains PA-GNN to
penalize perturbations using clean graphs and their adversarial counterparts,
and transfers such ability to improve the robustness of PA-GNN on the poisoned
graph. Experimental results on four real-world datasets demonstrate the
robustness of PA-GNN against poisoning attacks on graphs. Code and data are
available here: https://github.com/tangxianfeng/PA-GNN."
['stat.ML'],Infinitely Wide Graph Convolutional Networks: Semi-supervised Learning via Gaussian Processes,"Graph convolutional neural networks~(GCNs) have recently demonstrated
promising results on graph-based semi-supervised classification, but little
work has been done to explore their theoretical properties. Recently, several
deep neural networks, e.g., fully connected and convolutional neural networks,
with infinite hidden units have been proved to be equivalent to Gaussian
processes~(GPs). To exploit both the powerful representational capacity of GCNs
and the great expressive power of GPs, we investigate similar properties of
infinitely wide GCNs. More specifically, we propose a GP regression model via
GCNs~(GPGC) for graph-based semi-supervised learning. In the process, we
formulate the kernel matrix computation of GPGC in an iterative analytical
form. Finally, we derive a conditional distribution for the labels of
unobserved nodes based on the graph structure, labels for the observed nodes,
and the feature matrix of all the nodes. We conduct extensive experiments to
evaluate the semi-supervised classification performance of GPGC and demonstrate
that it outperforms other state-of-the-art methods by a clear margin on all the
datasets while being efficient."
['stat.ML'],Towards an Efficient and General Framework of Robust Training for Graph Neural Networks,"Graph Neural Networks (GNNs) have made significant advances on several
fundamental inference tasks. As a result, there is a surge of interest in using
these models for making potentially important decisions in high-regret
applications. However, despite GNNs' impressive performance, it has been
observed that carefully crafted perturbations on graph structures (or nodes
attributes) lead them to make wrong predictions. Presence of these adversarial
examples raises serious security concerns. Most of the existing robust GNN
design/training methods are only applicable to white-box settings where model
parameters are known and gradient based methods can be used by performing
convex relaxation of the discrete graph domain. More importantly, these methods
are not efficient and scalable which make them infeasible in time sensitive
tasks and massive graph datasets. To overcome these limitations, we propose a
general framework which leverages the greedy search algorithms and zeroth-order
methods to obtain robust GNNs in a generic and an efficient manner. On several
applications, we show that the proposed techniques are significantly less
computationally expensive and, in some cases, more robust than the
state-of-the-art methods making them suitable to large-scale problems which
were out of the reach of traditional robust training methods."
['stat.ML'],MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training Unit,"Are Graph Neural Networks (GNNs) fair? In many real world graphs, the
formation of edges is related to certain node attributes (e.g. gender,
community, reputation). In this case, standard GNNs using these edges will be
biased by this information, as it is encoded in the structure of the adjacency
matrix itself. In this paper, we show that when metadata is correlated with the
formation of node neighborhoods, unsupervised node embedding dimensions learn
this metadata. This bias implies an inability to control for important
covariates in real-world applications, such as recommendation systems. To solve
these issues, we introduce the Metadata-Orthogonal Node Embedding Training
(MONET) unit, a general model for debiasing embeddings of nodes in a graph.
MONET achieves this by ensuring that the node embeddings are trained on a
hyperplane orthogonal to that of the node metadata. This effectively organizes
unstructured embedding dimensions into an interpretable topology-only,
metadata-only division with no linear interactions. We illustrate the
effectiveness of MONET though our experiments on a variety of real world
graphs, which shows that our method can learn and remove the effect of
arbitrary covariates in tasks such as preventing the leakage of political party
affiliation in a blog network, and thwarting the gaming of embedding-based
recommendation systems."
['stat.ML'],Dual Graph Representation Learning,"Graph representation learning embeds nodes in large graphs as low-dimensional
vectors and is of great benefit to many downstream applications. Most embedding
frameworks, however, are inherently transductive and unable to generalize to
unseen nodes or learn representations across different graphs. Although
inductive approaches can generalize to unseen nodes, they neglect different
contexts of nodes and cannot learn node embeddings dually. In this paper, we
present a context-aware unsupervised dual encoding framework, \textbf{CADE}, to
generate representations of nodes by combining real-time neighborhoods with
neighbor-attentioned representation, and preserving extra memory of known
nodes. We exhibit that our approach is effective by comparing to
state-of-the-art methods."
['stat.ML'],Neural Message Passing on High Order Paths,"Graph neural network have achieved impressive results in predicting molecular
properties, but they do not directly account for local and hidden structures in
the graph such as functional groups and molecular geometry. At each propagation
step, GNNs aggregate only over first order neighbours, ignoring important
information contained in subsequent neighbours as well as the relationships
between those higher order connections. In this work, we generalize graph
neural nets to pass messages and aggregate across higher order paths. This
allows for information to propagate over various levels and substructures of
the graph. We demonstrate our model on a few tasks in molecular property
prediction."
['stat.ML'],Injective Domain Knowledge in Neural Networks for Transprecision Computing,"Machine Learning (ML) models are very effective in many learning tasks, due
to the capability to extract meaningful information from large data sets.
Nevertheless, there are learning problems that cannot be easily solved relying
on pure data, e.g. scarce data or very complex functions to be approximated.
Fortunately, in many contexts domain knowledge is explicitly available and can
be used to train better ML models. This paper studies the improvements that can
be obtained by integrating prior knowledge when dealing with a non-trivial
learning task, namely precision tuning of transprecision computing
applications. The domain information is injected in the ML models in different
ways: I) additional features, II) ad-hoc graph-based network topology, III)
regularization schemes. The results clearly show that ML models exploiting
problem-specific information outperform the purely data-driven ones, with an
average accuracy improvement around 38%."
['stat.ML'],FONDUE: A Framework for Node Disambiguation Using Network Embeddings,"Real-world data often presents itself in the form of a network. Examples
include social networks, citation networks, biological networks, and knowledge
graphs. In their simplest form, networks represent real-life entities (e.g.
people, papers, proteins, concepts) as nodes, and describe them in terms of
their relations with other entities by means of edges between these nodes. This
can be valuable for a range of purposes from the study of information diffusion
to bibliographic analysis, bioinformatics research, and question-answering.
  The quality of networks is often problematic though, affecting downstream
tasks. This paper focuses on the common problem where a node in the network in
fact corresponds to multiple real-life entities. In particular, we introduce
FONDUE, an algorithm based on network embedding for node disambiguation. Given
a network, FONDUE identifies nodes that correspond to multiple entities, for
subsequent splitting. Extensive experiments on twelve benchmark datasets
demonstrate that FONDUE is substantially and uniformly more accurate for
ambiguous node identification compared to the existing state-of-the-art, at a
comparable computational cost, while less optimal for determining the best way
to split ambiguous nodes."
['stat.ML'],Distributed Training of Embeddings using Graph Analytics,"Many applications today, such as NLP, network analysis, and code analysis,
rely on semantically embedding objects into low-dimensional fixed-length
vectors. Such embeddings naturally provide a way to perform useful downstream
tasks, such as identifying relations among objects or predicting objects for a
given context, etc. Unfortunately, the training necessary for accurate
embeddings is usually computationally intensive and requires processing large
amounts of data. Furthermore, distributing this training is challenging. Most
embedding training uses stochastic gradient descent (SGD), an ""inherently""
sequential algorithm. Prior approaches to parallelizing SGD do not honor these
dependencies and thus potentially suffer poor convergence.
  This paper presents a distributed training framework for a class of
applications that use Skip-gram-like models to generate embeddings. We call
this class Any2Vec and it includes Word2Vec, DeepWalk, and Node2Vec among
others. We first formulate Any2Vec training algorithm as a graph application
and leverage the state-of-the-art distributed graph analytics framework,
D-Galois. We adapt D-Galois to support dynamic graph generation and
repartitioning, and incorporate novel communication optimizations. Finally, we
introduce a novel way to combine gradients during distributed training to
prevent accuracy loss. We show that our framework, called GraphAny2Vec, matches
on a cluster of 32 hosts the accuracy of the state-of-the-art shared-memory
implementations of Word2Vec and Vertex2Vec on 1 host, and gives a geo-mean
speedup of 12x and 5x respectively. Furthermore, GraphAny2Vec is on average 2x
faster than the state-of-the-art distributed Word2Vec implementation, DMTK, on
32 hosts. We also show the superiority of our Gradient Combiner independent of
GraphAny2Vec by incorporating it in DMTK, which raises its accuracy by > 30%."
['stat.ML'],ChemGrapher: Optical Graph Recognition of Chemical Compounds by Deep Learning,"In drug discovery, knowledge of the graph structure of chemical compounds is
essential. Many thousands of scientific articles in chemistry and
pharmaceutical sciences have investigated chemical compounds, but in cases the
details of the structure of these chemical compounds is published only as an
images. A tool to analyze these images automatically and convert them into a
chemical graph structure would be useful for many applications, such drug
discovery. A few such tools are available and they are mostly derived from
optical character recognition. However, our evaluation of the performance of
those tools reveals that they make often mistakes in detecting the correct bond
multiplicity and stereochemical information. In addition, errors sometimes even
lead to missing atoms in the resulting graph. In our work, we address these
issues by developing a compound recognition method based on machine learning.
More specifically, we develop a deep neural network model for optical compound
recognition. The deep learning solution presented here consists of a
segmentation model, followed by three classification models that predict atom
locations, bonds and charges. Furthermore, this model not only predicts the
graph structure of the molecule but also produces all information necessary to
relate each component of the resulting graph to the source image. This solution
is scalable and could rapidly process thousands of images. Finally, we compare
empirically the proposed method to a well-established tool and observe
significant error reductions."
['stat.ML'],End-To-End Graph-based Deep Semi-Supervised Learning,"The quality of a graph is determined jointly by three key factors of the
graph: nodes, edges and similarity measure (or edge weights), and is very
crucial to the success of graph-based semi-supervised learning (SSL)
approaches. Recently, dynamic graph, which means part/all its factors are
dynamically updated during the training process, has demonstrated to be
promising for graph-based semi-supervised learning. However, existing
approaches only update part of the three factors and keep the rest manually
specified during the learning stage. In this paper, we propose a novel
graph-based semi-supervised learning approach to optimize all three factors
simultaneously in an end-to-end learning fashion. To this end, we concatenate
two neural networks (feature network and similarity network) together to learn
the categorical label and semantic similarity, respectively, and train the
networks to minimize a unified SSL objective function. We also introduce an
extended graph Laplacian regularization term to increase training efficiency.
Extensive experiments on several benchmark datasets demonstrate the
effectiveness of our approach."
['stat.ML'],Tree++: Truncated Tree Based Graph Kernels,"Graph-structured data arise ubiquitously in many application domains. A
fundamental problem is to quantify their similarities. Graph kernels are often
used for this purpose, which decompose graphs into substructures and compare
these substructures. However, most of the existing graph kernels do not have
the property of scale-adaptivity, i.e., they cannot compare graphs at multiple
levels of granularities. Many real-world graphs such as molecules exhibit
structure at varying levels of granularities. To tackle this problem, we
propose a new graph kernel called Tree++ in this paper. At the heart of Tree++
is a graph kernel called the path-pattern graph kernel. The path-pattern graph
kernel first builds a truncated BFS tree rooted at each vertex and then uses
paths from the root to every vertex in the truncated BFS tree as features to
represent graphs. The path-pattern graph kernel can only capture graph
similarity at fine granularities. In order to capture graph similarity at
coarse granularities, we incorporate a new concept called super path into it.
The super path contains truncated BFS trees rooted at the vertices in a path.
Our evaluation on a variety of real-world graphs demonstrates that Tree++
achieves the best classification accuracy compared with previous graph kernels."
['stat.ML'],Struct-MMSB: Mixed Membership Stochastic Blockmodels with Interpretable Structured Priors,"The mixed membership stochastic blockmodel (MMSB) is a popular framework for
community detection and network generation. It learns a low-rank mixed
membership representation for each node across communities by exploiting the
underlying graph structure. MMSB assumes that the membership distributions of
the nodes are independently drawn from a Dirichlet distribution, which limits
its capability to model highly correlated graph structures that exist in
real-world networks. In this paper, we present a flexible richly structured
MMSB model, \textit{Struct-MMSB}, that uses a recently developed statistical
relational learning model, hinge-loss Markov random fields (HL-MRFs), as a
structured prior to model complex dependencies among node attributes,
multi-relational links, and their relationship with mixed-membership
distributions. Our model is specified using a probabilistic programming
templating language that uses weighted first-order logic rules, which enhances
the model's interpretability. Further, our model is capable of learning latent
characteristics in real-world networks via meaningful latent variables encoded
as a complex combination of observed features and membership distributions. We
present an expectation-maximization based inference algorithm that learns
latent variables and parameters iteratively, a scalable stochastic variation of
the inference algorithm, and a method to learn the weights of HL-MRF structured
priors. We evaluate our model on six datasets across three different types of
networks and corresponding modeling scenarios and demonstrate that our models
are able to achieve an improvement of 15\% on average in test log-likelihood
and faster convergence when compared to state-of-the-art network models."
['stat.ML'],A Decentralized Communication Policy for Multi Agent Multi Armed Bandit Problems,"This paper proposes a novel policy for a group of agents to, individually as
well as collectively, solve a multi armed bandit (MAB) problem. The policy
relies solely on the information that an agent has obtained through sampling of
the options on its own and through communication with neighbors. The option
selection policy is based on an Upper Confidence Based (UCB) strategy while the
communication strategy that is proposed forces agents to communicate with other
agents who they believe are most likely to be exploring than exploiting. The
overall strategy is shown to significantly outperform an independent
Erd\H{o}s-R\'{e}nyi (ER) graph based random communication policy. The policy is
shown to be cost effective in terms of communication and thus to be easily
scalable to a large network of agents."
['stat.ML'],Forecaster: A Graph Transformer for Forecasting Spatial and Time-Dependent Data,"Spatial and time-dependent data is of interest in many applications. This
task is difficult due to its complex spatial dependency, long-range temporal
dependency, data non-stationarity, and data heterogeneity. To address these
challenges, we propose Forecaster, a graph Transformer architecture.
Specifically, we start by learning the structure of the graph that
parsimoniously represents the spatial dependency between the data at different
locations. Based on the topology of the graph, we sparsify the Transformer to
account for the strength of spatial dependency, long-range temporal dependency,
data non-stationarity, and data heterogeneity. We evaluate Forecaster in the
problem of forecasting taxi ride-hailing demand and show that our proposed
architecture significantly outperforms the state-of-the-art baselines."
['stat.ML'],Hebbian Graph Embeddings,"Representation learning has recently been successfully used to create vector
representations of entities in language learning, recommender systems and in
similarity learning. Graph embeddings exploit the locality structure of a graph
and generate embeddings for nodes which could be words in a language, products
of a retail website; and the nodes are connected based on a context window. In
this paper, we consider graph embeddings with an error-free associative
learning update rule, which models the embedding vector of node as a non-convex
Gaussian mixture of the embeddings of the nodes in its immediate vicinity with
some constant variance that is reduced as iterations progress. It is very easy
to parallelize our algorithm without any form of shared memory, which makes it
possible to use it on very large graphs with a much higher dimensionality of
the embeddings. We study the efficacy of proposed method on several benchmark
data sets and favorably compare with state of the art methods. Further,
proposed method is applied to generate relevant recommendations for a large
retailer."
['stat.ML'],"I-SPEC: An End-to-End Framework for Learning Transportable, Shift-Stable Models","Shifts in environment between development and deployment cause classical
supervised learning to produce models that fail to generalize well to new
target distributions. Recently, many solutions which find invariant predictive
distributions have been developed. Among these, graph-based approaches do not
require data from the target environment and can capture more stable
information than alternative methods which find stable feature sets. However,
these approaches assume that the data generating process is known in the form
of a full causal graph, which is generally not the case. In this paper, we
propose I-SPEC, an end-to-end framework that addresses this shortcoming by
using data to learn a partial ancestral graph (PAG). Using the PAG we develop
an algorithm that determines an interventional distribution that is stable to
the declared shifts; this subsumes existing approaches which find stable
feature sets that are less accurate. We apply I-SPEC to a mortality prediction
problem to show it can learn a model that is robust to shifts without needing
upfront knowledge of the full causal DAG."
['stat.ML'],Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labels,"Graph Convolutional Networks(GCNs) play a crucial role in graph learning
tasks, however, learning graph embedding with few supervised signals is still a
difficult problem. In this paper, we propose a novel training algorithm for
Graph Convolutional Network, called Multi-Stage Self-Supervised(M3S) Training
Algorithm, combined with self-supervised learning approach, focusing on
improving the generalization performance of GCNs on graphs with few labeled
nodes. Firstly, a Multi-Stage Training Framework is provided as the basis of
M3S training method. Then we leverage DeepCluster technique, a popular form of
self-supervised learning, and design corresponding aligning mechanism on the
embedding space to refine the Multi-Stage Training Framework, resulting in M3S
Training Algorithm. Finally, extensive experimental results verify the superior
performance of our algorithm on graphs with few labeled nodes under different
label rates compared with other state-of-the-art approaches."
['stat.ML'],Virtual Adversarial Training on Graph Convolutional Networks in Node Classification,"The effectiveness of Graph Convolutional Networks (GCNs) has been
demonstrated in a wide range of graph-based machine learning tasks. However,
the update of parameters in GCNs is only from labeled nodes, lacking the
utilization of unlabeled data. In this paper, we apply Virtual Adversarial
Training (VAT), an adversarial regularization method based on both labeled and
unlabeled data, on the supervised loss of GCN to enhance its generalization
performance. By imposing virtually adversarial smoothness on the posterior
distribution in semi-supervised learning, VAT yields improvement on the
Symmetrical Laplacian Smoothness of GCNs. In addition, due to the difference of
property in features, we perturb virtual adversarial perturbations on sparse
and dense features, resulting in GCN Sparse VAT (GCNSVAT) and GCN Dense VAT
(GCNDVAT) algorithms, respectively. Extensive experiments verify the
effectiveness of our two methods across different training sizes. Our work
paves the way towards better understanding the direction of improvement on GCNs
in the future."
['stat.ML'],Deep Graph Mapper: Seeing Graphs through the Neural Lens,"Recent advancements in graph representation learning have led to the
emergence of condensed encodings that capture the main properties of a graph.
However, even though these abstract representations are powerful for downstream
tasks, they are not equally suitable for visualisation purposes. In this work,
we merge Mapper, an algorithm from the field of Topological Data Analysis
(TDA), with the expressive power of Graph Neural Networks (GNNs) to produce
hierarchical, topologically-grounded visualisations of graphs. These
visualisations do not only help discern the structure of complex graphs but
also provide a means of understanding the models applied to them for solving
various tasks. We further demonstrate the suitability of Mapper as a
topological framework for graph pooling by mathematically proving an
equivalence with Min-Cut and Diff Pool. Building upon this framework, we
introduce a novel pooling algorithm based on PageRank, which obtains
competitive results with state of the art methods on graph classification
benchmarks."
['stat.ML'],Graph Convolutional Policy for Solving Tree Decomposition via Reinforcement Learning Heuristics,"We propose a Reinforcement Learning based approach to approximately solve the
Tree Decomposition (TD) problem. TD is a combinatorial problem, which is
central to the analysis of graph minor structure and computational complexity,
as well as in the algorithms of probabilistic inference, register allocation,
and other practical tasks. Recently, it has been shown that combinatorial
problems can be successively solved by learned heuristics. However, the
majority of existing works do not address the question of the generalization of
learning-based solutions. Our model is based on the graph convolution neural
network (GCN) for learning graph representations. We show that the agent
builton GCN and trained on a single graph using an Actor-Critic method can
efficiently generalize to real-world TD problem instances. We establish that
our method successfully generalizes from small graphs, where TD can be found by
exact algorithms, to large instances of practical interest, while still having
very low time-to-solution. On the other hand, the agent-based approach
surpasses all greedy heuristics by the quality of the solution."
['stat.ML'],Relation Embedding for Personalised POI Recommendation,"Point-of-Interest (POI) recommendation is one of the most important
location-based services helping people discover interesting venues or services.
However, the extreme user-POI matrix sparsity and the varying spatio-temporal
context pose challenges for POI systems, which affects the quality of POI
recommendations. To this end, we propose a translation-based relation embedding
for POI recommendation. Our approach encodes the temporal and geographic
information, as well as semantic contents effectively in a low-dimensional
relation space by using Knowledge Graph Embedding techniques. To further
alleviate the issue of user-POI matrix sparsity, a combined matrix
factorization framework is built on a user-POI graph to enhance the inference
of dynamic personal interests by exploiting the side-information. Experiments
on two real-world datasets demonstrate the effectiveness of our proposed model."
['stat.ML'],Molecule Attention Transformer,"Designing a single neural network architecture that performs competitively
across a range of molecule property prediction tasks remains largely an open
challenge, and its solution may unlock a widespread use of deep learning in the
drug discovery industry. To move towards this goal, we propose Molecule
Attention Transformer (MAT). Our key innovation is to augment the attention
mechanism in Transformer using inter-atomic distances and the molecular graph
structure. Experiments show that MAT performs competitively on a diverse set of
molecular prediction tasks. Most importantly, with a simple self-supervised
pretraining, MAT requires tuning of only a few hyperparameter values to achieve
state-of-the-art performance on downstream tasks. Finally, we show that
attention weights learned by MAT are interpretable from the chemical point of
view."
['stat.ML'],Indirect Adversarial Attacks via Poisoning Neighbors for Graph Convolutional Networks,"Graph convolutional neural networks, which learn aggregations over neighbor
nodes, have achieved great performance in node classification tasks. However,
recent studies reported that such graph convolutional node classifier can be
deceived by adversarial perturbations on graphs. Abusing graph convolutions, a
node's classification result can be influenced by poisoning its neighbors.
Given an attributed graph and a node classifier, how can we evaluate robustness
against such indirect adversarial attacks? Can we generate strong adversarial
perturbations which are effective on not only one-hop neighbors, but more far
from the target? In this paper, we demonstrate that the node classifier can be
deceived with high-confidence by poisoning just a single node even two-hops or
more far from the target. Towards achieving the attack, we propose a new
approach which searches smaller perturbations on just a single node far from
the target. In our experiments, our proposed method shows 99% attack success
rate within two-hops from the target in two datasets. We also demonstrate that
m-layer graph convolutional neural networks have chance to be deceived by our
indirect attack within m-hop neighbors. The proposed attack can be used as a
benchmark in future defense attempts to develop graph convolutional neural
networks with having adversary robustness."
['stat.ML'],Inductive Representation Learning on Temporal Graphs,"Inductive representation learning on temporal graphs is an important step
toward salable machine learning on real-world dynamic networks. The evolving
nature of temporal dynamic graphs requires handling new nodes as well as
capturing temporal patterns. The node embeddings, which are now functions of
time, should represent both the static node features and the evolving
topological structures. Moreover, node and topological features can be temporal
as well, whose patterns the node embeddings should also capture. We propose the
temporal graph attention (TGAT) layer to efficiently aggregate
temporal-topological neighborhood features as well as to learn the time-feature
interactions. For TGAT, we use the self-attention mechanism as building block
and develop a novel functional time encoding technique based on the classical
Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network
recognizes the node embeddings as functions of time and is able to inductively
infer embeddings for both new and observed nodes as the graph evolves. The
proposed approach handles both node classification and link prediction task,
and can be naturally extended to include the temporal edge features. We
evaluate our method with transductive and inductive tasks under temporal
settings with two benchmark and one industrial dataset. Our TGAT model compares
favorably to state-of-the-art baselines as well as the previous temporal graph
embedding approaches."
['stat.ML'],Strategies for Pre-training Graph Neural Networks,"Many applications of machine learning require a model to make accurate
pre-dictions on test examples that are distributionally different from training
ones, while task-specific labels are scarce during training. An effective
approach to this challenge is to pre-train a model on related tasks where data
is abundant, and then fine-tune it on a downstream task of interest. While
pre-training has been effective in many language and vision domains, it remains
an open question how to effectively use pre-training on graph datasets. In this
paper, we develop a new strategy and self-supervised methods for pre-training
Graph Neural Networks (GNNs). The key to the success of our strategy is to
pre-train an expressive GNN at the level of individual nodes as well as entire
graphs so that the GNN can learn useful local and global representations
simultaneously. We systematically study pre-training on multiple graph
classification datasets. We find that naive strategies, which pre-train GNNs at
the level of either entire graphs or individual nodes, give limited improvement
and can even lead to negative transfer on many downstream tasks. In contrast,
our strategy avoids negative transfer and improves generalization significantly
across downstream tasks, leading up to 9.4% absolute improvements in ROC-AUC
over non-pre-trained models and achieving state-of-the-art performance for
molecular property prediction and protein function prediction."
['stat.ML'],A flexible outlier detector based on a topology given by graph communities,"Outlier, or anomaly, detection is essential for optimal performance of
machine learning methods and statistical predictive models. It is not just a
technical step in a data cleaning process but a key topic in many fields such
as fraudulent document detection, in medical applications and assisted
diagnosis systems or detecting security threats. In contrast to
population-based methods, neighborhood based local approaches are simple
flexible methods that have the potential to perform well in small sample size
unbalanced problems. However, a main concern of local approaches is the impact
that the computation of each sample neighborhood has on the method performance.
Most approaches use a distance in the feature space to define a single
neighborhood that requires careful selection of several parameters. This work
presents a local approach based on a local measure of the heterogeneity of
sample labels in the feature space considered as a topological manifold.
Topology is computed using the communities of a weighted graph codifying mutual
nearest neighbors in the feature space. This way, we provide with a set of
multiple neighborhoods able to describe the structure of complex spaces without
parameter fine tuning. The extensive experiments on real-world data sets show
that our approach overall outperforms, both, local and global strategies in
multi and single view settings."
['stat.ML'],Gradient-Based Neural DAG Learning,"We propose a novel score-based approach to learning a directed acyclic graph
(DAG) from observational data. We adapt a recently proposed continuous
constrained optimization formulation to allow for nonlinear relationships
between variables using neural networks. This extension allows to model complex
interactions while avoiding the combinatorial nature of the problem. In
addition to comparing our method to existing continuous optimization methods,
we provide missing empirical comparisons to nonlinear greedy search methods. On
both synthetic and real-world data sets, this new method outperforms current
continuous methods on most tasks, while being competitive with existing greedy
search methods on important metrics for causal inference."
['stat.ML'],A Modified Perturbed Sampling Method for Local Interpretable Model-agnostic Explanation,"Explainability is a gateway between Artificial Intelligence and society as
the current popular deep learning models are generally weak in explaining the
reasoning process and prediction results. Local Interpretable Model-agnostic
Explanation (LIME) is a recent technique that explains the predictions of any
classifier faithfully by learning an interpretable model locally around the
prediction. However, the sampling operation in the standard implementation of
LIME is defective. Perturbed samples are generated from a uniform distribution,
ignoring the complicated correlation between features. This paper proposes a
novel Modified Perturbed Sampling operation for LIME (MPS-LIME), which is
formalized as the clique set construction problem. In image classification,
MPS-LIME converts the superpixel image into an undirected graph. Various
experiments show that the MPS-LIME explanation of the black-box model achieves
much better performance in terms of understandability, fidelity, and
efficiency."
['stat.ML'],Investigating Extensions to Random Walk Based Graph Embedding,"Graph embedding has recently gained momentum in the research community, in
particular after the introduction of random walk and neural network based
approaches. However, most of the embedding approaches focus on representing the
local neighborhood of nodes and fail to capture the global graph structure,
i.e. to retain the relations to distant nodes. To counter that problem, we
propose a novel extension to random walk based graph embedding, which removes a
percentage of least frequent nodes from the walks at different levels. By this
removal, we simulate farther distant nodes to reside in the close neighborhood
of a node and hence explicitly represent their connection. Besides the common
evaluation tasks for graph embeddings, such as node classification and link
prediction, we evaluate and compare our approach against related methods on
shortest path approximation. The results indicate, that extensions to random
walk based methods (including our own) improve the predictive performance only
slightly - if at all."
['stat.ML'],GraphZoom: A multi-level spectral approach for accurate and scalable graph embedding,"Graph embedding techniques have been increasingly deployed in a multitude of
different applications that involve learning on non-Euclidean data. However,
existing graph embedding models either fail to incorporate node attribute
information during training or suffer from node attribute noise, which
compromises the accuracy. Moreover, very few of them scale to large graphs due
to their high computational complexity and memory usage. In this paper we
propose GraphZoom, a multi-level framework for improving both accuracy and
scalability of unsupervised graph embedding algorithms. GraphZoom first
performs graph fusion to generate a new graph that effectively encodes the
topology of the original graph and the node attribute information. This fused
graph is then repeatedly coarsened into much smaller graphs by merging nodes
with high spectral similarities. GraphZoom allows any existing embedding
methods to be applied to the coarsened graph, before it progressively refine
the embeddings obtained at the coarsest level to increasingly finer graphs. We
have evaluated our approach on a number of popular graph datasets for both
transductive and inductive tasks. Our experiments show that GraphZoom can
substantially increase the classification accuracy and significantly accelerate
the entire graph embedding process by up to 40.8x, when compared to the
state-of-the-art unsupervised embedding methods."
['stat.ML'],Investigating the Compositional Structure Of Deep Neural Networks,"The current understanding of deep neural networks can only partially explain
how input structure, network parameters and optimization algorithms jointly
contribute to achieve the strong generalization power that is typically
observed in many real-world applications. In order to improve the comprehension
and interpretability of deep neural networks, we here introduce a novel
theoretical framework based on the compositional structure of piecewise linear
activation functions. By defining a direct acyclic graph representing the
composition of activation patterns through the network layers, it is possible
to characterize the instances of the input data with respect to both the
predicted label and the specific (linear) transformation used to perform
predictions. Preliminary tests on the MNIST dataset show that our method can
group input instances with regard to their similarity in the internal
representation of the neural network, providing an intuitive measure of input
complexity."
"['stat.ME', 'stat.ML']",Masked Gradient-Based Causal Structure Learning,"This paper studies the problem of learning causal structures from
observational data. We reformulate the Structural Equation Model (SEM) in an
augmented form with a binary graph adjacency matrix and show that, if the
original SEM is identifiable, then this augmented form can be identified up to
super-graphs of the true causal graph under mild conditions. Three methods are
further provided to remove spurious edges to recover the true graph. We next
utilize the augmented form to develop a masked structure learning method that
can be efficiently trained using gradient-based optimization methods, by
leveraging a smooth characterization on acyclicity and the Gumbel-Softmax
approach to approximate the binary adjacency matrix. It is found that the
obtained entries are typically near zero or one, and can be easily thresholded
to identify the edges. We conduct experiments on synthetic and real datasets to
validate the effectiveness of the proposed method and show that the method can
readily include different smooth functions to model causal relationships."
['stat.ML'],Unifying Graph Convolutional Neural Networks and Label Propagation,"Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are
both message passing algorithms on graphs. Both solve the task of node
classification but LPA propagates node label information across the edges of
the graph, while GCN propagates and transforms node feature information.
However, while conceptually similar, theoretical relation between LPA and GCN
has not yet been investigated. Here we study the relationship between LPA and
GCN in terms of two aspects: (1) feature/label smoothing where we analyze how
the feature/label of one node is spread over its neighbors; And, (2)
feature/label influence of how much the initial feature/label of one node
influences the final feature/label of another node. Based on our theoretical
analysis, we propose an end-to-end model that unifies GCN and LPA for node
classification. In our unified model, edge weights are learnable, and the LPA
serves as regularization to assist the GCN in learning proper edge weights that
lead to improved classification performance. Our model can also be seen as
learning attention weights based on node labels, which is more task-oriented
than existing feature-based attention models. In a number of experiments on
real-world graphs, our model shows superiority over state-of-the-art GCN-based
methods in terms of node classification accuracy."
['stat.ML'],Generalized Embedding Machines for Recommender Systems,"Factorization machine (FM) is an effective model for feature-based
recommendation which utilizes inner product to capture second-order feature
interactions. However, one of the major drawbacks of FM is that it couldn't
capture complex high-order interaction signals. A common solution is to change
the interaction function, such as stacking deep neural networks on the top of
FM. In this work, we propose an alternative approach to model high-order
interaction signals in the embedding level, namely Generalized Embedding
Machine (GEM). The embedding used in GEM encodes not only the information from
the feature itself but also the information from other correlated features.
Under such situation, the embedding becomes high-order. Then we can incorporate
GEM with FM and even its advanced variants to perform feature interactions.
More specifically, in this paper we utilize graph convolution networks (GCN) to
generate high-order embeddings. We integrate GEM with several FM-based models
and conduct extensive experiments on two real-world datasets. The results
demonstrate significant improvement of GEM over corresponding baselines."
['stat.ML'],GraphSAINT: Graph Sampling Based Inductive Learning Method,"Graph Convolutional Networks (GCNs) are powerful models for learning
representations of attributed graphs. To scale GCNs to large graphs,
state-of-the-art methods use various layer sampling techniques to alleviate the
""neighbor explosion"" problem during minibatch training. We propose GraphSAINT,
a graph sampling based inductive learning method that improves training
efficiency and accuracy in a fundamentally different way. By changing
perspective, GraphSAINT constructs minibatches by sampling the training graph,
rather than the nodes or edges across GCN layers. Each iteration, a complete
GCN is built from the properly sampled subgraph. Thus, we ensure fixed number
of well-connected nodes in all layers. We further propose normalization
technique to eliminate bias, and sampling algorithms for variance reduction.
Importantly, we can decouple the sampling from the forward and backward
propagation, and extend GraphSAINT with many architecture variants (e.g., graph
attention, jumping connection). GraphSAINT demonstrates superior performance in
both accuracy and training time on five large graphs, and achieves new
state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970)."
['stat.ML'],What Can Neural Networks Reason About?,"Neural networks have succeeded in many reasoning tasks. Empirically, these
tasks require specialized network structures, e.g., Graph Neural Networks
(GNNs) perform well on many such tasks, but less structured networks fail.
Theoretically, there is limited understanding of why and when a network
structure generalizes better than others, although they have equal expressive
power. In this paper, we develop a framework to characterize which reasoning
tasks a network can learn well, by studying how well its computation structure
aligns with the algorithmic structure of the relevant reasoning process. We
formally define this algorithmic alignment and derive a sample complexity bound
that decreases with better alignment. This framework offers an explanation for
the empirical success of popular reasoning models, and suggests their
limitations. As an example, we unify seemingly different reasoning tasks, such
as intuitive physics, visual question answering, and shortest paths, via the
lens of a powerful algorithmic paradigm, dynamic programming (DP). We show that
GNNs align with DP and thus are expected to solve these tasks. On several
reasoning tasks, our theory is supported by empirical results."
['stat.ML'],Interpretable Neuron Structuring with Graph Spectral Regularization,"While neural networks are powerful approximators used to classify or embed
data into lower dimensional spaces, they are often regarded as black boxes with
uninterpretable features. Here we propose Graph Spectral Regularization for
making hidden layers more interpretable without significantly impacting
performance on the primary task. Taking inspiration from spatial organization
and localization of neuron activations in biological networks, we use a graph
Laplacian penalty to structure the activations within a layer. This penalty
encourages activations to be smooth either on a predetermined graph or on a
feature-space graph learned from the data via co-activations of a hidden layer
of the neural network. We show numerous uses for this additional structure
including cluster indication and visualization in biological and image data
sets."
['stat.ML'],Generalization and Representational Limits of Graph Neural Networks,"We address two fundamental questions about graph neural networks (GNNs).
First, we prove that several important graph properties cannot be computed by
GNNs that rely entirely on local information. Such GNNs include the standard
message passing models, and more powerful spatial variants that exploit local
graph structure (e.g., via relative orientation of messages, or local port
ordering) to distinguish neighbors of each node. Our treatment includes a novel
graph-theoretic formalism. Second, we provide the first data dependent
generalization bounds for message passing GNNs. This analysis explicitly
accounts for the local permutation invariance of GNNs. Our bounds are much
tighter than existing VC-dimension based guarantees for GNNs, and are
comparable to Rademacher bounds for recurrent neural networks."
['stat.ML'],Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,"In recent years, graph neural networks (GNNs) have emerged as a powerful
neural architecture to learn vector representations of nodes and graphs in a
supervised, end-to-end fashion. Up to now, GNNs have only been evaluated
empirically---showing promising results. The following work investigates GNNs
from a theoretical point of view and relates them to the $1$-dimensional
Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have
the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic
(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on
this, we propose a generalization of GNNs, so-called $k$-dimensional GNNs
($k$-GNNs), which can take higher-order graph structures at multiple scales
into account. These higher-order structures play an essential role in the
characterization of social networks and molecule graphs. Our experimental
evaluation confirms our theoretical findings as well as confirms that
higher-order information is useful in the task of graph classification and
regression."
['stat.ML'],Feedback graph regret bounds for Thompson Sampling and UCB,"We study the stochastic multi-armed bandit problem with the graph-based
feedback structure introduced by Mannor and Shamir. We analyze the performance
of the two most prominent stochastic bandit algorithms, Thompson Sampling and
Upper Confidence Bound (UCB), in the graph-based feedback setting. We show that
these algorithms achieve regret guarantees that combine the graph structure and
the gaps between the means of the arm distributions. Surprisingly this holds
despite the fact that these algorithms do not explicitly use the graph
structure to select arms; they observe the additional feedback but do not
explore based on it. Towards this result we introduce a ""layering technique""
highlighting the commonalities in the two algorithms."
['stat.ML'],Graph Deconvolutional Generation,"Graph generation is an extremely important task, as graphs are found
throughout different areas of science and engineering. In this work, we focus
on the modern equivalent of the Erdos-Renyi random graph model: the graph
variational autoencoder (GVAE). This model assumes edges and nodes are
independent in order to generate entire graphs at a time using a multi-layer
perceptron decoder. As a result of these assumptions, GVAE has difficulty
matching the training distribution and relies on an expensive graph matching
procedure. We improve this class of models by building a message passing neural
network into GVAE's encoder and decoder. We demonstrate our model on the
specific task of generating small organic molecules"
['stat.ML'],Geom-GCN: Geometric Graph Convolutional Networks,"Message-passing neural networks (MPNNs) have been successfully applied to
representation learning on graphs in a variety of real-world applications.
However, two fundamental weaknesses of MPNNs' aggregators limit their ability
to represent graph-structured data: losing the structural information of nodes
in neighborhoods and lacking the ability to capture long-range dependencies in
disassortative graphs. Few studies have noticed the weaknesses from different
perspectives. From the observations on classical neural network and network
geometry, we propose a novel geometric aggregation scheme for graph neural
networks to overcome the two weaknesses. The behind basic idea is the
aggregation on a graph can benefit from a continuous space underlying the
graph. The proposed aggregation scheme is permutation-invariant and consists of
three modules, node embedding, structural neighborhood, and bi-level
aggregation. We also present an implementation of the scheme in graph
convolutional networks, termed Geom-GCN (Geometric Graph Convolutional
Networks), to perform transductive learning on graphs. Experimental results
show the proposed Geom-GCN achieved state-of-the-art performance on a wide
range of open datasets of graphs. Code is available at
https://github.com/graphdml-uiuc-jlu/geom-gcn."
['stat.ML'],Multiple Metric Learning for Structured Data,"We address the problem of merging graph and feature-space information while
learning a metric from structured data. Existing algorithms tackle the problem
in an asymmetric way, by either extracting vectorized summaries of the graph
structure or adding hard constraints to feature-space algorithms. Following a
different path, we define a metric regression scheme where we train
metric-constrained linear combinations of dissimilarity matrices. The idea is
that the input matrices can be pre-computed dissimilarity measures obtained
from any kind of available data (e.g. node attributes or edge structure). As
the model inputs are distance measures, we do not need to assume the existence
of any underlying feature space. Main challenge is that metric constraints
(especially positive-definiteness and sub-additivity), are not automatically
respected if, for example, the coefficients of the linear combination are
allowed to be negative. Both positive and sub-additive constraints are linear
inequalities, but the computational complexity of imposing them scales as
O(D3), where D is the size of the input matrices (i.e. the size of the data
set). This becomes quickly prohibitive, even when D is relatively small. We
propose a new graph-based technique for optimizing under such constraints and
show that, in some cases, our approach may reduce the original computational
complexity of the optimization process by one order of magnitude. Contrarily to
existing methods, our scheme applies to any (possibly non-convex)
metric-constrained objective function."
['stat.ML'],Towards Similarity Graphs Constructed by Deep Reinforcement Learning,"Similarity graphs are an active research direction for the nearest neighbor
search (NNS) problem. New algorithms for similarity graph construction are
continuously being proposed and analyzed by both theoreticians and
practitioners. However, existing construction algorithms are mostly based on
heuristics and do not explicitly maximize the target performance measure, i.e.,
search recall. Therefore, at the moment it is not clear whether the performance
of similarity graphs has plateaued or more effective graphs can be constructed
with more theoretically grounded methods. In this paper, we introduce a new
principled algorithm, based on adjacency matrix optimization, which explicitly
maximizes search efficiency. Namely, we propose a probabilistic model of a
similarity graph defined in terms of its edge probabilities and show how to
learn these probabilities from data as a reinforcement learning task. As
confirmed by experiments, the proposed construction method can be used to
refine the state-of-the-art similarity graphs, achieving higher recall rates
for the same number of distance computations. Furthermore, we analyze the
learned graphs and reveal the structural properties that are responsible for
more efficient search."
['stat.ML'],Simple Interactive Image Segmentation using Label Propagation through kNN graphs,"Many interactive image segmentation techniques are based on semi-supervised
learning. The user may label some pixels from each object and the SSL algorithm
will propagate the labels from the labeled to the unlabeled pixels, finding
object boundaries. This paper proposes a new SSL graph-based interactive image
segmentation approach, using undirected and unweighted kNN graphs, from which
the unlabeled nodes receive contributions from other nodes (either labeled or
unlabeled). It is simpler than many other techniques, but it still achieves
significant classification accuracy in the image segmentation task. Computer
simulations are performed using some real-world images, extracted from the
Microsoft GrabCut dataset. The segmentation results show the effectiveness of
the proposed approach."
['stat.ML'],Toward Universal Testing of Dynamic Network Models,"Numerous networks in the real world change over time, in the sense that nodes
and edges enter and leave the networks. Various dynamic random graph models
have been proposed to explain the macroscopic properties of these systems and
to provide a foundation for statistical inferences and predictions. It is of
interest to have a rigorous way to determine how well these models match
observed networks. We thus ask the following goodness of fit question: given a
sequence of observations/snapshots of a growing random graph, along with a
candidate model M, can we determine whether the snapshots came from M or from
some arbitrary alternative model that is well-separated from M in some natural
metric? We formulate this problem precisely and boil it down to goodness of fit
testing for graph-valued, infinite-state Markov processes and exhibit and
analyze a universal test based on non-stationary sampling for a natural class
of models."
['stat.ML'],The Power of Graph Convolutional Networks to Distinguish Random Graph Models: Short Version,"Graph convolutional networks (GCNs) are a widely used method for graph
representation learning. We investigate the power of GCNs, as a function of
their number of layers, to distinguish between different random graph models on
the basis of the embeddings of their sample graphs. In particular, the graph
models that we consider arise from graphons, which are the most general
possible parameterizations of infinite exchangeable graph models and which are
the central objects of study in the theory of dense graph limits. We exhibit an
infinite class of graphons that are well-separated in terms of cut distance and
are indistinguishable by a GCN with nonlinear activation functions coming from
a certain broad class if its depth is at least logarithmic in the size of the
sample graph. These results theoretically match empirical observations of
several prior works. Finally, we show a converse result that for pairs of
graphons satisfying a degree profile separation property, a very simple GCN
architecture suffices for distinguishability. To prove our results, we exploit
a connection to random walks on graphs."
['stat.ML'],AGATHA: Automatic Graph-mining And Transformer based Hypothesis generation Approach,"Medical research is risky and expensive. Drug discovery, as an example,
requires that researchers efficiently winnow thousands of potential targets to
a small candidate set for more thorough evaluation. However, research groups
spend significant time and money to perform the experiments necessary to
determine this candidate set long before seeing intermediate results.
Hypothesis generation systems address this challenge by mining the wealth of
publicly available scientific information to predict plausible research
directions. We present AGATHA, a deep-learning hypothesis generation system
that can introduce data-driven insights earlier in the discovery process.
Through a learned ranking criteria, this system quickly prioritizes plausible
term-pairs among entity sets, allowing us to recommend new research directions.
We massively validate our system with a temporal holdout wherein we predict
connections first introduced after 2015 using data published beforehand. We
additionally explore biomedical sub-domains, and demonstrate AGATHA's
predictive capacity across the twenty most popular relationship types. This
system achieves best-in-class performance on an established benchmark, and
demonstrates high recommendation scores across subdomains. Reproducibility: All
code, experimental data, and pre-trained models are available online:
sybrandt.com/2020/agatha"
['stat.ML'],PairNorm: Tackling Oversmoothing in GNNs,"The performance of graph neural nets (GNNs) is known to gradually decrease
with increasing number of layers. This decay is partly attributed to
oversmoothing, where repeated graph convolutions eventually make node
embeddings indistinguishable. We take a closer look at two different
interpretations, aiming to quantify oversmoothing. Our main contribution is
PairNorm, a novel normalization layer that is based on a careful analysis of
the graph convolution operator, which prevents all node embeddings from
becoming too similar. What is more, PairNorm is fast, easy to implement without
any change to network architecture nor any additional parameters, and is
broadly applicable to any GNN. Experiments on real-world graphs demonstrate
that PairNorm makes deeper GCN, GAT, and SGC models more robust against
oversmoothing, and significantly boosts performance for a new problem setting
that benefits from deeper GNNs. Code is available at
https://github.com/LingxiaoShawn/PairNorm."
['stat.ML'],Shortest path distance approximation using deep learning techniques,"Computing shortest path distances between nodes lies at the heart of many
graph algorithms and applications. Traditional exact methods such as
breadth-first-search (BFS) do not scale up to contemporary, rapidly evolving
today's massive networks. Therefore, it is required to find approximation
methods to enable scalable graph processing with a significant speedup. In this
paper, we utilize vector embeddings learnt by deep learning techniques to
approximate the shortest paths distances in large graphs. We show that a
feedforward neural network fed with embeddings can approximate distances with
relatively low distortion error. The suggested method is evaluated on the
Facebook, BlogCatalog, Youtube and Flickr social networks."
['stat.ML'],The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits,"We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup
consisting of $N$ agents, solving the same MAB instance to minimize individual
cumulative regret. In our model, agents collaborate by exchanging messages
through pairwise gossip style communications on an arbitrary connected graph.
We develop two novel algorithms, where each agent only plays from a subset of
all the arms. Agents use the communication medium to recommend only arm-IDs
(not samples), and thus update the set of arms from which they play. We
establish that, if agents communicate $\Omega(\log(T))$ times through any
connected pairwise gossip mechanism, then every agent's regret is a factor of
order $N$ smaller compared to the case of no collaborations. Furthermore, we
show that the communication constraints only have a second order effect on the
regret of our algorithm. We then analyze this second order term of the regret
to derive bounds on the regret-communication tradeoffs. Finally, we empirically
evaluate our algorithm and conclude that the insights are fundamental and not
artifacts of our bounds. We also show a lower bound which gives that the regret
scaling obtained by our algorithm cannot be improved even in the absence of any
communication constraints. Our results thus demonstrate that even a minimal
level of collaboration among agents greatly reduces regret for all agents."
['stat.ML'],Particle Competition and Cooperation for Semi-Supervised Learning with Label Noise,"Semi-supervised learning methods are usually employed in the classification
of data sets where only a small subset of the data items is labeled. In these
scenarios, label noise is a crucial issue, since the noise may easily spread to
a large portion or even the entire data set, leading to major degradation in
classification accuracy. Therefore, the development of new techniques to reduce
the nasty effects of label noise in semi-supervised learning is a vital issue.
Recently, a graph-based semi-supervised learning approach based on Particle
competition and cooperation was developed. In this model, particles walk in the
graphs constructed from the data sets. Competition takes place among particles
representing different class labels, while the cooperation occurs among
particles with the same label. This paper presents a new particle competition
and cooperation algorithm, specifically designed to increase the robustness to
the presence of label noise, improving its label noise tolerance. Different
from other methods, the proposed one does not require a separate technique to
deal with label noise. It performs classification of unlabeled nodes and
reclassification of the nodes affected by label noise in a unique process.
Computer simulations show the classification accuracy of the proposed method
when applied to some artificial and real-world data sets, in which we introduce
increasing amounts of label noise. The classification accuracy is compared to
those achieved by previous particle competition and cooperation algorithms and
other representative graph-based semi-supervised learning methods using the
same scenarios. Results show the effectiveness of the proposed method."
['stat.ML'],Structural Deep Clustering Network,"Clustering is a fundamental task in data analysis. Recently, deep clustering,
which derives inspiration primarily from deep learning approaches, achieves
state-of-the-art performance and has attracted considerable attention. Current
deep clustering methods usually boost the clustering results by means of the
powerful representation ability of deep learning, e.g., autoencoder, suggesting
that learning an effective representation for clustering is a crucial
requirement. The strength of deep clustering methods is to extract the useful
representations from the data itself, rather than the structure of data, which
receives scarce attention in representation learning. Motivated by the great
success of Graph Convolutional Network (GCN) in encoding the graph structure,
we propose a Structural Deep Clustering Network (SDCN) to integrate the
structural information into deep clustering. Specifically, we design a delivery
operator to transfer the representations learned by autoencoder to the
corresponding GCN layer, and a dual self-supervised mechanism to unify these
two different deep neural architectures and guide the update of the whole
model. In this way, the multiple structures of data, from low-order to
high-order, are naturally combined with the multiple representations learned by
autoencoder. Furthermore, we theoretically analyze the delivery operator, i.e.,
with the delivery operator, GCN improves the autoencoder-specific
representation as a high-order graph regularization constraint and autoencoder
helps alleviate the over-smoothing problem in GCN. Through comprehensive
experiments, we demonstrate that our propose model can consistently perform
better over the state-of-the-art techniques."
['stat.ML'],Deep Multi-Task Augmented Feature Learning via Hierarchical Graph Neural Network,"Deep multi-task learning attracts much attention in recent years as it
achieves good performance in many applications. Feature learning is important
to deep multi-task learning for sharing common information among tasks. In this
paper, we propose a Hierarchical Graph Neural Network (HGNN) to learn augmented
features for deep multi-task learning. The HGNN consists of two-level graph
neural networks. In the low level, an intra-task graph neural network is
responsible of learning a powerful representation for each data point in a task
by aggregating its neighbors. Based on the learned representation, a task
embedding can be generated for each task in a similar way to max pooling. In
the second level, an inter-task graph neural network updates task embeddings of
all the tasks based on the attention mechanism to model task relations. Then
the task embedding of one task is used to augment the feature representation of
data points in this task. Moreover, for classification tasks, an inter-class
graph neural network is introduced to conduct similar operations on a finer
granularity, i.e., the class level, to generate class embeddings for each class
in all the tasks use class embeddings to augment the feature representation.
The proposed feature augmentation strategy can be used in many deep multi-task
learning models. we analyze the HGNN in terms of training and generalization
losses. Experiments on real-world datastes show the significant performance
improvement when using this strategy."
['stat.ML'],Inductive Relation Prediction by Subgraph Reasoning,"The dominant paradigm for relation prediction in knowledge graphs involves
learning and operating on latent representations (i.e., embeddings) of entities
and relations. However, these embedding-based methods do not explicitly capture
the compositional logical rules underlying the knowledge graph, and they are
limited to the transductive setting, where the full set of entities must be
known during training. Here, we propose a graph neural network based relation
prediction framework, GraIL, that reasons over local subgraph structures and
has a strong inductive bias to learn entity-independent relational semantics.
Unlike embedding-based models, GraIL is naturally inductive and can generalize
to unseen entities and graphs after training. We provide theoretical proof and
strong empirical evidence that GraIL can represent a useful subset of
first-order logic and show that GraIL outperforms existing rule-induction
baselines in the inductive setting. We also demonstrate significant gains
obtained by ensembling GraIL with various knowledge graph embedding methods in
the transductive setting, highlighting the complementary inductive bias of our
method."
['stat.ML'],Neural Network Approximation of Graph Fourier Transforms for Sparse Sampling of Networked Flow Dynamics,"Infrastructure monitoring is critical for safe operations and sustainability.
Water distribution networks (WDNs) are large-scale networked critical systems
with complex cascade dynamics which are difficult to predict. Ubiquitous
monitoring is expensive and a key challenge is to infer the contaminant
dynamics from partial sparse monitoring data. Existing approaches use
multi-objective optimisation to find the minimum set of essential monitoring
points, but lack performance guarantees and a theoretical framework.
  Here, we first develop Graph Fourier Transform (GFT) operators to compress
networked contamination spreading dynamics to identify the essential principle
data collection points with inference performance guarantees. We then build
autoencoder (AE) inspired neural networks (NN) to generalize the GFT sampling
process and under-sample further from the initial sampling set, allowing a very
small set of data points to largely reconstruct the contamination dynamics over
real and artificial WDNs. Various sources of the contamination are tested and
we obtain high accuracy reconstruction using around 5-10% of the sample set.
This general approach of compression and under-sampled recovery via neural
networks can be applied to a wide range of networked infrastructures to enable
digital twins."
['stat.ML'],Graph Convolutional Reinforcement Learning,"Learning to cooperate is crucially important in multi-agent environments. The
key is to understand the mutual interplay between agents. However, multi-agent
environments are highly dynamic, where agents keep moving and their neighbors
change quickly. This makes it hard to learn abstract representations of mutual
interplay between agents. To tackle these difficulties, we propose graph
convolutional reinforcement learning, where graph convolution adapts to the
dynamics of the underlying graph of the multi-agent environment, and relation
kernels capture the interplay between agents by their relation representations.
Latent features produced by convolutional layers from gradually increased
receptive fields are exploited to learn cooperation, and cooperation is further
improved by temporal relation regularization for consistency. Empirically, we
show that our method substantially outperforms existing methods in a variety of
cooperative scenarios."
['stat.ML'],Graph Convolutional Gaussian Processes For Link Prediction,"Link prediction aims to reveal missing edges in a graph. We address this task
with a Gaussian process that is transformed using simplified graph convolutions
to better leverage the inductive bias of the domain. To scale the Gaussian
process model to large graphs, we introduce a variational inducing point method
that places pseudo inputs on a graph-structured domain. We evaluate our model
on eight large graphs with up to thousands of nodes and report consistent
improvements over existing Gaussian process models as well as competitive
performance when compared to state-of-the-art graph neural network approaches."
['stat.ML'],Regularizing Semi-supervised Graph Convolutional Networks with a Manifold Smoothness Loss,"Existing graph convolutional networks focus on the neighborhood aggregation
scheme. When applied to semi-supervised learning, they often suffer from the
overfitting problem as the networks are trained with the cross-entropy loss on
a small potion of labeled data. In this paper, we propose an unsupervised
manifold smoothness loss defined with respect to the graph structure, which can
be added to the loss function as a regularization. We draw connections between
the proposed loss with an iterative diffusion process, and show that minimizing
the loss is equivalent to aggregate neighbor predictions with infinite layers.
We conduct experiments on multi-layer perceptron and existing graph networks,
and demonstrate that adding the proposed loss can improve the performance
consistently."
['stat.ML'],Learning Structured Communication for Multi-agent Reinforcement Learning,"This work explores the large-scale multi-agent communication mechanism under
a multi-agent reinforcement learning (MARL) setting. We summarize the general
categories of topology for communication structures in MARL literature, which
are often manually specified. Then we propose a novel framework termed as
Learning Structured Communication (LSC) by using a more flexible and efficient
communication topology. Our framework allows for adaptive agent grouping to
form different hierarchical formations over episodes, which is generated by an
auxiliary task combined with a hierarchical routing protocol. Given each formed
topology, a hierarchical graph neural network is learned to enable effective
message information generation and propagation among inter- and intra-group
communications. In contrast to existing communication mechanisms, our method
has an explicit while learnable design for hierarchical communication.
Experiments on challenging tasks show the proposed LSC enjoys high
communication efficiency, scalability, and global cooperation capability."
['stat.ML'],A Regularized Attention Mechanism for Graph Attention Networks,"Machine learning models that can exploit the inherent structure in data have
gained prominence. In particular, there is a surge in deep learning solutions
for graph-structured data, due to its wide-spread applicability in several
fields. Graph attention networks (GAT), a recent addition to the broad class of
feature learning models in graphs, utilizes the attention mechanism to
efficiently learn continuous vector representations for semi-supervised
learning problems. In this paper, we perform a detailed analysis of GAT models,
and present interesting insights into their behavior. In particular, we show
that the models are vulnerable to heterogeneous rogue nodes and hence propose
novel regularization strategies to improve the robustness of GAT models. Using
benchmark datasets, we demonstrate performance improvements on semi-supervised
learning, using the proposed robust variant of GAT."
['stat.ML'],When Labelled Data Hurts: Deep Semi-Supervised Classification with the Graph 1-Laplacian,"We consider the task of classifying when a significantly reduced amount of
labelled data is available. This problem is of a great interest, in several
real-world problems, as obtaining large amounts of labelled data is expensive
and time consuming. We present a novel semi-supervised framework for
multi-class classification that is based on the non-smooth $\ell_1$ norm of the
normalised graph 1-Laplacian. Our transductive framework is framed under a
novel functional with carefully selected class priors - that enforces a
sufficiently smooth solution and strengthens the intrinsic relation between the
labelled and unlabelled data. We provide theoretical results of our new
optimisation model and show its connections with deep learning for handling
large-scale datasets. We demonstrate through extensive experimental results on
large datasets - CIFAR-10, CIFAR-100 and ChestX-Ray14 - that our method
outperforms classic methods and readily competes with recent deep-learning
approaches."
['stat.ML'],Laplacian-regularized graph bandits: Algorithms and theoretical analysis,"We consider a stochastic linear bandit problem with multiple users, where the
relationship between users is captured by an underlying graph and user
preferences are represented as smooth signals on the graph. We introduce a
novel bandit algorithm where the smoothness prior is imposed via the
random-walk graph Laplacian, which leads to a single-user cumulative regret
scaling as $\tilde{\mathcal{O}}(\Psi d \sqrt{T})$ with time horizon $T$,
feature dimensionality $d$, and the scalar parameter $\Psi \in (0,1)$ that
depends on the graph connectivity. This is an improvement over
$\tilde{\mathcal{O}}(d \sqrt{T})$ in \algo{LinUCB}~\Ccite{li2010contextual},
where user relationship is not taken into account. In terms of network regret
(sum of cumulative regret over $n$ users), the proposed algorithm leads to a
scaling as $\tilde{\mathcal{O}}(\Psi d\sqrt{nT})$, which is a significant
improvement over $\tilde{\mathcal{O}}(nd\sqrt{T})$ in the state-of-the-art
algorithm \algo{Gob.Lin} \Ccite{cesa2013gang}. To improve scalability, we
further propose a simplified algorithm with a linear computational complexity
with respect to the number of users, while maintaining the same regret.
Finally, we present a finite-time analysis on the proposed algorithms, and
demonstrate their advantage in comparison with state-of-the-art graph-based
bandit algorithms on both synthetic and real-world data."
['stat.ML'],Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs,"We present a deep reinforcement learning approach to minimizing the execution
cost of neural network computation graphs in an optimizing compiler. Unlike
earlier learning-based works that require training the optimizer on the same
graph to be optimized, we propose a learning approach that trains an optimizer
offline and then generalizes to previously unseen graphs without further
training. This allows our approach to produce high-quality execution decisions
on real-world TensorFlow graphs in seconds instead of hours. We consider two
optimization tasks for computation graphs: minimizing running time and peak
memory usage. In comparison to an extensive set of baselines, our approach
achieves significant improvements over classical and other learning-based
methods on these two tasks."
['stat.ML'],Graph Neural Distance Metric Learning with Graph-Bert,"Graph distance metric learning serves as the foundation for many graph
learning problems, e.g., graph clustering, graph classification and graph
matching. Existing research works on graph distance metric (or graph kernels)
learning fail to maintain the basic properties of such metrics, e.g.,
non-negative, identity of indiscernibles, symmetry and triangle inequality,
respectively. In this paper, we will introduce a new graph neural network based
distance metric learning approaches, namely GB-DISTANCE (GRAPH-BERT based
Neural Distance). Solely based on the attention mechanism, GB-DISTANCE can
learn graph instance representations effectively based on a pre-trained
GRAPH-BERT model. Different from the existing supervised/unsupervised metrics,
GB-DISTANCE can be learned effectively in a semi-supervised manner. In
addition, GB-DISTANCE can also maintain the distance metric basic properties
mentioned above. Extensive experiments have been done on several benchmark
graph datasets, and the results demonstrate that GB-DISTANCE can out-perform
the existing baseline methods, especially the recent graph neural network model
based graph metrics, with a significant gap in computing the graph distance."
['stat.ML'],Segmented Graph-Bert for Graph Instance Modeling,"In graph instance representation learning, both the diverse graph instance
sizes and the graph node orderless property have been the major obstacles that
render existing representation learning models fail to work. In this paper, we
will examine the effectiveness of GRAPH-BERT on graph instance representation
learning, which was designed for node representation learning tasks originally.
To adapt GRAPH-BERT to the new problem settings, we re-design it with a
segmented architecture instead, which is also named as SEG-BERT (Segmented
GRAPH-BERT) for reference simplicity in this paper. SEG-BERT involves no
node-order-variant inputs or functional components anymore, and it can handle
the graph node orderless property naturally. What's more, SEG-BERT has a
segmented architecture and introduces three different strategies to unify the
graph instance sizes, i.e., full-input, padding/pruning and segment shifting,
respectively. SEG-BERT is pre-trainable in an unsupervised manner, which can be
further transferred to new tasks directly or with necessary fine-tuning. We
have tested the effectiveness of SEG-BERT with experiments on seven graph
instance benchmark datasets, and SEG-BERT can out-perform the comparison
methods on six out of them with significant performance advantages."
['stat.ML'],Constant Time Graph Neural Networks,"The recent advancements in graph neural networks (GNNs) have led to
state-of-the-art performances in various applications, including
chemo-informatics, question-answering systems, and recommender systems.
However, scaling up these methods to huge graphs, such as social networks and
Web graphs, remains a challenge. In particular, the existing methods for
accelerating GNNs either are not theoretically guaranteed in terms of the
approximation error or incur at least a linear time computation cost. In this
study, we reveal the query complexity of the uniform node sampling scheme for
Message Passing Neural Networks including GraphSAGE, graph attention networks
(GATs), and graph convolutional networks (GCNs). Surprisingly, our analysis
reveals that the complexity of the node sampling method is completely
independent of the number of the nodes, edges, and neighbors of the input and
depends only on the error tolerance and confidence probability while providing
a theoretical guarantee for the approximation error. To the best of our
knowledge, this is the first paper to provide a theoretical guarantee of
approximation for GNNs within constant time. Through experiments with synthetic
and real-world datasets, we investigated the speed and precision of the node
sampling scheme and validated our theoretical results."
['stat.ML'],Multi-Agent Thompson Sampling for Bandit Applications with Sparse Neighbourhood Structures,"Multi-agent coordination is prevalent in many real-world applications.
However, such coordination is challenging due to its combinatorial nature. An
important observation in this regard is that agents in the real world often
only directly affect a limited set of neighbouring agents. Leveraging such
loose couplings among agents is key to making coordination in multi-agent
systems feasible. In this work, we focus on learning to coordinate.
Specifically, we consider the multi-agent multi-armed bandit framework, in
which fully cooperative loosely-coupled agents must learn to coordinate their
decisions to optimize a common objective. We propose multi-agent Thompson
sampling (MATS), a new Bayesian exploration-exploitation algorithm that
leverages loose couplings. We provide a regret bound that is sublinear in time
and low-order polynomial in the highest number of actions of a single agent for
sparse coordination graphs. Additionally, we empirically show that MATS
outperforms the state-of-the-art algorithm, MAUCE, on two synthetic benchmarks,
and a novel benchmark with Poisson distributions. An example of a
loosely-coupled multi-agent system is a wind farm. Coordination within the wind
farm is necessary to maximize power production. As upstream wind turbines only
affect nearby downstream turbines, we can use MATS to efficiently learn the
optimal control mechanism for the farm. To demonstrate the benefits of our
method toward applications we apply MATS to a realistic wind farm control task.
In this task, wind turbines must coordinate their alignments with respect to
the incoming wind vector in order to optimize power production. Our results
show that MATS improves significantly upon state-of-the-art coordination
methods in terms of performance, demonstrating the value of using MATS in
practical applications with sparse neighbourhood structures."
['stat.ML'],Low-dimensional statistical manifold embedding of directed graphs,"We propose a novel node embedding of directed graphs to statistical
manifolds, which is based on a global minimization of pairwise relative entropy
and graph geodesics in a non-linear way. Each node is encoded with a
probability density function over a measurable space. Furthermore, we analyze
the connection between the geometrical properties of such embedding and their
efficient learning procedure. Extensive experiments show that our proposed
embedding is better in preserving the global geodesic information of graphs, as
well as outperforming existing embedding models on directed graphs in a variety
of evaluation metrics, in an unsupervised setting."
['stat.ML'],Bayesian stochastic blockmodeling,"This chapter provides a self-contained introduction to the use of Bayesian
inference to extract large-scale modular structures from network data, based on
the stochastic blockmodel (SBM), as well as its degree-corrected and
overlapping generalizations. We focus on nonparametric formulations that allow
their inference in a manner that prevents overfitting, and enables model
selection. We discuss aspects of the choice of priors, in particular how to
avoid underfitting via increased Bayesian hierarchies, and we contrast the task
of sampling network partitions from the posterior distribution with finding the
single point estimate that maximizes it, while describing efficient algorithms
to perform either one. We also show how inferring the SBM can be used to
predict missing and spurious links, and shed light on the fundamental
limitations of the detectability of modular structures in networks."
['stat.ML'],Supervised Learning on Relational Databases with Graph Neural Networks,"The majority of data scientists and machine learning practitioners use
relational data in their work [State of ML and Data Science 2017, Kaggle,
Inc.]. But training machine learning models on data stored in relational
databases requires significant data extraction and feature engineering efforts.
These efforts are not only costly, but they also destroy potentially important
relational structure in the data. We introduce a method that uses Graph Neural
Networks to overcome these challenges. Our proposed method outperforms
state-of-the-art automatic feature engineering methods on two out of three
datasets."
['stat.ML'],Proximity Preserving Binary Code using Signed Graph-Cut,"We introduce a binary embedding framework, called Proximity Preserving Code
(PPC), which learns similarity and dissimilarity between data points to create
a compact and affinity-preserving binary code. This code can be used to apply
fast and memory-efficient approximation to nearest-neighbor searches. Our
framework is flexible, enabling different proximity definitions between data
points. In contrast to previous methods that extract binary codes based on
unsigned graph partitioning, our system models the attractive and repulsive
forces in the data by incorporating positive and negative graph weights. The
proposed framework is shown to boil down to finding the minimal cut of a signed
graph, a problem known to be NP-hard. We offer an efficient approximation and
achieve superior results by constructing the code bit after bit. We show that
the proposed approximation is superior to the commonly used spectral methods
with respect to both accuracy and complexity. Thus, it is useful for many other
problems that can be translated into signed graph cut."
['stat.ML'],Graph Transformer Networks,"Graph neural networks (GNNs) have been widely used in representation learning
on graphs and achieved state-of-the-art performance in tasks such as node
classification and link prediction. However, most existing GNNs are designed to
learn node representations on the fixed and homogeneous graphs. The limitations
especially become problematic when learning representations on a misspecified
graph or a heterogeneous graph that consists of various types of nodes and
edges. In this paper, we propose Graph Transformer Networks (GTNs) that are
capable of generating new graph structures, which involve identifying useful
connections between unconnected nodes on the original graph, while learning
effective node representation on the new graphs in an end-to-end fashion. Graph
Transformer layer, a core layer of GTNs, learns a soft selection of edge types
and composite relations for generating useful multi-hop connections so-called
meta-paths. Our experiments show that GTNs learn new graph structures, based on
data and tasks without domain knowledge, and yield powerful node representation
via convolution on the new graphs. Without domain-specific graph preprocessing,
GTNs achieved the best performance in all three benchmark node classification
tasks against the state-of-the-art methods that require pre-defined meta-paths
from domain knowledge."
['stat.ML'],Domain Adaptation on Graphs by Learning Aligned Graph Bases,"A common assumption in semi-supervised learning with graph models is that the
class label function varies smoothly on the data graph, resulting in the rather
strict prior that the label function has low-frequency content. Meanwhile, in
many classification problems, the label function may vary abruptly in certain
graph regions, resulting in high-frequency components. Although the
semi-supervised estimation of class labels is an ill-posed problem in general,
in several applications it is possible to find a source graph on which the
label function has similar frequency content to that on the target graph where
the actual classification problem is defined. In this paper, we propose a
method for domain adaptation on graphs motivated by these observations. Our
algorithm is based on learning the spectrum of the label function in a source
graph with many labeled nodes, and transferring the information of the spectrum
to the target graph with fewer labeled nodes. While the frequency content of
the class label function can be identified through the graph Fourier transform,
it is not easy to transfer the Fourier coefficients directly between the two
graphs, since no one-to-one match exists between the Fourier basis vectors of
independently constructed graphs in the domain adaptation setting. We solve
this problem by learning a transformation between the Fourier bases of the two
graphs that flexibly ``aligns'' them. The unknown class label function on the
target graph is then reconstructed such that its spectrum matches that on the
source graph while also ensuring the consistency with the available labels. The
proposed method is tested in the classification of image, online product
review, and social network data sets. Comparative experiments suggest that the
proposed algorithm performs better than recent domain adaptation methods in the
literature in most settings."
['stat.ML'],Towards Graph Representation Learning in Emergent Communication,"Recent findings in neuroscience suggest that the human brain represents
information in a geometric structure (for instance, through conceptual spaces).
In order to communicate, we flatten the complex representation of entities and
their attributes into a single word or a sentence. In this paper we use graph
convolutional networks to support the evolution of language and cooperation in
multi-agent systems. Motivated by an image-based referential game, we propose a
graph referential game with varying degrees of complexity, and we provide
strong baseline models that exhibit desirable properties in terms of language
emergence and cooperation. We show that the emerged communication protocol is
robust, that the agents uncover the true factors of variation in the game, and
that they learn to generalize beyond the samples encountered during training."
['stat.ML'],A Survey on Graph Kernels,"Graph kernels have become an established and widely-used technique for
solving classification tasks on graphs. This survey gives a comprehensive
overview of techniques for kernel-based graph classification developed in the
past 15 years. We describe and categorize graph kernels based on properties
inherent to their design, such as the nature of their extracted graph features,
their method of computation and their applicability to problems in practice. In
an extensive experimental evaluation, we study the classification accuracy of a
large suite of graph kernels on established benchmarks as well as new datasets.
We compare the performance of popular kernels with several baseline methods and
study the effect of applying a Gaussian RBF kernel to the metric induced by a
graph kernel. In doing so, we find that simple baselines become competitive
after this transformation on some datasets. Moreover, we study the extent to
which existing graph kernels agree in their predictions (and prediction errors)
and obtain a data-driven categorization of kernels as result. Finally, based on
our experimental results, we derive a practitioner's guide to kernel-based
graph classification."
['stat.ML'],Graph Representation Learning via Graphical Mutual Information Maximization,"The richness in the content of various information networks such as social
networks and communication networks provides the unprecedented potential for
learning high-quality expressive representations without external supervision.
This paper investigates how to preserve and extract the abundant information
from graph-structured data into embedding space in an unsupervised manner. To
this end, we propose a novel concept, Graphical Mutual Information (GMI), to
measure the correlation between input graphs and high-level hidden
representations. GMI generalizes the idea of conventional mutual information
computations from vector space to the graph domain where measuring mutual
information from two aspects of node features and topological structure is
indispensable. GMI exhibits several benefits: First, it is invariant to the
isomorphic transformation of input graphs---an inevitable constraint in many
existing graph representation learning algorithms; Besides, it can be
efficiently estimated and maximized by current mutual information estimation
methods such as MINE; Finally, our theoretical analysis confirms its
correctness and rationality. With the aid of GMI, we develop an unsupervised
learning model trained by maximizing GMI between the input and output of a
graph neural encoder. Considerable experiments on transductive as well as
inductive node classification and link prediction demonstrate that our method
outperforms state-of-the-art unsupervised counterparts, and even sometimes
exceeds the performance of supervised ones."
['stat.ML'],ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations,"Graph Neural Networks (GNN) have been shown to work effectively for modeling
graph structured data to solve tasks such as node classification, link
prediction and graph classification. There has been some recent progress in
defining the notion of pooling in graphs whereby the model tries to generate a
graph level representation by downsampling and summarizing the information
present in the nodes. Existing pooling methods either fail to effectively
capture the graph substructure or do not easily scale to large graphs. In this
work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and
differentiable pooling method that addresses the limitations of previous graph
pooling architectures. ASAP utilizes a novel self-attention network along with
a modified GNN formulation to capture the importance of each node in a given
graph. It also learns a sparse soft cluster assignment for nodes at each layer
to effectively pool the subgraphs to form the pooled graph. Through extensive
experiments on multiple datasets and theoretical analysis, we motivate our
choice of the components used in ASAP. Our experimental results show that
combining existing GNN architectures with ASAP leads to state-of-the-art
results on multiple graph classification benchmarks. ASAP has an average
improvement of 4%, compared to current sparse hierarchical state-of-the-art
method."
"['stat.ML', 'stat.ME']",The Sylvester Graphical Lasso (SyGlasso),"This paper introduces the Sylvester graphical lasso (SyGlasso) that captures
multiway dependencies present in tensor-valued data. The model is based on the
Sylvester equation that defines a generative model. The proposed model
complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a
Kronecker sum model for the inverse covariance matrix by providing an
alternative Kronecker sum model that is generative and interpretable. A
nodewise regression approach is adopted for estimating the conditional
independence relationships among variables. The statistical convergence of the
method is established, and empirical studies are provided to demonstrate the
recovery of meaningful conditional dependency graphs. We apply the SyGlasso to
an electroencephalography (EEG) study to compare the brain connectivity of
alcoholic and nonalcoholic subjects. We demonstrate that our model can
simultaneously estimate both the brain connectivity and its temporal
dependencies."
['stat.ML'],Molecule Property Prediction and Classification with Graph Hypernetworks,"Graph neural networks are currently leading the performance charts in
learning-based molecule property prediction and classification. Computational
chemistry has, therefore, become the a prominent testbed for generic graph
neural networks, as well as for specialized message passing methods. In this
work, we demonstrate that the replacement of the underlying networks with
hypernetworks leads to a boost in performance, obtaining state of the art
results in various benchmarks. A major difficulty in the application of
hypernetworks is their lack of stability. We tackle this by combining the
current message and the first message. A recent work has tackled the training
instability of hypernetworks in the context of error correcting codes, by
replacing the activation function of the message passing network with a
low-order Taylor approximation of it. We demonstrate that our generic solution
can replace this domain-specific solution."
['stat.ML'],Edge-based sequential graph generation with recurrent neural networks,"Graph generation with Machine Learning is an open problem with applications
in various research fields. In this work, we propose to cast the generative
process of a graph into a sequential one, relying on a node ordering procedure.
We use this sequential process to design a novel generative model composed of
two recurrent neural networks that learn to predict the edges of graphs: the
first network generates one endpoint of each edge, while the second network
generates the other endpoint conditioned on the state of the first. We test our
approach extensively on five different datasets, comparing with two well-known
baselines coming from graph literature, and two recurrent approaches, one of
which holds state of the art performances. Evaluation is conducted considering
quantitative and qualitative characteristics of the generated samples. Results
show that our approach is able to yield novel, and unique graphs originating
from very different distributions, while retaining structural properties very
similar to those in the training sample. Under the proposed evaluation
framework, our approach is able to reach performances comparable to the current
state of the art on the graph generation task."
['stat.ML'],Exploratory Combinatorial Optimization with Reinforcement Learning,"Many real-world problems can be reduced to combinatorial optimization on a
graph, where the subset or ordering of vertices that maximize some objective
function must be found. With such tasks often NP-hard and analytically
intractable, reinforcement learning (RL) has shown promise as a framework with
which efficient heuristic methods to tackle these problems can be learned.
Previous works construct the solution subset incrementally, adding one element
at a time, however, the irreversible nature of this approach prevents the agent
from revising its earlier decisions, which may be necessary given the
complexity of the optimization task. We instead propose that the agent should
seek to continuously improve the solution by learning to explore at test time.
Our approach of exploratory combinatorial optimization (ECO-DQN) is, in
principle, applicable to any combinatorial problem that can be defined on a
graph. Experimentally, we show our method to produce state-of-the-art RL
performance on the Maximum Cut problem. Moreover, because ECO-DQN can start
from any arbitrary configuration, it can be combined with other search methods
to further improve performance, which we demonstrate using a simple random
search."
['stat.ML'],Data-Driven Factor Graphs for Deep Symbol Detection,"Many important schemes in signal processing and communications, ranging from
the BCJR algorithm to the Kalman filter, are instances of factor graph methods.
This family of algorithms is based on recursive message passing-based
computations carried out over graphical models, representing a factorization of
the underlying statistics. Consequently, in order to implement these
algorithms, one must have accurate knowledge of the statistical model of the
considered signals. In this work we propose to implement factor graph methods
in a data-driven manner. In particular, we propose to use machine learning (ML)
tools to learn the factor graph, instead of the overall system task, which in
turn is used for inference by message passing over the learned graph. We apply
the proposed approach to learn the factor graph representing a finite-memory
channel, demonstrating the resulting ability to implement BCJR detection in a
data-driven fashion. We demonstrate that the proposed system, referred to as
BCJRNet, learns to implement the BCJR algorithm from a small training set, and
that the resulting receiver exhibits improved robustness to inaccurate training
compared to the conventional channel-model-based receiver operating under the
same level of uncertainty. Our results indicate that by utilizing ML tools to
learn factor graphs from labeled data, one can implement a broad range of
model-based algorithms, which traditionally require full knowledge of the
underlying statistics, in a data-driven fashion."
['stat.ML'],Balancing the Tradeoff Between Clustering Value and Interpretability,"Graph clustering groups entities -- the vertices of a graph -- based on their
similarity, typically using a complex distance function over a large number of
features. Successful integration of clustering approaches in automated
decision-support systems hinges on the interpretability of the resulting
clusters. This paper addresses the problem of generating interpretable
clusters, given features of interest that signify interpretability to an
end-user, by optimizing interpretability in addition to common clustering
objectives. We propose a $\beta$-interpretable clustering algorithm that
ensures that at least $\beta$ fraction of nodes in each cluster share the same
feature value. The tunable parameter $\beta$ is user-specified. We also present
a more efficient algorithm for scenarios with $\beta\!=\!1$ and analyze the
theoretical guarantees of the two algorithms. Finally, we empirically
demonstrate the benefits of our approaches in generating interpretable clusters
using four real-world datasets. The interpretability of the clusters is
complemented by generating simple explanations denoting the feature values of
the nodes in the clusters, using frequent pattern mining."
['stat.ML'],Which way? Direction-Aware Attributed Graph Embedding,"Graph embedding algorithms are used to efficiently represent (encode) a graph
in a low-dimensional continuous vector space that preserves the most important
properties of the graph. One aspect that is often overlooked is whether the
graph is directed or not. Most studies ignore the directionality, so as to
learn high-quality representations optimized for node classification. On the
other hand, studies that capture directionality are usually effective on link
prediction but do not perform well on other tasks. This preliminary study
presents a novel text-enriched, direction-aware algorithm called DIAGRAM ,
based on a carefully designed multi-objective model to learn embeddings that
preserve the direction of edges, textual features and graph context of nodes.
As a result, our algorithm does not have to trade one property for another and
jointly learns high-quality representations for multiple network analysis
tasks. We empirically show that DIAGRAM significantly outperforms six
state-of-the-art baselines, both direction-aware and oblivious ones,on link
prediction and network reconstruction experiments using two popular datasets.
It also achieves a comparable performance on node classification experiments
against these baselines using the same datasets."
['stat.ML'],Covariance and Correlation Kernels on a Graph in the Generalized Bag-of-Paths Formalism,"This work derives closed-form expressions computing the expectation of
co-presence and of number of co-occurrences of nodes on paths sampled from a
network according to general path weights (a bag of paths). The underlying idea
is that two nodes are considered as similar when they often appear together on
(preferably short) paths of the network. The different expressions are obtained
for both regular and hitting paths and serve as a basis for computing new
covariance and correlation measures between nodes, which are valid positive
semi-definite kernels on a graph. Experiments on semi-supervised classification
problems show that the introduced similarity measures provide competitive
results compared to other state-of-the-art distance and similarity measures
between nodes."
['stat.ML'],A Graph-Based Approach for Active Learning in Regression,"Active learning aims to reduce labeling efforts by selectively asking humans
to annotate the most important data points from an unlabeled pool and is an
example of human-machine interaction. Though active learning has been
extensively researched for classification and ranking problems, it is
relatively understudied for regression problems. Most existing active learning
for regression methods use the regression function learned at each active
learning iteration to select the next informative point to query. This
introduces several challenges such as handling noisy labels, parameter
uncertainty and overcoming initially biased training data. Instead, we propose
a feature-focused approach that formulates both sequential and batch-mode
active regression as a novel bipartite graph optimization problem. We conduct
experiments on both noise-free and noisy settings. Our experimental results on
benchmark data sets demonstrate the effectiveness of our proposed approach."
['stat.ML'],Graph Neighborhood Attentive Pooling,"Network representation learning (NRL) is a powerful technique for learning
low-dimensional vector representation of high-dimensional and sparse graphs.
Most studies explore the structure and metadata associated with the graph using
random walks and employ an unsupervised or semi-supervised learning schemes.
Learning in these methods is context-free, because only a single representation
per node is learned. Recently studies have argued on the sufficiency of a
single representation and proposed a context-sensitive approach that proved to
be highly effective in applications such as link prediction and ranking.
  However, most of these methods rely on additional textual features that
require RNNs or CNNs to capture high-level features or rely on a community
detection algorithm to identify multiple contexts of a node.
  In this study, without requiring additional features nor a community
detection algorithm, we propose a novel context-sensitive algorithm called GAP
that learns to attend on different parts of a node's neighborhood using
attentive pooling networks. We show the efficacy of GAP using three real-world
datasets on link prediction and node clustering tasks and compare it against 10
popular and state-of-the-art (SOTA) baselines. GAP consistently outperforms
them and achieves up to ~9% and ~20% gain over the best performing methods on
link prediction and clustering tasks, respectively."
['stat.ML'],The Indian Chefs Process,"This paper introduces the Indian Chefs Process (ICP), a Bayesian
nonparametric prior on the joint space of infinite directed acyclic graphs
(DAGs) and orders that generalizes Indian Buffet Processes. As our construction
shows, the proposed distribution relies on a latent Beta Process controlling
both the orders and outgoing connection probabilities of the nodes, and yields
a probability distribution on sparse infinite graphs. The main advantage of the
ICP over previously proposed Bayesian nonparametric priors for DAG structures
is its greater flexibility. To the best of our knowledge, the ICP is the first
Bayesian nonparametric model supporting every possible DAG. We demonstrate the
usefulness of the ICP on learning the structure of deep generative sigmoid
networks as well as convolutional neural networks."
['stat.ML'],The KEEN Universe: An Ecosystem for Knowledge Graph Embeddings with a Focus on Reproducibility and Transferability,"There is an emerging trend of embedding knowledge graphs (KGs) in continuous
vector spaces in order to use those for machine learning tasks. Recently, many
knowledge graph embedding (KGE) models have been proposed that learn low
dimensional representations while trying to maintain the structural properties
of the KGs such as the similarity of nodes depending on their edges to other
nodes. KGEs can be used to address tasks within KGs such as the prediction of
novel links and the disambiguation of entities. They can also be used for
downstream tasks like question answering and fact-checking. Overall, these
tasks are relevant for the semantic web community. Despite their popularity,
the reproducibility of KGE experiments and the transferability of proposed KGE
models to research fields outside the machine learning community can be a major
challenge. Therefore, we present the KEEN Universe, an ecosystem for knowledge
graph embeddings that we have developed with a strong focus on reproducibility
and transferability. The KEEN Universe currently consists of the Python
packages PyKEEN (Python KnowlEdge EmbeddiNgs), BioKEEN (Biological KnowlEdge
EmbeddiNgs), and the KEEN Model Zoo for sharing trained KGE models with the
community."
['stat.ML'],Tri-graph Information Propagation for Polypharmacy Side Effect Prediction,"The use of drug combinations often leads to polypharmacy side effects (POSE).
A recent method formulates POSE prediction as a link prediction problem on a
graph of drugs and proteins, and solves it with Graph Convolutional Networks
(GCNs). However, due to the complex relationships in POSE, this method has high
computational cost and memory demand. This paper proposes a flexible Tri-graph
Information Propagation (TIP) model that operates on three subgraphs to learn
representations progressively by propagation from protein-protein graph to
drug-drug graph via protein-drug graph. Experiments show that TIP improves
accuracy by 7%+, time efficiency by 83$\times$, and space efficiency by
3$\times$."
['stat.ML'],What graph neural networks cannot learn: depth vs width,"This paper studies the expressive power of graph neural networks falling
within the message-passing framework (GNNmp). Two results are presented. First,
GNNmp are shown to be Turing universal under sufficient conditions on their
depth, width, node attributes, and layer expressiveness. Second, it is
discovered that GNNmp can lose a significant portion of their power when their
depth and width is restricted. The proposed impossibility statements stem from
a new technique that enables the repurposing of seminal results from
distributed computing and leads to lower bounds for an array of decision,
optimization, and estimation problems involving graphs. Strikingly, several of
these problems are deemed impossible unless the product of a GNNmp's depth and
width exceeds a polynomial of the graph size; this dependence remains
significant even for tasks that appear simple or when considering
approximation."
['stat.ML'],An empirical study on using CNNs for fast radio signal prediction,"Accurate radio frequency power prediction in a geographic region is a
computationally expensive part of finding the optimal transmitter location
using a ray tracing software. We empirically analyze the viability of deep
learning models to speed up this process. Specifically, deep learning methods
including CNNs and UNET are typically used for segmentation, and can also be
employed in power prediction tasks. We consider a dataset that consists of
radio frequency power values for five different regions with four different
frame dimensions. We compare deep learning-based prediction models including
RadioUNET and four different variations of the UNET model for the power
prediction task. More complex UNET variations improve the model on higher
resolution frames such as 256x256. However, using the same models on lower
resolutions results in overfitting and simpler models perform better. Our
detailed numerical analysis shows that the deep learning models are effective
in power prediction and they are able to generalize well to the new regions."
['stat.ML'],ViWi: A Deep Learning Dataset Framework for Vision-Aided Wireless Communications,"The growing role that artificial intelligence and specifically machine
learning is playing in shaping the future of wireless communications has opened
up many new and intriguing research directions. This paper motivates the
research in the novel direction of \textit{vision-aided wireless
communications}, which aims at leveraging visual sensory information in
tackling wireless communication problems. Like any new research direction
driven by machine learning, obtaining a development dataset poses the first and
most important challenge to vision-aided wireless communications. This paper
addresses this issue by introducing the Vision-Wireless (ViWi) dataset
framework. It is developed to be a parametric, systematic, and scalable data
generation framework. It utilizes advanced 3D-modeling and ray-tracing
softwares to generate high-fidelity synthetic wireless and vision data samples
for the same scenes. The result is a framework that does not only offer a way
to generate training and testing datasets but helps provide a common ground on
which the quality of different machine learning-powered solutions could be
assessed."
['stat.ML'],Deep Learning and Gaussian Process based Band Assignment in Dual Band Systems,"We consider the band assignment (BA) problem in dual-band systems, where the
basestation (BS) chooses one of the two available frequency bands
(centimeter-wave and millimeter-wave bands) to communicate with the user
equipment (UE). While the millimeter-wave band might offer higher data rate,
there is a significant probability of outage during which the communication
should be carried on the (more reliable) centimeter-wave band. We consider two
variations of the BA problem, one-shot and sequential BA. For the former the BS
uses only the currently observed information to decide whether to switch to the
other frequency band, for the sequential BA, the BS uses a window of previously
observed information to predict the best band for a future time step. We
provide two approaches to solve the BA problem, (i) a deep learning approach
that is based on Long Short Term Memory and/or multi-layer Neural Networks, and
(ii) a Gaussian Process based approach, which relies on the assumption that the
channel states are jointly Gaussian. We compare the achieved performances to
several benchmarks in two environments: (i) a stochastic environment, and (ii)
microcellular outdoor channels obtained by ray-tracing. In general, the deep
learning solution shows superior performance in both environments."
"['stat.ML', 'stat.CO']",A geometric perspective on functional outlier detection,"We consider functional outlier detection from a geometric perspective,
specifically: for functional data sets drawn from a functional manifold which
is defined by the data's modes of variation in amplitude and phase. Based on
this manifold, we develop a conceptualization of functional outlier detection
that is more widely applicable and realistic than previously proposed. Our
theoretical and experimental analyses demonstrate several important advantages
of this perspective: It considerably improves theoretical understanding and
allows to describe and analyse complex functional outlier scenarios
consistently and in full generality, by differentiating between structurally
anomalous outlier data that are off-manifold and distributionally outlying data
that are on-manifold but at its margins. This improves practical feasibility of
functional outlier detection: We show that simple manifold learning methods can
be used to reliably infer and visualize the geometric structure of functional
data sets. We also show that standard outlier detection methods requiring
tabular data inputs can be applied to functional data very successfully by
simply using their vector-valued representations learned from manifold learning
methods as input features. Our experiments on synthetic and real data sets
demonstrate that this approach leads to outlier detection performances at least
on par with existing functional data-specific methods in a large variety of
settings, without the highly specialized, complex methodology and narrow domain
of application these methods often entail."
['stat.ML'],Feature Importance in Gradient Boosting Trees with Cross-Validation Feature Selection,"Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular
data, which produce state of the art results in many prediction tasks. Despite
its popularity, the GBM framework suffers from a fundamental flaw in its base
learners. Specifically, most implementations utilize decision trees that are
typically biased towards categorical variables with large cardinalities. The
effect of this bias was extensively studied over the years, mostly in terms of
predictive performance. In this work, we extend the scope and study the effect
of biased base learners on GBM feature importance (FI) measures. We show that
although these implementation demonstrate highly competitive predictive
performance, they still, surprisingly, suffer from bias in FI. By utilizing
cross-validated (CV) unbiased base learners, we fix this flaw at a relatively
low computational cost. We demonstrate the suggested framework in a variety of
synthetic and real-world setups, showing a significant improvement in all GBM
FI measures while maintaining relatively the same level of prediction accuracy."
['stat.ML'],YAHPO Gym -- Design Criteria and a new Multifidelity Benchmark for Hyperparameter Optimization,"When developing and analyzing new hyperparameter optimization (HPO) methods,
it is vital to empirically evaluate and compare them on well-curated benchmark
suites. In this work, we list desirable properties and requirements for such
benchmarks and propose a new set of challenging and relevant multifidelity HPO
benchmark problems motivated by these requirements. For this, we revisit the
concept of surrogate-based benchmarks and empirically compare them to more
widely-used tabular benchmarks, showing that the latter ones may induce bias in
performance estimation and ranking of HPO methods. We present a new
surrogate-based benchmark suite for multifidelity HPO methods consisting of 9
benchmark collections that constitute over 700 multifidelity HPO problems in
total. All our benchmarks also allow for querying of multiple optimization
targets, enabling the benchmarking of multi-objective HPO. We examine and
compare our benchmark suite with respect to the defined requirements and show
that our benchmarks provide viable additions to existing suites."
['stat.ML'],Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning,"Actor-critic algorithms are widely used in reinforcement learning, but are
challenging to mathematically analyze due to the online arrival of non-i.i.d.
data samples. The distribution of the data samples dynamically changes as the
model is updated, introducing a complex feedback loop between the data
distribution and the reinforcement learning algorithm. We prove that, under a
time rescaling, the online actor-critic algorithm with tabular parametrization
converges to an ordinary differential equations (ODEs) as the number of updates
becomes large. The proof first establishes the geometric ergodicity of the data
samples under a fixed actor policy. Then, using a Poisson equation, we prove
that the fluctuations of the data samples around a dynamic probability measure,
which is a function of the evolving actor model, vanish as the number of
updates become large. Once the ODE limit has been derived, we study its
convergence properties using a two time-scale analysis which asymptotically
de-couples the critic ODE from the actor ODE. The convergence of the critic to
the solution of the Bellman equation and the actor to the optimal policy are
proven. In addition, a convergence rate to this global minimum is also
established. Our convergence analysis holds under specific choices for the
learning rates and exploration rates in the actor-critic algorithm, which could
provide guidance for the implementation of actor-critic algorithms in practice."
['stat.ML'],Corruption-robust exploration in episodic reinforcement learning,"We initiate the study of multi-stage episodic reinforcement learning under
adversarial corruptions in both the rewards and the transition probabilities of
the underlying system extending recent results for the special case of
stochastic bandits. We provide a framework which modifies the aggressive
exploration enjoyed by existing reinforcement learning approaches based on
""optimism in the face of uncertainty"", by complementing them with principles
from ""action elimination"". Importantly, our framework circumvents the major
challenges posed by naively applying action elimination in the RL setting, as
formalized by a lower bound we demonstrate. Our framework yields efficient
algorithms which (a) attain near-optimal regret in the absence of corruptions
and (b) adapt to unknown levels corruption, enjoying regret guarantees which
degrade gracefully in the total corruption encountered. To showcase the
generality of our approach, we derive results for both tabular settings (where
states and actions are finite) as well as linear-function-approximation
settings (where the dynamics and rewards admit a linear underlying
representation). Notably, our work provides the first sublinear regret
guarantee which accommodates any deviation from purely i.i.d. transitions in
the bandit-feedback model for episodic reinforcement learning."
['stat.ML'],Retiring Adult: New Datasets for Fair Machine Learning,"Although the fairness community has recognized the importance of data,
researchers in the area primarily rely on UCI Adult when it comes to tabular
data. Derived from a 1994 US Census survey, this dataset has appeared in
hundreds of research papers where it served as the basis for the development
and comparison of many algorithmic fairness interventions. We reconstruct a
superset of the UCI Adult data from available US Census sources and reveal
idiosyncrasies of the UCI Adult dataset that limit its external validity. Our
primary contribution is a suite of new datasets derived from US Census surveys
that extend the existing data ecosystem for research on fair machine learning.
We create prediction tasks relating to income, employment, health,
transportation, and housing. The data span multiple years and all states of the
United States, allowing researchers to study temporal shift and geographic
variation. We highlight a broad initial sweep of new empirical insights
relating to trade-offs between fairness criteria, performance of algorithmic
interventions, and the role of distribution shift based on our new datasets.
Our findings inform ongoing debates, challenge some existing narratives, and
point to future research directions. Our datasets are available at
https://github.com/zykls/folktables."
['stat.ML'],Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks,"Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification."
['stat.ML'],Beyond No Regret: Instance-Dependent PAC Reinforcement Learning,"The theory of reinforcement learning has focused on two fundamental problems:
achieving low regret, and identifying $\epsilon$-optimal policies. While a
simple reduction allows one to apply a low-regret algorithm to obtain an
$\epsilon$-optimal policy and achieve the worst-case optimal rate, it is
unknown whether low-regret algorithms can obtain the instance-optimal rate for
policy identification. We show that this is not possible -- there exists a
fundamental tradeoff between achieving low regret and identifying an
$\epsilon$-optimal policy at the instance-optimal rate.
  Motivated by our negative finding, we propose a new measure of
instance-dependent sample complexity for PAC tabular reinforcement learning
which explicitly accounts for the attainable state visitation distributions in
the underlying MDP. We then propose and analyze a novel, planning-based
algorithm which attains this sample complexity -- yielding a complexity which
scales with the suboptimality gaps and the ``reachability'' of a state. We show
that our algorithm is nearly minimax optimal, and on several examples that our
instance-dependent sample complexity offers significant improvements over
worst-case bounds."
['stat.ML'],Learning more skills through optimistic exploration,"Unsupervised skill learning objectives (Gregor et al., 2016, Eysenbach et
al., 2018) allow agents to learn rich repertoires of behavior in the absence of
extrinsic rewards. They work by simultaneously training a policy to produce
distinguishable latent-conditioned trajectories, and a discriminator to
evaluate distinguishability by trying to infer latents from trajectories. The
hope is for the agent to explore and master the environment by encouraging each
skill (latent) to reliably reach different states. However, an inherent
exploration problem lingers: when a novel state is actually encountered, the
discriminator will necessarily not have seen enough training data to produce
accurate and confident skill classifications, leading to low intrinsic reward
for the agent and effective penalization of the sort of exploration needed to
actually maximize the objective. To combat this inherent pessimism towards
exploration, we derive an information gain auxiliary objective that involves
training an ensemble of discriminators and rewarding the policy for their
disagreement. Our objective directly estimates the epistemic uncertainty that
comes from the discriminator not having seen enough training examples, thus
providing an intrinsic reward more tailored to the true objective compared to
pseudocount-based methods (Burda et al., 2019). We call this exploration bonus
discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate
empirically that DISDAIN improves skill learning both in a tabular grid world
(Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we
encourage researchers to treat pessimism with DISDAIN."
['stat.ML'],Picket: Guarding Against Corrupted Data in Tabular Data during Learning and Inference,"Data corruption is an impediment to modern machine learning deployments.
Corrupted data can severely bias the learned model and can also lead to invalid
inferences. We present, Picket, a simple framework to safeguard against data
corruptions during both training and deployment of machine learning models over
tabular data. For the training stage, Picket identifies and removes corrupted
data points from the training data to avoid obtaining a biased model. For the
deployment stage, Picket flags, in an online manner, corrupted query points to
a trained machine learning model that due to noise will result in incorrect
predictions. To detect corrupted data, Picket uses a self-supervised deep
learning model for mixed-type tabular data, which we call PicketNet. To
minimize the burden of deployment, learning a PicketNet model does not require
any human-labeled data. Picket is designed as a plugin that can increase the
robustness of any machine learning pipeline. We evaluate Picket on a diverse
array of real-world data considering different corruption models that include
systematic and adversarial noise during both training and testing. We show that
Picket consistently safeguards against corrupted data during both training and
deployment of various models ranging from SVMs to neural networks, beating a
diverse array of competing methods that span from data quality validation
models to robust outlier-detection models."
['stat.ML'],Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks,"There has been significant research done on developing methods for improving
robustness to distributional shift and uncertainty estimation. In contrast,
only limited work has examined developing standard datasets and benchmarks for
assessing these approaches. Additionally, most work on uncertainty estimation
and robustness has developed new techniques based on small-scale regression or
image classification tasks. However, many tasks of practical interest have
different modalities, such as tabular data, audio, text, or sensor data, which
offer significant challenges involving regression and discrete or continuous
structured prediction. Thus, given the current state of the field, a
standardized large-scale dataset of tasks across a range of modalities affected
by distributional shifts is necessary. This will enable researchers to
meaningfully evaluate the plethora of recently developed uncertainty
quantification methods, as well as assessment criteria and state-of-the-art
baselines. In this work, we propose the \emph{Shifts Dataset} for evaluation of
uncertainty estimates and robustness to distributional shift. The dataset,
which has been collected from industrial sources and services, is composed of
three tasks, with each corresponding to a particular data modality: tabular
weather prediction, machine translation, and self-driving car (SDC) vehicle
motion prediction. All of these data modalities and tasks are affected by real,
`in-the-wild' distributional shifts and pose interesting challenges with
respect to uncertainty estimation. In this work we provide a description of the
dataset and baseline results for all tasks."
['stat.ML'],LocalGLMnet: interpretable deep learning for tabular data,"Deep learning models have gained great popularity in statistical modeling
because they lead to very competitive regression models, often outperforming
classical statistical models such as generalized linear models. The
disadvantage of deep learning models is that their solutions are difficult to
interpret and explain, and variable selection is not easily possible because
deep learning models solve feature engineering and variable selection
internally in a nontransparent way. Inspired by the appealing structure of
generalized linear models, we propose a new network architecture that shares
similar features as generalized linear models, but provides superior predictive
power benefiting from the art of representation learning. This new architecture
allows for variable selection of tabular data and for interpretation of the
calibrated deep learning model, in fact, our approach provides an additive
decomposition in the spirit of Shapley values and integrated gradients."
['stat.ML'],Towards Domain-Agnostic Contrastive Learning,"Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach."
['stat.ML'],Accuracy Prediction with Non-neural Model for Neural Architecture Search,"Neural architecture search (NAS) with an accuracy predictor that predicts the
accuracy of candidate architectures has drawn increasing attention due to its
simplicity and effectiveness. Previous works usually employ neural
network-based predictors which require more delicate design and are easy to
overfit. Considering that most architectures are represented as sequences of
discrete symbols which are more like tabular data and preferred by non-neural
predictors, in this paper, we study an alternative approach which uses
non-neural model for accuracy prediction. Specifically, as decision tree based
models can better handle tabular data, we leverage gradient boosting decision
tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor
can achieve comparable (if not better) prediction accuracy than neural network
based predictors. Moreover, considering that a compact search space can ease
the search process, we propose to prune the search space gradually according to
important features derived from GBDT. In this way, NAS can be performed by
first pruning the search space and then searching a neural architecture, which
is more efficient and effective. Experiments on NASBench-101 and ImageNet
demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On
NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,
regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global
optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further
achieves 23.4% top-1 error rate on ImageNet when enhanced with search space
pruning. Code is provided at https://github.com/renqianluo/GBDT-NAS."
['stat.ML'],Policy Optimization in Adversarial MDPs: Improved Exploration via Dilated Bonuses,"Policy optimization is a widely-used method in reinforcement learning. Due to
its local-search nature, however, theoretical guarantees on global optimality
often rely on extra assumptions on the Markov Decision Processes (MDPs) that
bypass the challenge of global exploration. To eliminate the need of such
assumptions, in this work, we develop a general solution that adds dilated
bonuses to the policy update to facilitate global exploration. To showcase the
power and generality of this technique, we apply it to several episodic MDP
settings with adversarial losses and bandit feedback, improving and
generalizing the state-of-the-art. Specifically, in the tabular case, we obtain
$\widetilde{\mathcal{O}}(\sqrt{T})$ regret where $T$ is the number of episodes,
improving the $\widetilde{\mathcal{O}}({T}^{2/3})$ regret bound by Shani et al.
(2020). When the number of states is infinite, under the assumption that the
state-action values are linear in some low-dimensional features, we obtain
$\widetilde{\mathcal{O}}({T}^{2/3})$ regret with the help of a simulator,
matching the result of Neu and Olkhovskaya (2020) while importantly removing
the need of an exploratory policy that their algorithm requires. When a
simulator is unavailable, we further consider a linear MDP setting and obtain
$\widetilde{\mathcal{O}}({T}^{14/15})$ regret, which is the first result for
linear MDPs with adversarial losses and bandit feedback."
['stat.ML'],"Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms","Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems."
['stat.ML'],Lockout: Sparse Regularization of Neural Networks,"Many regression and classification procedures fit a parameterized function
$f(x;w)$ of predictor variables $x$ to data $\{x_{i},y_{i}\}_1^N$ based on some
loss criterion $L(y,f)$. Often, regularization is applied to improve accuracy
by placing a constraint $P(w)\leq t$ on the values of the parameters $w$.
Although efficient methods exist for finding solutions to these constrained
optimization problems for all values of $t\geq0$ in the special case when $f$
is a linear function, none are available when $f$ is non-linear (e.g. Neural
Networks). Here we present a fast algorithm that provides all such solutions
for any differentiable function $f$ and loss $L$, and any constraint $P$ that
is an increasing monotone function of the absolute value of each parameter.
Applications involving sparsity inducing regularization of arbitrary Neural
Networks are discussed. Empirical results indicate that these sparse solutions
are usually superior to their dense counterparts in both accuracy and
interpretability. This improvement in accuracy can often make Neural Networks
competitive with, and sometimes superior to, state-of-the-art methods in the
analysis of tabular data."
['stat.ML'],"Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data","Deep Neural Networks (DNNs) have an enormous potential to learn from complex
biomedical data. In particular, DNNs have been used to seamlessly fuse
heterogeneous information from neuroanatomy, genetics, biomarkers, and
neuropsychological tests for highly accurate Alzheimer's disease diagnosis. On
the other hand, their black-box nature is still a barrier for the adoption of
such a system in the clinic, where interpretability is absolutely essential. We
propose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for
explaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of
the neuroanatomy and tabular biomarkers. Our explanations are based on the
Shapley value, which is the unique method that satisfies all fundamental axioms
for local explanations previously established in the literature. Thus, SVEHNN
has many desirable characteristics that previous work on interpretability for
medical decision making is lacking. To avoid the exponential time complexity of
the Shapley value, we propose to transform a given DNN into a Lightweight
Probabilistic Deep Network without re-training, thus achieving a complexity
only quadratic in the number of features. In our experiments on synthetic and
real data, we show that we can closely approximate the exact Shapley value with
a dramatically reduced runtime and can reveal the hidden knowledge the network
has learned from the data."
['stat.ML'],Gap-Dependent Bounds for Two-Player Markov Games,"As one of the most popular methods in the field of reinforcement learning,
Q-learning has received increasing attention. Recently, there have been more
theoretical works on the regret bound of algorithms that belong to the
Q-learning class in different settings. In this paper, we analyze the
cumulative regret when conducting Nash Q-learning algorithm on 2-player
turn-based stochastic Markov games (2-TBSG), and propose the very first gap
dependent logarithmic upper bounds in the episodic tabular setting. This bound
matches the theoretical lower bound only up to a logarithmic term. Furthermore,
we extend the conclusion to the discounted game setting with infinite horizon
and propose a similar gap dependent logarithmic regret bound. Also, under the
linear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both
centralized and independent settings."
['stat.ML'],Instance-based Counterfactual Explanations for Time Series Classification,"In recent years, there has been a rapidly expanding focus on explaining the
predictions made by black-box AI systems that handle image and tabular data.
However, considerably less attention has been paid to explaining the
predictions of opaque AI systems handling time series data. In this paper, we
advance a novel model-agnostic, case-based technique -- Native Guide -- that
generates counterfactual explanations for time series classifiers. Given a
query time series, $T_{q}$, for which a black-box classification system
predicts class, $c$, a counterfactual time series explanation shows how $T_{q}$
could change, such that the system predicts an alternative class, $c'$. The
proposed instance-based technique adapts existing counterfactual instances in
the case-base by highlighting and modifying discriminative areas of the time
series that underlie the classification. Quantitative and qualitative results
from two comparative experiments indicate that Native Guide generates
plausible, proximal, sparse and diverse explanations that are better than those
produced by key benchmark counterfactual methods."
['stat.ML'],A Unified Framework for Conservative Exploration,"We study bandits and reinforcement learning (RL) subject to a conservative
constraint where the agent is asked to perform at least as well as a given
baseline policy. This setting is particular relevant in real-world domains
including digital marketing, healthcare, production, finance, etc. For
multi-armed bandits, linear bandits and tabular RL, specialized algorithms and
theoretical analyses were proposed in previous work. In this paper, we present
a unified framework for conservative bandits and RL, in which our core
technique is to calculate the necessary and sufficient budget obtained from
running the baseline policy. For lower bounds, our framework gives a black-box
reduction that turns a certain lower bound in the nonconservative setting into
a new lower bound in the conservative setting. We strengthen the existing lower
bound for conservative multi-armed bandits and obtain new lower bounds for
conservative linear bandits, tabular RL and low-rank MDP. For upper bounds, our
framework turns a certain nonconservative upper-confidence-bound (UCB)
algorithm into a conservative algorithm with a simple analysis. For multi-armed
bandits, linear bandits and tabular RL, our new upper bounds tighten or match
existing ones with significantly simpler analyses. We also obtain a new upper
bound for conservative low-rank MDP."
['stat.ML'],Low-rank Characteristic Tensor Density Estimation Part II: Compression and Latent Density Estimation,"Learning generative probabilistic models is a core problem in machine
learning, which presents significant challenges due to the curse of
dimensionality. This paper proposes a joint dimensionality reduction and
non-parametric density estimation framework, using a novel estimator that can
explicitly capture the underlying distribution of appropriate reduced-dimension
representations of the input data. The idea is to jointly design a nonlinear
dimensionality reducing auto-encoder to model the training data in terms of a
parsimonious set of latent random variables, and learn a canonical low-rank
tensor model of the joint distribution of the latent variables in the Fourier
domain. The proposed latent density model is non-parametric and universal, as
opposed to the predefined prior that is assumed in variational auto-encoders.
Joint optimization of the auto-encoder and the latent density estimator is
pursued via a formulation which learns both by minimizing a combination of the
negative log-likelihood in the latent domain and the auto-encoder
reconstruction loss. We demonstrate that the proposed model achieves very
promising results on toy, tabular, and image datasets on regression tasks,
sampling, and anomaly detection."
['stat.ML'],MADE: Exploration via Maximizing Deviation from Explored Regions,"In online reinforcement learning (RL), efficient exploration remains
particularly challenging in high-dimensional environments with sparse rewards.
In low-dimensional environments, where tabular parameterization is possible,
count-based upper confidence bound (UCB) exploration methods achieve minimax
near-optimal rates. However, it remains unclear how to efficiently implement
UCB in realistic RL tasks that involve non-linear function approximation. To
address this, we propose a new exploration approach via \textit{maximizing} the
deviation of the occupancy of the next policy from the explored regions. We add
this term as an adaptive regularizer to the standard RL objective to balance
exploration vs. exploitation. We pair the new objective with a provably
convergent algorithm, giving rise to a new intrinsic reward that adjusts
existing bonuses. The proposed intrinsic reward is easy to implement and
combine with other existing RL algorithms to conduct exploration. As a proof of
concept, we evaluate the new intrinsic reward on tabular examples across a
variety of model-based and model-free algorithms, showing improvements over
count-only exploration strategies. When tested on navigation and locomotion
tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach
significantly improves sample efficiency over state-of-the-art methods. Our
code is available at https://github.com/tianjunz/MADE."
['stat.ML'],Leakage of Dataset Properties in Multi-Party Machine Learning,"Secure multi-party machine learning allows several parties to build a model
on their pooled data to increase utility while not explicitly sharing data with
each other. We show that such multi-party computation can cause leakage of
global dataset properties between the parties even when parties obtain only
black-box access to the final model. In particular, a ``curious'' party can
infer the distribution of sensitive attributes in other parties' data with high
accuracy. This raises concerns regarding the confidentiality of properties
pertaining to the whole dataset as opposed to individual data records. We show
that our attack can leak population-level properties in datasets of different
types, including tabular, text, and graph data. To understand and measure the
source of leakage, we consider several models of correlation between a
sensitive attribute and the rest of the data. Using multiple machine learning
models, we show that leakage occurs even if the sensitive attribute is not
included in the training data and has a low correlation with other attributes
or the target variable."
['stat.ML'],The Partial Response Network: a neural network nomogram,"Among interpretable machine learning methods, the class of Generalised
Additive Neural Networks (GANNs) is referred to as Self-Explaining Neural
Networks (SENN) because of the linear dependence on explicit functions of the
inputs. In binary classification this shows the precise weight that each input
contributes towards the logit. The nomogram is a graphical representation of
these weights. We show that functions of individual and pairs of variables can
be derived from a functional Analysis of Variance (ANOVA) representation,
enabling an efficient feature selection to be carried by application of the
logistic Lasso. This process infers the structure of GANNs which otherwise
needs to be predefined. As this method is particularly suited for tabular data,
it starts by fitting a generic flexible model, in this case a Multi-layer
Perceptron (MLP) to which the ANOVA decomposition is applied. This has the
further advantage that the resulting GANN can be replicated as a SENN, enabling
further refinement of the univariate and bivariate component functions to take
place. The component functions are partial responses hence the SENN is a
partial response network. The Partial Response Network (PRN) is equally as
transparent as a traditional logistic regression model, but capable of
non-linear classification with comparable or superior performance to the
original MLP. In other words, the PRN is a fully interpretable representation
of the MLP, at the level of univariate and bivariate effects. The performance
of the PRN is shown to be competitive for benchmark data, against
state-of-the-art machine learning methods including GBM, SVM and Random
Forests. It is also compared with spline-based Sparse Additive Models (SAM)
showing that a semi-parametric representation of the GAM as a neural network
can be as effective as the SAM though less constrained by the need to set
spline nodes."
['stat.ML'],An Analysis of the Deployment of Models Trained on Private Tabular Synthetic Data: Unexpected Surprises,"Diferentially private (DP) synthetic datasets are a powerful approach for
training machine learning models while respecting the privacy of individual
data providers. The effect of DP on the fairness of the resulting trained
models is not yet well understood. In this contribution, we systematically
study the effects of differentially private synthetic data generation on
classification. We analyze disparities in model utility and bias caused by the
synthetic dataset, measured through algorithmic fairness metrics. Our first set
of results show that although there seems to be a clear negative correlation
between privacy and utility (the more private, the less accurate) across all
data synthesizers we evaluated, more privacy does not necessarily imply more
bias. Additionally, we assess the effects of utilizing synthetic datasets for
model training and model evaluation. We show that results obtained on synthetic
data can misestimate the actual model performance when it is deployed on real
data. We hence advocate on the need for defining proper testing protocols in
scenarios where differentially private synthetic datasets are utilized for
model training and evaluation."
['stat.ML'],On the Sample Complexity and Metastability of Heavy-tailed Policy Search in Continuous Control,"Reinforcement learning is a framework for interactive decision-making with
incentives sequentially revealed across time without a system dynamics model.
Due to its scaling to continuous spaces, we focus on policy search where one
iteratively improves a parameterized policy with stochastic policy gradient
(PG) updates. In tabular Markov Decision Problems (MDPs), under persistent
exploration and suitable parameterization, global optimality may be obtained.
By contrast, in continuous space, the non-convexity poses a pathological
challenge as evidenced by existing convergence results being mostly limited to
stationarity or arbitrary local extrema. To close this gap, we step towards
persistent exploration in continuous space through policy parameterizations
defined by distributions of heavier tails defined by tail-index parameter
alpha, which increases the likelihood of jumping in state space. Doing so
invalidates smoothness conditions of the score function common to PG. Thus, we
establish how the convergence rate to stationarity depends on the policy's tail
index alpha, a Holder continuity parameter, integrability conditions, and an
exploration tolerance parameter introduced here for the first time. Further, we
characterize the dependence of the set of local maxima on the tail index
through an exit and transition time analysis of a suitably defined Markov
chain, identifying that policies associated with Levy Processes of a heavier
tail converge to wider peaks. This phenomenon yields improved stability to
perturbations in supervised learning, which we corroborate also manifests in
improved performance of policy search, especially when myopic and farsighted
incentives are misaligned."
['stat.ML'],MobILE: Model-Based Imitation Learning From Observation Alone,"This paper studies Imitation Learning from Observations alone (ILFO) where
the learner is presented with expert demonstrations that consist only of states
visited by an expert (without access to actions taken by the expert). We
present a provably efficient model-based framework MobILE to solve the ILFO
problem. MobILE involves carefully trading off strategic exploration against
imitation - this is achieved by integrating the idea of optimism in the face of
uncertainty into the distribution matching imitation learning (IL) framework.
We provide a unified analysis for MobILE, and demonstrate that MobILE enjoys
strong performance guarantees for classes of MDP dynamics that satisfy certain
well studied notions of structural complexity. We also show that the ILFO
problem is strictly harder than the standard IL problem by presenting an
exponential sample complexity separation between IL and ILFO. We complement
these theoretical results with experimental simulations on benchmark OpenAI Gym
tasks that indicate the efficacy of MobILE."
['stat.ML'],Model-Free Learning for Two-Player Zero-Sum Partially Observable Markov Games with Perfect Recall,"We study the problem of learning a Nash equilibrium (NE) in an imperfect
information game (IIG) through self-play. Precisely, we focus on two-player,
zero-sum, episodic, tabular IIG under the perfect-recall assumption where the
only feedback is realizations of the game (bandit feedback). In particular, the
dynamic of the IIG is not known -- we can only access it by sampling or
interacting with a game simulator. For this learning setting, we provide the
Implicit Exploration Online Mirror Descent (IXOMD) algorithm. It is a
model-free algorithm with a high-probability bound on the convergence rate to
the NE of order $1/\sqrt{T}$ where $T$ is the number of played games. Moreover,
IXOMD is computationally efficient as it needs to perform the updates only
along the sampled trajectory."
['stat.ML'],Thompson Sampling with a Mixture Prior,"We study Thompson sampling (TS) in online decision-making problems where the
uncertain environment is sampled from a mixture distribution. This is relevant
to multi-task settings, where a learning agent is faced with different classes
of problems. We incorporate this structure in a natural way by initializing TS
with a mixture prior -- dubbed MixTS -- and develop a novel, general technique
for analyzing the regret of TS with such priors. We apply this technique to
derive Bayes regret bounds for MixTS in both linear bandits and tabular Markov
decision processes (MDPs). Our regret bounds reflect the structure of the
problem and depend on the number of components and confidence width of each
component of the prior. Finally, we demonstrate the empirical effectiveness of
MixTS in both synthetic and real-world experiments."
['stat.ML'],Polynomial magic! Hermite polynomials for private data generation,"Kernel mean embedding is a useful tool to compare probability measures.
Despite its usefulness, kernel mean embedding considers infinite-dimensional
features, which are challenging to handle in the context of differentially
private data generation. A recent work proposes to approximate the kernel mean
embedding of data distribution using finite-dimensional random features, where
the sensitivity of the features becomes analytically tractable. More
importantly, this approach significantly reduces the privacy cost, compared to
other known privatization methods (e.g., DP-SGD), as the approximate kernel
mean embedding of the data distribution is privatized only once and can then be
repeatedly used during training of a generator without incurring any further
privacy cost. However, the required number of random features is excessively
high, often ten thousand to a hundred thousand, which worsens the sensitivity
of the approximate kernel mean embedding. To improve the sensitivity, we
propose to replace random features with Hermite polynomial features. Unlike the
random features, the Hermite polynomial features are ordered, where the
features at the low orders contain more information on the distribution than
those at the high orders. Hence, a relatively low order of Hermite polynomial
features can more accurately approximate the mean embedding of the data
distribution compared to a significantly higher number of random features. As a
result, using the Hermite polynomial features, we significantly improve the
privacy-accuracy trade-off, reflected in the high quality and diversity of the
generated data, when tested on several heterogeneous tabular datasets, as well
as several image benchmark datasets."
['stat.ML'],Softmax Policy Gradient Methods Can Take Exponential Time to Converge,"The softmax policy gradient (PG) method, which performs gradient ascent under
softmax policy parameterization, is arguably one of the de facto
implementations of policy optimization in modern reinforcement learning. For
$\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs),
remarkable progress has recently been achieved towards establishing global
convergence of softmax PG methods in finding a near-optimal policy. However,
prior results fall short of delineating clear dependencies of convergence rates
on salient parameters such as the cardinality of the state space $\mathcal{S}$
and the effective horizon $\frac{1}{1-\gamma}$, both of which could be
excessively large. In this paper, we deliver a pessimistic message regarding
the iteration complexity of softmax PG methods, despite assuming access to
exact gradient computation. Specifically, we demonstrate that the softmax PG
method with stepsize $\eta$ can take \[
  \frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}}
~\text{iterations} \] to converge, even in the presence of a benign policy
initialization and an initial state distribution amenable to exploration (so
that the distribution mismatch coefficient is not exceedingly large). This is
accomplished by characterizing the algorithmic dynamics over a
carefully-constructed MDP containing only three actions. Our exponential lower
bound hints at the necessity of carefully adjusting update rules or enforcing
proper regularization in accelerating PG methods."
['stat.ML'],Improved Worst-Case Regret Bounds for Randomized Least-Squares Value Iteration,"This paper studies regret minimization with randomized value functions in
reinforcement learning. In tabular finite-horizon Markov Decision Processes, we
introduce a clipping variant of one classical Thompson Sampling (TS)-like
algorithm, randomized least-squares value iteration (RLSVI). Our
$\tilde{\mathrm{O}}(H^2S\sqrt{AT})$ high-probability worst-case regret bound
improves the previous sharpest worst-case regret bounds for RLSVI and matches
the existing state-of-the-art worst-case TS-based regret bounds."
['stat.ML'],The Power of Exploiter: Provable Multi-Agent RL in Large State Spaces,"Modern reinforcement learning (RL) commonly engages practical problems with
large state spaces, where function approximation must be deployed to
approximate either the value function or the policy. While recent progresses in
RL theory address a rich set of RL problems with general function
approximation, such successes are mostly restricted to the single-agent
setting. It remains elusive how to extend these results to multi-agent RL,
especially due to the new challenges arising from its game-theoretical nature.
This paper considers two-player zero-sum Markov Games (MGs). We propose a new
algorithm that can provably find the Nash equilibrium policy using a polynomial
number of samples, for any MG with low multi-agent Bellman-Eluder dimension --
a new complexity measure adapted from its single-agent version (Jin et al.,
2021). A key component of our new algorithm is the exploiter, which facilitates
the learning of the main player by deliberately exploiting her weakness. Our
theoretical framework is generic, which applies to a wide range of models
including but not limited to tabular MGs, MGs with linear or kernel function
approximation, and MGs with rich observations."
['stat.ML'],Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression,"Gradient Boosting Machines (GBM) are hugely popular for solving tabular data
problems. However, practitioners are not only interested in point predictions,
but also in probabilistic predictions in order to quantify the uncertainty of
the predictions. Creating such probabilistic predictions is difficult with
existing GBM-based solutions: they either require training multiple models or
they become too computationally expensive to be useful for large-scale
settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method
to create probabilistic predictions with a single ensemble of decision trees in
a computationally efficient manner. PGBM approximates the leaf weights in a
decision tree as a random variable, and approximates the mean and variance of
each sample in a dataset via stochastic tree ensemble update equations. These
learned moments allow us to subsequently sample from a specified distribution
after training. We empirically demonstrate the advantages of PGBM compared to
existing state-of-the-art methods: (i) PGBM enables probabilistic estimates
without compromising on point performance in a single model, (ii) PGBM learns
probabilistic estimates via a single model only (and without requiring
multi-parameter boosting), and thereby offers a speedup of up to several orders
of magnitude over existing state-of-the-art methods on large datasets, and
(iii) PGBM achieves accurate probabilistic estimates in tasks with complex
differentiable loss functions, such as hierarchical time series problems, where
we observed up to 10% improvement in point forecasting performance and up to
300% improvement in probabilistic forecasting performance."
['stat.ML'],Constrained episodic reinforcement learning in concave-convex and knapsack settings,"We propose an algorithm for tabular episodic reinforcement learning with
constraints. We provide a modular analysis with strong theoretical guarantees
for settings with concave rewards and convex constraints, and for settings with
hard constraints (knapsacks). Most of the previous work in constrained
reinforcement learning is limited to linear constraints, and the remaining work
focuses on either the feasibility question or settings with a single episode.
Our experiments demonstrate that the proposed algorithm significantly
outperforms these approaches in existing constrained episodic environments."
['stat.ML'],Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning,"We challenge a common assumption underlying most supervised deep learning:
that a model makes a prediction depending only on its parameters and the
features of a single input. To this end, we introduce a general-purpose deep
learning architecture that takes as input the entire dataset instead of
processing one datapoint at a time. Our approach uses self-attention to reason
about relationships between datapoints explicitly, which can be seen as
realizing non-parametric models using parametric attention mechanisms. However,
unlike conventional non-parametric models, we let the model learn end-to-end
from the data how to make use of other datapoints for prediction. Empirically,
our models solve cross-datapoint lookup and complex reasoning tasks unsolvable
by traditional deep learning models. We show highly competitive results on
tabular data, early results on CIFAR-10, and give insight into how the model
makes use of the interactions between points."
['stat.ML'],SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training,"Tabular data underpins numerous high-impact applications of machine learning
from fraud detection to genomics and healthcare. Classical approaches to
solving tabular problems, such as gradient boosting and random forests, are
widely used by practitioners. However, recent deep learning methods have
achieved a degree of performance competitive with popular techniques. We devise
a hybrid deep learning approach to solving tabular data problems. Our method,
SAINT, performs attention over both rows and columns, and it includes an
enhanced embedding method. We also study a new contrastive self-supervised
pre-training method for use when labels are scarce. SAINT consistently improves
performance over previous deep learning methods, and it even outperforms
gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on
average over a variety of benchmark tasks."
['stat.ML'],FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,"High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud."
['stat.ML'],Fooling Partial Dependence via Data Poisoning,"Many methods have been developed to understand complex predictive models and
high expectations are placed on post-hoc model explainability. It turns out
that such explanations are not robust nor trustworthy, and they can be fooled.
This paper presents techniques for attacking Partial Dependence (plots,
profiles, PDP), which are among the most popular methods of explaining any
predictive model trained on tabular data. We showcase that PD can be
manipulated in an adversarial manner, which is alarming, especially in
financial or medical applications where auditability became a must-have trait
supporting black-box models. The fooling is performed via poisoning the data to
bend and shift explanations in the desired direction using genetic and gradient
algorithms. To the best of our knowledge, this is the first work performing
attacks on variable dependence explanations. The novel approach of using a
genetic algorithm for doing so is highly transferable as it generalizes both
ways: in a model-agnostic and an explanation-agnostic manner."
['stat.ML'],DP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation,"We propose a differentially private data generation paradigm using random
feature representations of kernel mean embeddings when comparing the
distribution of true data with that of synthetic data. We exploit the random
feature representations for two important benefits. First, we require a minimal
privacy cost for training deep generative models. This is because unlike
kernel-based distance metrics that require computing the kernel matrix on all
pairs of true and synthetic data points, we can detach the data-dependent term
from the term solely dependent on synthetic data. Hence, we need to perturb the
data-dependent term only once and then use it repeatedly during the generator
training. Second, we can obtain an analytic sensitivity of the kernel mean
embedding as the random features are norm bounded by construction. This removes
the necessity of hyper-parameter search for a clipping norm to handle the
unknown sensitivity of a generator network. We provide several variants of our
algorithm, differentially-private mean embeddings with random features
(DP-MERF) to jointly generate labels and input features for datasets such as
heterogeneous tabular data and image data. Our algorithm achieves drastically
better privacy-utility trade-offs than existing methods when tested on several
datasets."
"['stat.ML', 'stat.TH']",Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model,"The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper."
['stat.ML'],Accounting for Unobserved Confounding in Domain Generalization,"The ability to generalize from observed to new related environments is
central to any form of reliable machine learning, yet most methods fail when
moving beyond i.i.d data. This work argues that in some cases the reason lies
in a misapreciation of the causal structure in data; and in particular due to
the influence of unobserved confounders which void many of the invariances and
principles of minimum error between environments presently used for the problem
of domain generalization. This observation leads us to study generalization in
the context of a broader class of interventions in an underlying causal model
(including changes in observed, unobserved and target variable distributions)
and to connect this causal intuition with an explicit distributionally robust
optimization problem. From this analysis derives a new proposal for model
learning with explicit generalization guarantees that is based on the partial
equality of error derivatives with respect to model parameters. We demonstrate
the empirical performance of our approach on healthcare data from different
modalities, including image, speech and tabular data."
['stat.ML'],Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL,"While early AutoML frameworks focused on optimizing traditional ML pipelines
and their hyperparameters, a recent trend in AutoML is to focus on neural
architecture search. In this paper, we introduce Auto-PyTorch, which brings the
best of these two worlds together by jointly and robustly optimizing the
architecture of networks and the training hyperparameters to enable fully
automated deep learning (AutoDL). Auto-PyTorch achieves state-of-the-art
performance on several tabular benchmarks by combining multi-fidelity
optimization with portfolio construction for warmstarting and ensembling of
deep neural networks (DNNs) and common baselines for tabular data. To
thoroughly study our assumptions on how to design such an AutoDL system, we
additionally introduce a new benchmark on learning curves for DNNs, dubbed
LCBench, and run extensive ablation studies of the full Auto-PyTorch on typical
AutoML benchmarks, eventually showing that Auto-PyTorch performs better than
several state-of-the-art competitors on average."
['stat.ML'],Deep and interpretable regression models for ordinal outcomes,"Outcomes with a natural order commonly occur in prediction tasks and often
the available input data are a mixture of complex data like images and tabular
predictors. Deep Learning (DL) models are state-of-the-art for image
classification tasks but frequently treat ordinal outcomes as unordered and
lack interpretability. In contrast, classical ordinal regression models
consider the outcome's order and yield interpretable predictor effects but are
limited to tabular data. We present ordinal neural network transformation
models (ONTRAMs), which unite DL with classical ordinal regression approaches.
ONTRAMs are a special case of transformation models and trade off flexibility
and interpretability by additively decomposing the transformation function into
terms for image and tabular data using jointly trained neural networks. The
performance of the most flexible ONTRAM is by definition equivalent to a
standard multi-class DL model trained with cross-entropy while being faster in
training when facing ordinal outcomes. Lastly, we discuss how to interpret
model components for both tabular and image data on two publicly available
datasets."
['stat.ML'],Knowledge Distillation as Semiparametric Inference,"A popular approach to model compression is to train an inexpensive student
model to mimic the class probabilities of a highly accurate but cumbersome
teacher model. Surprisingly, this two-step knowledge distillation process often
leads to higher accuracy than training the student directly on labeled data. To
explain and enhance this phenomenon, we cast knowledge distillation as a
semiparametric inference problem with the optimal student model as the target,
the unknown Bayes class probabilities as nuisance, and the teacher
probabilities as a plug-in nuisance estimate. By adapting modern semiparametric
tools, we derive new guarantees for the prediction error of standard
distillation and develop two enhancements -- cross-fitting and loss correction
-- to mitigate the impact of teacher overfitting and underfitting on student
performance. We validate our findings empirically on both tabular and image
data and observe consistent improvements from our knowledge distillation
enhancements."
['stat.ML'],Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization,"Natural policy gradient (NPG) methods are among the most widely used policy
optimization algorithms in contemporary reinforcement learning. This class of
methods is often applied in conjunction with entropy regularization -- an
algorithmic scheme that encourages exploration -- and is closely related to
soft policy iteration and trust region policy optimization. Despite the
empirical success, the theoretical underpinnings for NPG methods remain limited
even for the tabular setting. This paper develops $\textit{non-asymptotic}$
convergence guarantees for entropy-regularized NPG methods under softmax
parameterization, focusing on discounted Markov decision processes (MDPs).
Assuming access to exact policy evaluation, we demonstrate that the algorithm
converges linearly -- or even quadratically once it enters a local region
around the optimal policy -- when computing optimal value functions of the
regularized MDP. Moreover, the algorithm is provably stable vis-\`a-vis
inexactness of policy evaluation. Our convergence results accommodate a wide
range of learning rates, and shed light upon the role of entropy regularization
in enabling fast convergence."
['stat.ML'],Uncertainty in Gradient Boosting via Ensembles,"For many practical, high-risk applications, it is essential to quantify
uncertainty in a model's predictions to avoid costly mistakes. While predictive
uncertainty is widely studied for neural networks, the topic seems to be
under-explored for models based on gradient boosting. However, gradient
boosting often achieves state-of-the-art results on tabular data. This work
examines a probabilistic ensemble-based framework for deriving uncertainty
estimates in the predictions of gradient boosting classification and regression
models. We conducted experiments on a range of synthetic and real datasets and
investigated the applicability of ensemble approaches to gradient boosting
models that are themselves ensembles of decision trees. Our analysis shows that
ensembles of gradient boosting models successfully detect anomalous inputs
while having limited ability to improve the predicted total uncertainty.
Importantly, we also propose a concept of a virtual ensemble to get the
benefits of an ensemble via only one gradient boosting model, which
significantly reduces complexity."
['stat.ML'],Holdout-Based Fidelity and Privacy Assessment of Mixed-Type Synthetic Data,"AI-based data synthesis has seen rapid progress over the last several years,
and is increasingly recognized for its promise to enable privacy-respecting
high-fidelity data sharing. However, adequately evaluating the quality of
generated synthetic datasets is still an open challenge. We introduce and
demonstrate a holdout-based empirical assessment framework for quantifying the
fidelity as well as the privacy risk of synthetic data solutions for mixed-type
tabular data. Measuring fidelity is based on statistical distances of
lower-dimensional marginal distributions, which provide a model-free and
easy-to-communicate empirical metric for the representativeness of a synthetic
dataset. Privacy risk is assessed by calculating the individual-level distances
to closest record with respect to the training data. By showing that the
synthetic samples are just as close to the training as to the holdout data, we
yield strong evidence that the synthesizer indeed learned to generalize
patterns and is independent of individual training records. We demonstrate the
presented framework for seven distinct synthetic data solutions across four
mixed-type datasets and compare these to more traditional statistical
disclosure techniques. The results highlight the need to systematically assess
the fidelity just as well as the privacy of these emerging class of synthetic
data generators."
['stat.ML'],Individually Fair Gradient Boosting,"We consider the task of enforcing individual fairness in gradient boosting.
Gradient boosting is a popular method for machine learning from tabular data,
which arise often in applications where algorithmic fairness is a concern. At a
high level, our approach is a functional gradient descent on a
(distributionally) robust loss function that encodes our intuition of
algorithmic fairness for the ML task at hand. Unlike prior approaches to
individual fairness that only work with smooth ML models, our approach also
works with non-smooth models such as decision trees. We show that our algorithm
converges globally and generalizes. We also demonstrate the efficacy of our
algorithm on three ML problems susceptible to algorithmic bias."
['stat.ML'],Logical Team Q-learning: An approach towards factored policies in cooperative MARL,"We address the challenge of learning factored policies in cooperative MARL
scenarios. In particular, we consider the situation in which a team of agents
collaborates to optimize a common cost. The goal is to obtain factored policies
that determine the individual behavior of each agent so that the resulting
joint policy is optimal. The main contribution of this work is the introduction
of Logical Team Q-learning (LTQL). LTQL does not rely on assumptions about the
environment and hence is generally applicable to any collaborative MARL
scenario. We derive LTQL as a stochastic approximation to a dynamic programming
method we introduce in this work. We conclude the paper by providing
experiments (both in the tabular and deep settings) that illustrate the claims."
['stat.ML'],Nearly Horizon-Free Offline Reinforcement Learning,"We revisit offline reinforcement learning on episodic time-homogeneous
tabular Markov Decision Processes with $S$ states, $A$ actions and planning
horizon $H$. Given the collected $N$ episodes data with minimum cumulative
reaching probability $d_m$, we obtain the first set of nearly $H$-free sample
complexity bounds for evaluation and planning using the empirical MDPs: 1.For
the offline evaluation, we obtain an $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}}
\right)$ error rate, which matches the lower bound and does not have additional
dependency on $\poly\left(S,A\right)$ in higher-order term, that is different
from previous works~\citep{yin2020near,yin2020asymptotically}. 2.For the
offline policy optimization, we obtain an $\tilde{O}\left(\sqrt{\frac{1}{Nd_m}}
+ \frac{S}{Nd_m}\right)$ error rate, improving upon the best known result by
\cite{cui2020plug}, which has additional $H$ and $S$ factors in the main term.
Furthermore, this bound approaches the
$\Omega\left(\sqrt{\frac{1}{Nd_m}}\right)$ lower bound up to logarithmic
factors and a high-order term. To the best of our knowledge, these are the
first set of nearly horizon-free bounds in offline reinforcement learning."
['stat.ML'],i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning,"Contrastive representation learning has shown to be effective to learn
representations from unlabeled data. However, much progress has been made in
vision domains relying on data augmentations carefully designed using domain
knowledge. In this work, we propose i-Mix, a simple yet effective
domain-agnostic regularization strategy for improving contrastive
representation learning. We cast contrastive learning as training a
non-parametric classifier by assigning a unique virtual class to each data in a
batch. Then, data instances are mixed in both the input and virtual label
spaces, providing more augmented data during training. In experiments, we
demonstrate that i-Mix consistently improves the quality of learned
representations across domains, including image, speech, and tabular data.
Furthermore, we confirm its regularization effect via extensive ablation
studies across model and dataset sizes. The code is available at
https://github.com/kibok90/imix."
['stat.ML'],Fair Mixup: Fairness via Interpolation,"Training classifiers under fairness constraints such as group fairness,
regularizes the disparities of predictions between the groups. Nevertheless,
even though the constraints are satisfied during training, they might not
generalize at evaluation time. To improve the generalizability of fair
classifiers, we propose fair mixup, a new data augmentation strategy for
imposing the fairness constraint. In particular, we show that fairness can be
achieved by regularizing the models on paths of interpolated samples between
the groups. We use mixup, a powerful data augmentation strategy to generate
these interpolates. We analyze fair mixup and empirically show that it ensures
a better generalization for both accuracy and fairness measurement in tabular,
vision, and language benchmarks."
['stat.ML'],UCB Momentum Q-learning: Correcting the bias without forgetting,"We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm
for reinforcement learning in tabular and possibly stage-dependent, episodic
Markov decision process. UCBMQ is based on Q-learning where we add a momentum
term and rely on the principle of optimism in face of uncertainty to deal with
exploration. Our new technical ingredient of UCBMQ is the use of momentum to
correct the bias that Q-learning suffers while, at the same time, limiting the
impact it has on the second-order term of the regret. For UCBMQ , we are able
to guarantee a regret of at most $O(\sqrt{H^3SAT}+ H^4 S A )$ where $H$ is the
length of an episode, $S$ the number of states, $A$ the number of actions, $T$
the number of episodes and ignoring terms in poly$log(SAHT)$. Notably, UCBMQ is
the first algorithm that simultaneously matches the lower bound of
$\Omega(\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with
respect to the horizon $T$) that scales only linearly with the number of states
$S$."
['stat.ML'],Semi-Structured Deep Piecewise Exponential Models,"We propose a versatile framework for survival analysis that combines advanced
concepts from statistics with deep learning. The presented framework is based
on piecewise exponential models and thereby supports various survival tasks,
such as competing risks and multi-state modeling, and further allows for
estimation of time-varying effects and time-varying features. To also include
multiple data sources and higher-order interaction effects into the model, we
embed the model class in a neural network and thereby enable the simultaneous
estimation of both inherently interpretable structured regression inputs as
well as deep neural network components which can potentially process additional
unstructured data sources. A proof of concept is provided by using the
framework to predict Alzheimer's disease progression based on tabular and 3D
point cloud data and applying it to synthetic data."
['stat.ML'],Recreation of the Periodic Table with an Unsupervised Machine Learning Algorithm,"In 1869, the first draft of the periodic table was published by Russian
chemist Dmitri Mendeleev. In terms of data science, his achievement can be
viewed as a successful example of feature embedding based on human cognition:
chemical properties of all known elements at that time were compressed onto the
two-dimensional grid system for tabular display. In this study, we seek to
answer the question of whether machine learning can reproduce or recreate the
periodic table by using observed physicochemical properties of the elements. To
achieve this goal, we developed a periodic table generator (PTG). The PTG is an
unsupervised machine learning algorithm based on the generative topographic
mapping (GTM), which can automate the translation of high-dimensional data into
a tabular form with varying layouts on-demand. The PTG autonomously produced
various arrangements of chemical symbols, which organized a two-dimensional
array such as Mendeleev's periodic table or three-dimensional spiral table
according to the underlying periodicity in the given data. We further showed
what the PTG learned from the element data and how the element features, such
as melting point and electronegativity, are compressed to the lower-dimensional
latent spaces."
['stat.ML'],RAB: Provable Robustness Against Backdoor Attacks,"Recent studies have shown that deep neural networks (DNNs) are highly
vulnerable to adversarial attacks, including evasion and backdoor (poisoning)
attacks. On the defense side, there have been intensive interests in both
empirical and provable robustness against evasion attacks; however, provable
robustness against backdoor attacks remains largely unexplored. In this paper,
we focus on certifying robustness against backdoor attacks. To this end, we
first provide a unified framework for robustness certification and show that it
leads to a tight robustness condition for backdoor attacks. We then propose the
first robust training process, RAB, to smooth the trained model and certify its
robustness against backdoor attacks. Moreover, we evaluate the certified
robustness of a family of ""smoothed"" models which are trained in a
differentially private fashion, and show that they achieve better certified
robustness bounds. In addition, we theoretically show that it is possible to
train the robust smoothed models efficiently for simple models such as
K-nearest neighbor classifiers, and we propose an exact smooth-training
algorithm which eliminates the need to sample from a noise distribution.
Empirically, we conduct comprehensive experiments for different machine
learning (ML) models such as DNNs, differentially private DNNs, and K-NN models
on MNIST, CIFAR-10 and ImageNet datasets (focusing on binary classifiers), and
provide the first benchmark for certified robustness against backdoor attacks.
In addition, we evaluate K-NN models on a spambase tabular dataset to
demonstrate the advantages of the proposed exact algorithm. Both the
theoretical analysis and the comprehensive benchmark on diverse ML models and
datasets shed lights on further robust learning strategies against training
time attacks or other general adversarial attacks."
['stat.ML'],Data Preprocessing to Mitigate Bias with Boosted Fair Mollifiers,"In a recent paper, Celis et al. (2020) introduced a new approach to fairness
that corrects the data distribution itself. The approach is computationally
appealing, but its approximation guarantees with respect to the target
distribution can be quite loose as they need to rely on a (typically limited)
number of constraints on data-based aggregated statistics; also resulting in a
fairness guarantee which can be data dependent.
  Our paper makes use of a mathematical object recently introduced in privacy
-- mollifiers of distributions -- and a popular approach to machine learning --
boosting -- to get an approach in the same lineage as Celis et al. but without
the same impediments, including in particular, better guarantees in terms of
accuracy and finer guarantees in terms of fairness. The approach involves
learning the sufficient statistics of an exponential family. When the training
data is tabular, the sufficient statistics can be defined by decision trees
whose interpretability can provide clues on the source of (un)fairness.
Experiments display the quality of the results for simulated and real-world
data."
['stat.ML'],$Q$-learning with Logarithmic Regret,"This paper presents the first non-asymptotic result showing that a model-free
algorithm can achieve a logarithmic cumulative regret for episodic tabular
reinforcement learning if there exists a strictly positive sub-optimality gap
in the optimal $Q$-function. We prove that the optimistic $Q$-learning studied
in [Jin et al. 2018] enjoys a ${\mathcal{O}}\left(\frac{SA\cdot
\mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$
cumulative regret bound, where $S$ is the number of states, $A$ is the number
of actions, $H$ is the planning horizon, $T$ is the total number of steps, and
$\Delta_{\min}$ is the minimum sub-optimality gap. This bound matches the
information theoretical lower bound in terms of $S,A,T$ up to a
$\log\left(SA\right)$ factor. We further extend our analysis to the discounted
setting and obtain a similar logarithmic cumulative regret bound."
['stat.ML'],Nearly Minimax Optimal Reinforcement Learning for Discounted MDPs,"We study the reinforcement learning problem for discounted Markov Decision
Processes (MDPs) under the tabular setting. We propose a model-based algorithm
named UCBVI-$\gamma$, which is based on the \emph{optimism in the face of
uncertainty principle} and the Bernstein-type bonus. We show that
UCBVI-$\gamma$ achieves an $\tilde{O}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$
regret, where $S$ is the number of states, $A$ is the number of actions,
$\gamma$ is the discount factor and $T$ is the number of steps. In addition, we
construct a class of hard MDPs and show that for any algorithm, the expected
regret is at least $\tilde{\Omega}\big({\sqrt{SAT}}/{(1-\gamma)^{1.5}}\big)$.
Our upper bound matches the minimax lower bound up to logarithmic factors,
which suggests that UCBVI-$\gamma$ is nearly minimax optimal for discounted
MDPs."
"['stat.ML', 'stat.TH']",Bootstrapping Statistical Inference for Off-Policy Evaluation,"Bootstrapping provides a flexible and effective approach for assessing the
quality of batch reinforcement learning, yet its theoretical property is less
understood. In this paper, we study the use of bootstrapping in off-policy
evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE)
that is known to be minimax-optimal in the tabular and linear-model cases. We
propose a bootstrapping FQE method for inferring the distribution of the policy
evaluation error and show that this method is asymptotically efficient and
distributionally consistent for off-policy statistical inference. To overcome
the computation limit of bootstrapping, we further adapt a subsampling
procedure that improves the runtime by an order of magnitude. We numerically
evaluate the bootrapping method in classical RL environments for confidence
interval estimation, estimating the variance of off-policy evaluator, and
estimating the correlation between multiple off-policy evaluators."
['stat.ML'],Model Rectification via Unknown Unknowns Extraction from Deployment Samples,"Model deficiency that results from incomplete training data is a form of
structural blindness that leads to costly errors, oftentimes with high
confidence. During the training of classification tasks, underrepresented
class-conditional distributions that a given hypothesis space can recognize
results in a mismatch between the model and the target space. To mitigate the
consequences of this discrepancy, we propose Random Test Sampling and
Cross-Validation (RTSCV) as a general algorithmic framework that aims to
perform a post-training model rectification at deployment time in a supervised
way. RTSCV extracts unknown unknowns (u.u.s), i.e., examples from the
class-conditional distributions that a classifier is oblivious to, and works in
combination with a diverse family of modern prediction models. RTSCV augments
the training set with a sample of the test set (or deployment data) and uses
this redefined class layout to discover u.u.s via cross-validation, without
relying on active learning or budgeted queries to an oracle. We contribute a
theoretical analysis that establishes performance guarantees based on the
design bases of modern classifiers. Our experimental evaluation demonstrates
RTSCV's effectiveness, using 7 benchmark tabular and computer vision datasets,
by reducing a performance gap as large as 41% from the respective
pre-rectification models. Last we show that RTSCV consistently outperforms
state-of-the-art approaches."
"['stat.ML', 'stat.TH']","Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-Order Efficiency","We offer a theoretical characterization of off-policy evaluation (OPE) in
reinforcement learning using function approximation for marginal importance
weights and $q$-functions when these are estimated using recent minimax
methods. Under various combinations of realizability and completeness
assumptions, we show that the minimax approach enables us to achieve a fast
rate of convergence for weights and quality functions, characterized by the
critical inequality \citep{bartlett2005}. Based on this result, we analyze
convergence rates for OPE. In particular, we introduce novel alternative
completeness conditions under which OPE is feasible and we present the first
finite-sample result with first-order efficiency in non-tabular environments,
i.e., having the minimal coefficient in the leading term."
['stat.ML'],Fast Rates for the Regret of Offline Reinforcement Learning,"We study the regret of reinforcement learning from offline data generated by
a fixed behavior policy in an infinite-horizon discounted Markov decision
process (MDP). While existing analyses of common approaches, such as fitted
$Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret,
empirical behavior exhibits much faster convergence. In this paper, we present
a finer regret analysis that exactly characterizes this phenomenon by providing
fast rates for the regret convergence. First, we show that given any estimate
for the optimal quality function $Q^*$, the regret of the policy it defines
converges at a rate given by the exponentiation of the $Q^*$-estimate's
pointwise convergence rate, thus speeding it up. The level of exponentiation
depends on the level of noise in the decision-making problem, rather than the
estimation problem. We establish such noise levels for linear and tabular MDPs
as examples. Second, we provide new analyses of FQI and Bellman residual
minimization to establish the correct pointwise convergence guarantees. As
specific cases, our results imply $O(1/n)$ regret rates in linear cases and
$\exp(-\Omega(n))$ regret rates in tabular cases."
['stat.ML'],Decision Machines: Interpreting Decision Tree as a Model Combination Method,"Based on decision trees, it is efficient to handle tabular data. Conventional
decision tree growth methods often result in suboptimal trees because of their
greedy nature. Their inherent structure limits the options of hardware to
implement decision trees in parallel. Here is a compact representation of
binary decision trees to overcome these deficiencies. We explicitly formulate
the dependence of prediction on binary tests for binary decision trees and
construct a function to guide the input sample from the root to the appropriate
leaf node. And based on this formulation we introduce a new interpretation of
binary decision trees. Then we approximate this formulation via continuous
functions. Finally, we interpret the decision tree as a model combination
method. And we propose the selection-prediction scheme to unify a few learning
methods."
['stat.ML'],Conditional Generative Models for Counterfactual Explanations,"Counterfactual instances offer human-interpretable insight into the local
behaviour of machine learning models. We propose a general framework to
generate sparse, in-distribution counterfactual model explanations which match
a desired target prediction with a conditional generative model, allowing
batches of counterfactual instances to be generated with a single forward pass.
The method is flexible with respect to the type of generative model used as
well as the task of the underlying predictive model. This allows
straightforward application of the framework to different modalities such as
images, time series or tabular data as well as generative model paradigms such
as GANs or autoencoders and predictive tasks like classification or regression.
We illustrate the effectiveness of our method on image (CelebA), time series
(ECG) and mixed-type tabular (Adult Census) data."
['stat.ML'],DNN2LR: Interpretation-inspired Feature Crossing for Real-world Tabular Data,"For sake of reliability, it is necessary for models in real-world
applications to be both powerful and globally interpretable. Simple
classifiers, e.g., Logistic Regression (LR), are globally interpretable, but
not powerful enough to model complex nonlinear interactions among features in
tabular data. Meanwhile, Deep Neural Networks (DNNs) have shown great
effectiveness for modeling tabular data, but is not globally interpretable. In
this work, we find local piece-wise interpretations in DNN of a specific
feature are usually inconsistent in different samples, which is caused by
feature interactions in the hidden layers. Accordingly, we can design an
automatic feature crossing method to find feature interactions in DNN, and use
them as cross features in LR. We give definition of the interpretation
inconsistency in DNN, based on which a novel feature crossing method called
DNN2LR is proposed. Extensive experiments have been conducted on four public
datasets and two real-world datasets. The final model, i.e., a LR model
empowered with cross features, generated by DNN2LR can outperform the complex
DNN model, as well as several state-of-the-art feature crossing methods. The
experimental results strongly verify the effectiveness and efficiency of
DNN2LR, especially on real-world datasets with large numbers of feature fields."
['stat.ML'],Matrix Completion with Quantified Uncertainty through Low Rank Gaussian Copula,"Modern large scale datasets are often plagued with missing entries. For
tabular data with missing values, a flurry of imputation algorithms solve for a
complete matrix which minimizes some penalized reconstruction error. However,
almost none of them can estimate the uncertainty of its imputations. This paper
proposes a probabilistic and scalable framework for missing value imputation
with quantified uncertainty. Our model, the Low Rank Gaussian Copula, augments
a standard probabilistic model, Probabilistic Principal Component Analysis,
with marginal transformations for each column that allow the model to better
match the distribution of the data. It naturally handles Boolean, ordinal, and
real-valued observations and quantifies the uncertainty in each imputation. The
time required to fit the model scales linearly with the number of rows and the
number of columns in the dataset. Empirical results show the method yields
state-of-the-art imputation accuracy across a wide range of data types,
including those with high rank. Our uncertainty measure predicts imputation
error well: entries with lower uncertainty do have lower imputation error (on
average). Moreover, for real-valued data, the resulting confidence intervals
are well-calibrated."
['stat.ML'],A Provably Efficient Algorithm for Linear Markov Decision Process with Low Switching Cost,"Many real-world applications, such as those in medical domains,
recommendation systems, etc, can be formulated as large state space
reinforcement learning problems with only a small budget of the number of
policy changes, i.e., low switching cost. This paper focuses on the linear
Markov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al
2020] where the linear function approximation is used for generalization on the
large state space. We present the first algorithm for linear MDP with a low
switching cost. Our algorithm achieves an
$\widetilde{O}\left(\sqrt{d^3H^4K}\right)$ regret bound with a near-optimal
$O\left(d H\log K\right)$ global switching cost where $d$ is the feature
dimension, $H$ is the planning horizon and $K$ is the number of episodes the
agent plays. Our regret bound matches the best existing polynomial algorithm by
[Jin et al 2020] and our switching cost is exponentially smaller than theirs.
When specialized to tabular MDP, our switching cost bound improves those in
[Bai et al 2019, Zhang et al 20020]. We complement our positive result with an
$\Omega\left(dH/\log d\right)$ global switching cost lower bound for any
no-regret algorithm."
"['stat.ML', 'stat.TH']",On Function Approximation in Reinforcement Learning: Optimism in the Face of Large State Spaces,"The classical theory of reinforcement learning (RL) has focused on tabular
and linear representations of value functions. Further progress hinges on
combining RL with modern function approximators such as kernel functions and
deep neural networks, and indeed there have been many empirical successes that
have exploited such combinations in large-scale applications. There are
profound challenges, however, in developing a theory to support this
enterprise, most notably the need to take into consideration the
exploration-exploitation tradeoff at the core of RL in conjunction with the
computational and statistical tradeoffs that arise in modern
function-approximation-based learning systems. We approach these challenges by
studying an optimistic modification of the least-squares value iteration
algorithm, in the context of the action-value function
  represented by a kernel function or an overparameterized neural network. We
establish both polynomial runtime complexity and polynomial sample complexity
for this algorithm, without additional assumptions on the data-generating
model. In particular, we prove that the algorithm incurs an
$\tilde{\mathcal{O}}(\delta_{\mathcal{F}} H^2 \sqrt{T})$ regret, where
$\delta_{\mathcal{F}}$ characterizes the intrinsic complexity of the function
class $\mathcal{F}$, $H$ is the length of each episode, and $T$ is the total
number of episodes. Our regret bounds are independent of the number of states,
a result which exhibits clearly the benefit of function approximation in RL."
['stat.ML'],Unsupervised Functional Data Analysis via Nonlinear Dimension Reduction,"In recent years, manifold methods have moved into focus as tools for
dimension reduction. Assuming that the high-dimensional data actually lie on or
close to a low-dimensional nonlinear manifold, these methods have shown
convincing results in several settings. This manifold assumption is often
reasonable for functional data, i.e., data representing continuously observed
functions, as well. However, the performance of manifold methods recently
proposed for tabular or image data has not been systematically assessed in the
case of functional data yet. Moreover, it is unclear how to evaluate the
quality of learned embeddings that do not yield invertible mappings, since the
reconstruction error cannot be used as a performance measure for such
representations. In this work, we describe and investigate the specific
challenges for nonlinear dimension reduction posed by the functional data
setting. The contributions of the paper are three-fold: First of all, we define
a theoretical framework which allows to systematically assess specific
challenges that arise in the functional data context, transfer several
nonlinear dimension reduction methods for tabular and image data to functional
data, and show that manifold methods can be used successfully in this setting.
Secondly, we subject performance assessment and tuning strategies to a thorough
and systematic evaluation based on several different functional data settings
and point out some previously undescribed weaknesses and pitfalls which can
jeopardize reliable judgment of embedding quality. Thirdly, we propose a
nuanced approach to make trustworthy decisions for or against competing
nonconforming embeddings more objectively."
['stat.ML'],When stakes are high: balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates,"Highly regulated industries, like banking and insurance, ask for transparent
decision-making algorithms. At the same time, competitive markets are pushing
for the use of complex black box models. We therefore present a procedure to
develop a Model-Agnostic Interpretable Data-driven suRRogate (maidrr) suited
for structured tabular data. Knowledge is extracted from a black box via
partial dependence effects. These are used to perform smart feature engineering
by grouping variable values. This results in a segmentation of the feature
space with automatic variable selection. A transparent generalized linear model
(GLM) is fit to the features in categorical format and their relevant
interactions. We demonstrate our R package maidrr with a case study on general
insurance claim frequency modeling for six publicly available datasets. Our
maidrr GLM closely approximates a gradient boosting machine (GBM) black box and
outperforms both a linear and tree surrogate as benchmarks."
['stat.ML'],TabNet: Attentive Interpretable Tabular Learning,"We propose a novel high-performance and interpretable canonical deep tabular
data learning architecture, TabNet. TabNet uses sequential attention to choose
which features to reason from at each decision step, enabling interpretability
and more efficient learning as the learning capacity is used for the most
salient features. We demonstrate that TabNet outperforms other neural network
and decision tree variants on a wide range of non-performance-saturated tabular
datasets and yields interpretable feature attributions plus insights into the
global model behavior. Finally, for the first time to our knowledge, we
demonstrate self-supervised learning for tabular data, significantly improving
performance with unsupervised representation learning when unlabeled data is
abundant."
['stat.ML'],Joint Policy Search for Multi-agent Collaboration with Imperfect Information,"To learn good joint policies for multi-agent collaboration with imperfect
information remains a fundamental challenge. While for two-player zero-sum
games, coordinate-ascent approaches (optimizing one agent's policy at a time,
e.g., self-play) work with guarantees, in multi-agent cooperative setting they
often converge to sub-optimal Nash equilibrium. On the other hand, directly
modeling joint policy changes in imperfect information game is nontrivial due
to complicated interplay of policies (e.g., upstream updates affect downstream
state reachability). In this paper, we show global changes of game values can
be decomposed to policy changes localized at each information set, with a novel
term named policy-change density. Based on this, we propose Joint Policy
Search(JPS) that iteratively improves joint policies of collaborative agents in
imperfect information games, without re-evaluating the entire game. On
multi-agent collaborative tabular games, JPS is proven to never worsen
performance and can improve solutions provided by unilateral approaches (e.g,
CFR), outperforming algorithms designed for collaborative policy learning (e.g.
BAD). Furthermore, for real-world games, JPS has an online form that naturally
links with gradient updates. We test it to Contract Bridge, a 4-player
imperfect-information game where a team of $2$ collaborates to compete against
the other. In its bidding phase, players bid in turn to find a good contract
through a limited information channel. Based on a strong baseline agent that
bids competitive bridge purely through domain-agnostic self-play, JPS improves
collaboration of team players and outperforms WBridge5, a championship-winning
software, by $+0.63$ IMPs (International Matching Points) per board over 1k
games, substantially better than previous SoTA ($+0.41$ IMPs/b) under
Double-Dummy evaluation."
['stat.ML'],Learning Interpretable Concept-Based Models with Human Feedback,"Machine learning models that first learn a representation of a domain in
terms of human-understandable concepts, then use it to make predictions, have
been proposed to facilitate interpretation and interaction with models trained
on high-dimensional data. However these methods have important limitations: the
way they define concepts are not inherently interpretable, and they assume that
concept labels either exist for individual instances or can easily be acquired
from users. These limitations are particularly acute for high-dimensional
tabular features. We propose an approach for learning a set of transparent
concept definitions in high-dimensional tabular data that relies on users
labeling concept features instead of individual instances. Our method produces
concepts that both align with users' intuitive sense of what a concept means,
and facilitate prediction of the downstream label by a transparent machine
learning model. This ensures that the full model is transparent and intuitive,
and as predictive as possible given this constraint. We demonstrate with
simulated user feedback on real prediction problems, including one in a
clinical domain, that this kind of direct feedback is much more efficient at
learning solutions that align with ground truth concept definitions than
alternative transparent approaches that rely on labeling instances or other
existing interaction mechanisms, while maintaining similar predictive
performance."
['stat.ML'],Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning,"Motivated by the prevailing paradigm of using unsupervised learning for
efficient exploration in reinforcement learning (RL) problems
[tang2017exploration,bellemare2016unifying], we investigate when this paradigm
is provably efficient. We study episodic Markov decision processes with rich
observations generated from a small number of latent states. We present a
general algorithmic framework that is built upon two components: an
unsupervised learning algorithm and a no-regret tabular RL algorithm.
Theoretically, we prove that as long as the unsupervised learning algorithm
enjoys a polynomial sample complexity guarantee, we can find a near-optimal
policy with sample complexity polynomial in the number of latent states, which
is significantly smaller than the number of observations. Empirically, we
instantiate our framework on a class of hard exploration problems to
demonstrate the practicality of our theory."
['stat.ML'],GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values,"We present GradientDICE for estimating the density ratio between the state
distribution of the target policy and the sampling distribution in off-policy
reinforcement learning. GradientDICE fixes several problems of GenDICE (Zhang
et al., 2020), the state-of-the-art for estimating such density ratios. Namely,
the optimization problem in GenDICE is not a convex-concave saddle-point
problem once nonlinearity in optimization variable parameterization is
introduced to ensure positivity, so any primal-dual algorithm is not guaranteed
to converge or find the desired solution. However, such nonlinearity is
essential to ensure the consistency of GenDICE even with a tabular
representation. This is a fundamental contradiction, resulting from GenDICE's
original formulation of the optimization problem. In GradientDICE, we optimize
a different objective from GenDICE by using the Perron-Frobenius theorem and
eliminating GenDICE's use of divergence. Consequently, nonlinearity in
parameterization is not necessary for GradientDICE, which is provably
convergent under linear function approximation."
['stat.ML'],Group-Connected Multilayer Perceptron Networks,"Despite the success of deep learning in domains such as image, voice, and
graphs, there has been little progress in deep representation learning for
domains without a known structure between features. For instance, a tabular
dataset of different demographic and clinical factors where the feature
interactions are not given as a prior. In this paper, we propose
Group-Connected Multilayer Perceptron (GMLP) networks to enable deep
representation learning in these domains. GMLP is based on the idea of learning
expressive feature combinations (groups) and exploiting them to reduce the
network complexity by defining local group-wise operations. During the training
phase, GMLP learns a sparse feature grouping matrix using temperature annealing
softmax with an added entropy loss term to encourage the sparsity. Furthermore,
an architecture is suggested which resembles binary trees, where group-wise
operations are followed by pooling operations to combine information; reducing
the number of groups as the network grows in depth. To evaluate the proposed
method, we conducted experiments on different real-world datasets covering
various application areas. Additionally, we provide visualizations on MNIST and
synthesized data. According to the results, GMLP is able to successfully learn
and exploit expressive feature combinations and achieve state-of-the-art
classification performance on different datasets."
['stat.ML'],Active Reinforcement Learning: Observing Rewards at a Cost,"Active reinforcement learning (ARL) is a variant on reinforcement learning
where the agent does not observe the reward unless it chooses to pay a query
cost c > 0. The central question of ARL is how to quantify the long-term value
of reward information. Even in multi-armed bandits, computing the value of this
information is intractable and we have to rely on heuristics. We propose and
evaluate several heuristic approaches for ARL in multi-armed bandits and
(tabular) Markov decision processes, and discuss and illustrate some
challenging aspects of the ARL problem."
['stat.ML'],Sampling Techniques in Bayesian Target Encoding,"Target encoding is an effective encoding technique of categorical variables
and is often used in machine learning systems for processing tabular data sets
with mixed numeric and categorical variables. Recently en enhanced version of
this encoding technique was proposed by using conjugate Bayesian modeling. This
paper presents a further development of Bayesian encoding method by using
sampling techniques, which helps in extracting information from intra-category
distribution of the target variable, improves generalization and reduces target
leakage."
['stat.ML'],Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD Detection On Medical Tabular Data,"When deploying machine learning models in high-stakes real-world environments
such as health care, it is crucial to accurately assess the uncertainty
concerning a model's prediction on abnormal inputs. However, there is a
scarcity of literature analyzing this problem on medical data, especially on
mixed-type tabular data such as Electronic Health Records. We close this gap by
presenting a series of tests including a large variety of contemporary
uncertainty estimation techniques, in order to determine whether they are able
to identify out-of-distribution (OOD) patients. In contrast to previous work,
we design tests on realistic and clinically relevant OOD groups, and run
experiments on real-world medical data. We find that almost all techniques fail
to achieve convincing results, partly disagreeing with earlier findings."
['stat.ML'],AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with Autotuned Data-Parallel Training for Tabular Data,"Developing high-performing predictive models for large tabular data sets is a
challenging task. The state-of-the-art methods are based on expert-developed
model ensembles from different supervised learning methods. Recently, automated
machine learning (AutoML) is emerging as a promising approach to automate
predictive model development. Neural architecture search (NAS) is an AutoML
approach that generates and evaluates multiple neural network architectures
concurrently and improves the accuracy of the generated models iteratively. A
key issue in NAS, particularly for large data sets, is the large computation
time required to evaluate each generated architecture. While data-parallel
training is a promising approach that can address this issue, its use within
NAS is difficult. For different data sets, the data-parallel training settings
such as the number of parallel processes, learning rate, and batch size need to
be adapted to achieve high accuracy and reduction in training time. To that
end, we have developed AgEBO-Tabular, an approach to combine aging evolution
(AgE), a parallel NAS method that searches over neural architecture space, and
an asynchronous Bayesian optimization method for tuning the hyperparameters of
the data-parallel training simultaneously. We demonstrate the efficacy of the
proposed method to generate high-performing neural network models for large
tabular benchmark data sets. Furthermore, we demonstrate that the automatically
discovered neural network models using our method outperform the
state-of-the-art AutoML ensemble models in inference speed by two orders of
magnitude while reaching similar accuracy values."
['stat.ML'],Q-FIT: The Quantifiable Feature Importance Technique for Explainable Machine Learning,"We introduce a novel framework to quantify the importance of each input
feature for model explainability. A user of our framework can choose between
two modes: (a) global explanation: providing feature importance globally across
all the data points; and (b) local explanation: providing feature importance
locally for each individual data point. The core idea of our method comes from
utilizing the Dirichlet distribution to define a distribution over the
importance of input features. This particular distribution is useful in ranking
the importance of the input features as a sample from this distribution is a
probability vector (i.e., the vector components sum to 1), Thus, the ranking
uncovered by our framework which provides a \textit{quantifiable explanation}
of how significant each input feature is to a model's output. This quantifiable
explainability differentiates our method from existing feature-selection
methods, which simply determine whether a feature is relevant or not.
Furthermore, a distribution over the explanation allows to define a closed-form
divergence to measure the similarity between learned feature importance under
different models. We use this divergence to study how the feature importance
trade-offs with essential notions in modern machine learning, such as privacy
and fairness. We show the effectiveness of our method on a variety of synthetic
and real datasets, taking into account both tabular and image datasets."
['stat.ML'],GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction,"Despite the recent development in the topic of explainable AI/ML for image
and text data, the majority of current solutions are not suitable to explain
the prediction of neural network models when the datasets are tabular and their
features are in high-dimensional vectorized formats. To mitigate this
limitation, therefore, we borrow two notable ideas (i.e., ""explanation by
intervention"" from causality and ""explanation are contrastive"" from philosophy)
and propose a novel solution, named as GRACE, that better explains neural
network models' predictions for tabular datasets. In particular, given a
model's prediction as label X, GRACE intervenes and generates a
minimally-modified contrastive sample to be classified as Y, with an intuitive
textual explanation, answering the question of ""Why X rather than Y?"" We carry
out comprehensive experiments using eleven public datasets of different scales
and domains (e.g., # of features ranges from 5 to 216) and compare GRACE with
competing baselines on different measures: fidelity, conciseness, info-gain,
and influence. The user-studies show that our generated explanation is not only
more intuitive and easy-to-understand but also facilitates end-users to make as
much as 60% more accurate post-explanation decisions than that of Lime."
['stat.ML'],Explaining Predictions by Approximating the Local Decision Boundary,"Constructing accurate model-agnostic explanations for opaque machine learning
models remains a challenging task. Classification models for high-dimensional
data, like images, are often inherently complex. To reduce this complexity,
individual predictions may be explained locally, either in terms of a simpler
local surrogate model or by communicating how the predictions contrast with
those of another class. However, existing approaches still fall short in the
following ways: a) they measure locality using a (Euclidean) metric that is not
meaningful for non-linear high-dimensional data; or b) they do not attempt to
explain the decision boundary, which is the most relevant characteristic of
classifiers that are optimized for classification accuracy; or c) they do not
give the user any freedom in specifying attributes that are meaningful to them.
We address these issues in a new procedure for local decision boundary
approximation (DBA). To construct a meaningful metric, we train a variational
autoencoder to learn a Euclidean latent space of encoded data representations.
We impose interpretability by exploiting attribute annotations to map the
latent space to attributes that are meaningful to the user. A difficulty in
evaluating explainability approaches is the lack of a ground truth. We address
this by introducing a new benchmark data set with artificially generated Iris
images, and showing that we can recover the latent attributes that locally
determine the class. We further evaluate our approach on tabular data and on
the CelebA image data set."
['stat.ML'],Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,"QMIX is a popular $Q$-learning algorithm for cooperative MARL in the
centralised training and decentralised execution paradigm. In order to enable
easy decentralisation, QMIX restricts the joint action $Q$-values it can
represent to be a monotonic mixing of each agent's utilities. However, this
restriction prevents it from representing value functions in which an agent's
ordering over its actions can depend on other agents' actions. To analyse this
representational limitation, we first formalise the objective QMIX optimises,
which allows us to view QMIX as an operator that first computes the
$Q$-learning targets and then projects them into the space representable by
QMIX. This projection returns a representable $Q$-value that minimises the
unweighted squared error across all joint actions. We show in particular that
this projection can fail to recover the optimal policy even with access to
$Q^*$, which primarily stems from the equal weighting placed on each joint
action. We rectify this by introducing a weighting into the projection, in
order to place more importance on the better joint actions. We propose two
weighting schemes and prove that they recover the correct maximal action for
any joint action $Q$-values, and therefore for $Q^*$ as well. Based on our
analysis and results in the tabular setting, we introduce two scalable versions
of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW)
QMIX and demonstrate improved performance on both predator-prey and challenging
multi-agent StarCraft benchmark tasks."
['stat.ML'],TraDE: Transformers for Density Estimation,"We present TraDE, a self-attention-based architecture for auto-regressive
density estimation with continuous and discrete valued data. Our model is
trained using a penalized maximum likelihood objective, which ensures that
samples from the density estimate resemble the training data distribution. The
use of self-attention means that the model need not retain conditional
sufficient statistics during the auto-regressive process beyond what is needed
for each covariate. On standard tabular and image data benchmarks, TraDE
produces significantly better density estimates than existing approaches such
as normalizing flow estimators and recurrent auto-regressive models. However
log-likelihood on held-out data only partially reflects how useful these
estimates are in real-world applications. In order to systematically evaluate
density estimators, we present a suite of tasks such as regression using
generated samples, out-of-distribution detection, and robustness to noise in
the training data and demonstrate that TraDE works well in these scenarios."
['stat.ML'],"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift","Policy gradient methods are among the most effective methods in challenging
reinforcement learning problems with large state and/or action spaces. However,
little is known about even their most basic theoretical convergence properties,
including: if and how fast they converge to a globally optimal solution or how
they cope with approximation error due to using a restricted class of
parametric policies. This work provides provable characterizations of the
computational, approximation, and sample size properties of policy gradient
methods in the context of discounted Markov Decision Processes (MDPs). We focus
on both: ""tabular"" policy parameterizations, where the optimal policy is
contained in the class and where we show global convergence to the optimal
policy; and parametric policy classes (considering both log-linear and neural
policy classes), which may not contain the optimal policy and where we provide
agnostic learning results. One central contribution of this work is in
providing approximation guarantees that are average case -- which avoid
explicit worst-case dependencies on the size of state space -- by making a
formal connection to supervised learning under distribution shift. This
characterization shows an important interplay between estimation error,
approximation error, and exploration (as characterized through a precisely
defined condition number)."
['stat.ML'],Minimax Weight and Q-Function Learning for Off-Policy Evaluation,"We provide theoretical investigations into off-policy evaluation in
reinforcement learning using function approximators for (marginalized)
importance weights and value functions. Our contributions include: (1) A new
estimator, MWL, that directly estimates importance ratios over the state-action
distributions, removing the reliance on knowledge of the behavior policy as in
prior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by
swapping the roles of importance weights and value-functions in MWL. MQL has an
intuitive interpretation of minimizing average Bellman errors and can be
combined with MWL in a doubly robust manner. (3) Several additional results
that offer further insights into these methods, including the sample complexity
analyses of MWL and MQL, their asymptotic optimality in the tabular setting,
how the learned importance weights depend the choice of the discriminator
class, and how our methods provide a unified view of some old and new
algorithms in RL."
['stat.ML'],Latent World Models For Intrinsically Motivated Exploration,"In this work we consider partially observable environments with sparse
rewards. We present a self-supervised representation learning method for
image-based observations, which arranges embeddings respecting temporal
distance of observations. This representation is empirically robust to
stochasticity and suitable for novelty detection from the error of a predictive
forward model. We consider episodic and life-long uncertainties to guide the
exploration. We propose to estimate the missing information about the
environment with the world model, which operates in the learned latent space.
As a motivation of the method, we analyse the exploration problem in a tabular
Partially Observable Labyrinth. We demonstrate the method on image-based hard
exploration environments from the Atari benchmark and report significant
improvement with respect to prior work. The source code of the method and all
the experiments is available at https://github.com/htdt/lwm."
['stat.ML'],Attention augmented differentiable forest for tabular data,"Differentiable forest is an ensemble of decision trees with full
differentiability. Its simple tree structure is easy to use and explain. With
full differentiability, it would be trained in the end-to-end learning
framework with gradient-based optimization method. In this paper, we propose
tree attention block(TAB) in the framework of differentiable forest. TAB block
has two operations, squeeze and regulate. The squeeze operation would extract
the characteristic of each tree. The regulate operation would learn nonlinear
relations between these trees. So TAB block would learn the importance of each
tree and adjust its weight to improve accuracy. Our experiment on large tabular
dataset shows attention augmented differentiable forest would get comparable
accuracy with gradient boosted decision trees(GBDT), which is the
state-of-the-art algorithm for tabular datasets. And on some datasets, our
model has higher accuracy than best GBDT libs (LightGBM, Catboost, and
XGBoost). Differentiable forest model supports batch training and batch size is
much smaller than the size of training set. So on larger data sets, its memory
usage is much lower than GBDT model. The source codes are available at
https://github.com/closest-git/QuantumForest."
['stat.ML'],Deep learning for time series classification,"Time series analysis is a field of data science which is interested in
analyzing sequences of numerical values ordered in time. Time series are
particularly interesting because they allow us to visualize and understand the
evolution of a process over time. Their analysis can reveal trends,
relationships and similarities across the data. There exists numerous fields
containing data in the form of time series: health care (electrocardiogram,
blood sugar, etc.), activity recognition, remote sensing, finance (stock market
price), industry (sensors), etc. Time series classification consists of
constructing algorithms dedicated to automatically label time series data. The
sequential aspect of time series data requires the development of algorithms
that are able to harness this temporal property, thus making the existing
off-the-shelf machine learning models for traditional tabular data suboptimal
for solving the underlying task. In this context, deep learning has emerged in
recent years as one of the most effective methods for tackling the supervised
classification task, particularly in the field of computer vision. The main
objective of this thesis was to study and develop deep neural networks
specifically constructed for the classification of time series data. We thus
carried out the first large scale experimental study allowing us to compare the
existing deep methods and to position them compared other non-deep learning
based state-of-the-art methods. Subsequently, we made numerous contributions in
this area, notably in the context of transfer learning, data augmentation,
ensembling and adversarial attacks. Finally, we have also proposed a novel
architecture, based on the famous Inception network (Google), which ranks among
the most efficient to date."
['stat.ML'],Approximate exploitability: Learning a best response in large games,"A standard metric used to measure the approximate optimality of policies in
imperfect information games is exploitability, i.e. the performance of a policy
against its worst-case opponent. However, exploitability is intractable to
compute in large games as it requires a full traversal of the game tree to
calculate a best response to the given policy. We introduce a new metric,
approximate exploitability, that calculates an analogous metric using an
approximate best response; the approximation is done by using search and
reinforcement learning. This is a generalization of local best response, a
domain specific evaluation metric used in poker. We provide empirical results
for a specific instance of the method, demonstrating that our method converges
to exploitability in the tabular and function approximation settings for small
games. In large games, our method learns to exploit both strong and weak
agents, learning to exploit an AlphaZero agent."
['stat.ML'],Learning Representations for Axis-Aligned Decision Forests through Input Perturbation,"Axis-aligned decision forests have long been the leading class of machine
learning algorithms for modeling tabular data. In many applications of machine
learning such as learning-to-rank, decision forests deliver remarkable
performance. They also possess other coveted characteristics such as
interpretability. Despite their widespread use and rich history, decision
forests to date fail to consume raw structured data such as text, or learn
effective representations for them, a factor behind the success of deep neural
networks in recent years. While there exist methods that construct smoothed
decision forests to achieve representation learning, the resulting models are
decision forests in name only: They are no longer axis-aligned, use stochastic
decisions, or are not interpretable. Furthermore, none of the existing methods
are appropriate for problems that require a Transfer Learning treatment. In
this work, we present a novel but intuitive proposal to achieve representation
learning for decision forests without imposing new restrictions or
necessitating structural changes. Our model is simply a decision forest,
possibly trained using any forest learning algorithm, atop a deep neural
network. By approximating the gradients of the decision forest through input
perturbation, a purely analytical procedure, the decision forest directs the
neural network to learn or fine-tune representations. Our framework has the
advantage that it is applicable to any arbitrary decision forest and that it
allows the use of arbitrary deep neural networks for representation learning.
We demonstrate the feasibility and effectiveness of our proposal through
experiments on synthetic and benchmark classification datasets."
['stat.ML'],Looking Deeper into Tabular LIME,"Interpretability of machine learning algorithms is an urgent need. Numerous
methods appeared in recent years, but do their explanations make sense? In this
paper, we present a thorough theoretical analysis of one of these methods,
LIME, in the case of tabular data. We prove that in the large sample limit, the
interpretable coefficients provided by Tabular LIME can be computed in an
explicit way as a function of the algorithm parameters and some expectation
computations related to the black-box model. When the function to explain has
some nice algebraic structure (linear, multiplicative, or sparsely depending on
a subset of the coordinates), our analysis provides interesting insights into
the explanations provided by LIME. These can be applied to a range of machine
learning models including Gaussian kernels or CART random forests. As an
example, for linear functions we show that LIME has the desirable property to
provide explanations that are proportional to the coefficients of the function
to explain and to ignore coordinates that are not used by the function to
explain. For partition-based regressors, on the other side, we show that LIME
produces undesired artifacts that may provide misleading explanations."
"['stat.ML', 'stat.TH']",Instance-dependent $\ell_\infty$-bounds for policy evaluation in tabular reinforcement learning,"Markov reward processes (MRPs) are used to model stochastic phenomena arising
in operations research, control engineering, robotics, and artificial
intelligence, as well as communication and transportation networks. In many of
these cases, such as in the policy evaluation problem encountered in
reinforcement learning, the goal is to estimate the long-term value function of
such a process without access to the underlying population transition and
reward functions. Working with samples generated under the synchronous model,
we study the problem of estimating the value function of an infinite-horizon,
discounted MRP on finitely many states in the $\ell_\infty$-norm. We analyze
both the standard plug-in approach to this problem and a more robust variant,
and establish non-asymptotic bounds that depend on the (unknown) problem
instance, as well as data-dependent bounds that can be evaluated based on the
observations of state-transitions and rewards. We show that these approaches
are minimax-optimal up to constant factors over natural sub-classes of MRPs.
Our analysis makes use of a leave-one-out decoupling argument tailored to the
policy evaluation problem, one which may be of independent interest."
['stat.ML'],Deep Constrained Q-learning,"In many real world applications, reinforcement learning agents have to
optimize multiple objectives while following certain rules or satisfying a list
of constraints. Classical methods based on reward shaping, i.e. a weighted
combination of different objectives in the reward signal, or Lagrangian
methods, including constraints in the loss function, have no guarantees that
the agent satisfies the constraints at all points in time and can lead to
undesired behavior. When a discrete policy is extracted from an action-value
function, safe actions can be ensured by restricting the action space at
maximization, but can lead to sub-optimal solutions among feasible
alternatives. In this work, we propose Constrained Q-learning, a novel
off-policy reinforcement learning framework restricting the action space
directly in the Q-update to learn the optimal Q-function for the induced
constrained MDP and the corresponding safe policy. In addition to single-step
constraints referring only to the next action, we introduce a formulation for
approximate multi-step constraints under the current target policy based on
truncated value-functions. We analyze the advantages of Constrained Q-learning
in the tabular case and compare Constrained DQN to reward shaping and
Lagrangian methods in the application of high-level decision making in
autonomous driving, considering constraints for safety, keeping right and
comfort. We train our agent in the open-source simulator SUMO and on the real
HighD data set."
['stat.ML'],A Markov Decision Process Approach to Active Meta Learning,"In supervised learning, we fit a single statistical model to a given data
set, assuming that the data is associated with a singular task, which yields
well-tuned models for specific use, but does not adapt well to new contexts. By
contrast, in meta-learning, the data is associated with numerous tasks, and we
seek a model that may perform well on all tasks simultaneously, in pursuit of
greater generalization. One challenge in meta-learning is how to exploit
relationships between tasks and classes, which is overlooked by commonly used
random or cyclic passes through data. In this work, we propose actively
selecting samples on which to train by discerning covariates inside and between
meta-training sets. Specifically, we cast the problem of selecting a sample
from a number of meta-training sets as either a multi-armed bandit or a Markov
Decision Process (MDP), depending on how one encapsulates correlation across
tasks. We develop scheduling schemes based on Upper Confidence Bound (UCB),
Gittins Index and tabular Markov Decision Problems (MDPs) solved with linear
programming, where the reward is the scaled statistical accuracy to ensure it
is a time-invariant function of state and action. Across a variety of
experimental contexts, we observe significant reductions in sample complexity
of active selection scheme relative to cyclic or i.i.d. sampling, demonstrating
the merit of exploiting covariates in practice."
['stat.ML'],Controlling Level of Unconsciousness by Titrating Propofol with Deep Reinforcement Learning,"Reinforcement Learning (RL) can be used to fit a mapping from patient state
to a medication regimen. Prior studies have used deterministic and value-based
tabular learning to learn a propofol dose from an observed anesthetic state.
Deep RL replaces the table with a deep neural network and has been used to
learn medication regimens from registry databases. Here we perform the first
application of deep RL to closed-loop control of anesthetic dosing in a
simulated environment. We use the cross-entropy method to train a deep neural
network to map an observed anesthetic state to a probability of infusing a
fixed propofol dosage. During testing, we implement a deterministic policy that
transforms the probability of infusion to a continuous infusion rate. The model
is trained and tested on simulated pharmacokinetic/pharmacodynamic models with
randomized parameters to ensure robustness to patient variability. The deep RL
agent significantly outperformed a proportional-integral-derivative controller
(median absolute performance error 1.7% +/- 0.6 and 3.4% +/- 1.2). Modeling
continuous input variables instead of a table affords more robust pattern
recognition and utilizes our prior domain knowledge. Deep RL learned a smooth
policy with a natural interpretation to data scientists and anesthesia care
providers alike."
['stat.ML'],Deep Active Learning by Model Interpretability,"Recent successes of Deep Neural Networks (DNNs) in a variety of research
tasks, however, heavily rely on the large amounts of labeled samples. This may
require considerable annotation cost in real-world applications. Fortunately,
active learning is a promising methodology to train high-performing model with
minimal annotation cost. In the deep learning context, the critical question of
active learning is how to precisely identify the informativeness of samples for
DNN. In this paper, inspired by piece-wise linear interpretability in DNN, we
introduce the linearly separable regions of samples to the problem of active
learning, and propose a novel Deep Active learning approach by Model
Interpretability (DAMI). To keep the maximal representativeness of the entire
unlabeled data, DAMI tries to select and label samples on different linearly
separable regions introduced by the piece-wise linear interpretability in DNN.
We focus on modeling Multi-Layer Perception (MLP) for modeling tabular data.
Specifically, we use the local piece-wise interpretation in MLP as the
representation of each sample, and directly run K-Center clustering to select
and label samples. To be noted, this whole process of DAMI does not require any
hyper-parameters to tune manually. To verify the effectiveness of our approach,
extensive experiments have been conducted on several tabular datasets. The
experimental results demonstrate that DAMI constantly outperforms several
state-of-the-art compared approaches."
['stat.ML'],On the Global Convergence Rates of Softmax Policy Gradient Methods,"We make three contributions toward better understanding policy gradient
methods in the tabular setting. First, we show that with the true gradient,
policy gradient with a softmax parametrization converges at a $O(1/t)$ rate,
with constants depending on the problem and initialization. This result
significantly expands the recent asymptotic convergence results. The analysis
relies on two findings: that the softmax policy gradient satisfies a
\L{}ojasiewicz inequality, and the minimum probability of an optimal action
during optimization can be bounded in terms of its initial value. Second, we
analyze entropy regularized policy gradient and show that it enjoys a
significantly faster linear convergence rate $O(e^{-t})$ toward softmax optimal
policy. This result resolves an open question in the recent literature.
Finally, combining the above two results and additional new $\Omega(1/t)$ lower
bound results, we explain how entropy regularization improves policy
optimization, even with the true gradient, from the perspective of convergence
rate. The separation of rates is further explained using the notion of
non-uniform \L{}ojasiewicz degree. These results provide a theoretical
understanding of the impact of entropy and corroborate existing empirical
studies."
['stat.ML'],Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets.
In the literature, missing data imputation techniques have been mostly
concerned with filling missing values. However, the existence of missing values
is synonymous with uncertainties not only over the distribution of missing
values but also over target class assignments that require careful
consideration. In this paper, we propose a simple and effective method for
imputing missing features and estimating the distribution of target assignments
given incomplete data. In order to make imputations, we train a simple and
effective generator network to generate imputations that a discriminator
network is tasked to distinguish. Following this, a predictor network is
trained using the imputed samples from the generator network to capture the
classification uncertainties and make predictions accordingly. The proposed
method is evaluated on CIFAR-10 and MNIST image datasets as well as five
real-world tabular classification datasets, under different missingness rates
and structures. Our experimental results show the effectiveness of the proposed
method in generating imputations as well as providing estimates for the class
uncertainties in a classification task when faced with missing values."
['stat.ML'],PermuteAttack: Counterfactual Explanation of Machine Learning Credit Scorecards,"This paper is a note on new directions and methodologies for validation and
explanation of Machine Learning (ML) models employed for retail credit scoring
in finance. Our proposed framework draws motivation from the field of
Artificial Intelligence (AI) security and adversarial ML where the need for
certifying the performance of the ML algorithms in the face of their
overwhelming complexity poses a need for rethinking the traditional notions of
model architecture selection, sensitivity analysis and stress testing. Our
point of view is that the phenomenon of adversarial perturbations when detached
from the AI security domain, has purely algorithmic roots and fall within the
scope of model risk assessment. We propose a model criticism and explanation
framework based on adversarially generated counterfactual examples for tabular
data. A counterfactual example to a given instance in this context is defined
as a synthetically generated data point sampled from the estimated data
distribution which is treated differently by a model. The counterfactual
examples can be used to provide a black-box instance-level explanation of the
model behaviour as well as studying the regions in the input space where the
model performance deteriorates. Adversarial example generating algorithms are
extensively studied in the image and natural language processing (NLP) domains.
However, most financial data come in tabular format and naive application of
the existing techniques on this class of datasets generates unrealistic
samples. In this paper, we propose a counterfactual example generation method
capable of handling tabular data including discrete and categorical variables.
Our proposed algorithm uses a gradient-free optimization based on genetic
algorithms and therefore is applicable to any classification model."
['stat.ML'],InstanceFlow: Visualizing the Evolution of Classifier Confusion on the Instance Level,"Classification is one of the most important supervised machine learning
tasks. During the training of a classification model, the training instances
are fed to the model multiple times (during multiple epochs) in order to
iteratively increase the classification performance. The increasing complexity
of models has led to a growing demand for model interpretability through
visualizations. Existing approaches mostly focus on the visual analysis of the
final model performance after training and are often limited to aggregate
performance measures. In this paper we introduce InstanceFlow, a novel
dual-view visualization tool that allows users to analyze the learning behavior
of classifiers over time on the instance-level. A Sankey diagram visualizes the
flow of instances throughout epochs, with on-demand detailed glyphs and traces
for individual instances. A tabular view allows users to locate interesting
instances by ranking and filtering. In this way, InstanceFlow bridges the gap
between class-level and instance-level performance evaluation while enabling
users to perform a full temporal analysis of the training process."
['stat.ML'],Towards Faithful and Meaningful Interpretable Representations,"Interpretable representations are the backbone of many black-box explainers.
They translate the low-level data representation necessary for good predictive
performance into high-level human-intelligible concepts used to convey the
explanation. Notably, the explanation type and its cognitive complexity are
directly controlled by the interpretable representation, allowing to target a
particular audience and use case. However, many explainers that rely on
interpretable representations overlook their merit and fall back on default
solutions, which may introduce implicit assumptions, thereby degrading the
explanatory power of such techniques. To address this problem, we study
properties of interpretable representations that encode presence and absence of
human-comprehensible concepts. We show how they are operationalised for
tabular, image and text data, discussing their strengths and weaknesses.
Finally, we analyse their explanatory properties in the context of tabular
data, where a linear model is used to quantify the importance of interpretable
concepts."
['stat.ML'],The reinforcement learning-based multi-agent cooperative approach for the adaptive speed regulation on a metallurgical pickling line,"We present a holistic data-driven approach to the problem of productivity
increase on the example of a metallurgical pickling line. The proposed approach
combines mathematical modeling as a base algorithm and a cooperative
Multi-Agent Reinforcement Learning (MARL) system implemented such as to enhance
the performance by multiple criteria while also meeting safety and reliability
requirements and taking into account the unexpected volatility of certain
technological processes. We demonstrate how Deep Q-Learning can be applied to a
real-life task in a heavy industry, resulting in significant improvement of
previously existing automation systems.The problem of input data scarcity is
solved by a two-step combination of LSTM and CGAN, which helps to embrace both
the tabular representation of the data and its sequential properties. Offline
RL training, a necessity in this setting, has become possible through the
sophisticated probabilistic kinematic environment."
['stat.ML'],DROCC: Deep Robust One-Class Classification,"Classical approaches for one-class problems such as one-class SVM and
isolation forest require careful feature engineering when applied to structured
domains like images. State-of-the-art methods aim to leverage deep learning to
learn appropriate features via two main approaches. The first approach based on
predicting transformations (Golan & El-Yaniv, 2018; Hendrycks et al., 2019a)
while successful in some domains, crucially depends on an appropriate
domain-specific set of transformations that are hard to obtain in general. The
second approach of minimizing a classical one-class loss on the learned final
layer representations, e.g., DeepSVDD (Ruff et al., 2018) suffers from the
fundamental drawback of representation collapse. In this work, we propose Deep
Robust One-Class Classification (DROCC) that is both applicable to most
standard domains without requiring any side-information and robust to
representation collapse. DROCC is based on the assumption that the points from
the class of interest lie on a well-sampled, locally linear low dimensional
manifold. Empirical evaluation demonstrates that DROCC is highly effective in
two different one-class problem settings and on a range of real-world datasets
across different domains: tabular data, images (CIFAR and ImageNet), audio, and
time-series, offering up to 20% increase in accuracy over the state-of-the-art
in anomaly detection. Code is available at https://github.com/microsoft/EdgeML."
['stat.ML'],Composite Q-learning: Multi-scale Q-function Decomposition and Separable Optimization,"In the past few years, off-policy reinforcement learning methods have shown
promising results in their application for robot control. Deep Q-learning,
however, still suffers from poor data-efficiency and is susceptible to
stochasticity in the environment or reward functions which is limiting with
regard to real-world applications. We alleviate these problems by proposing two
novel off-policy Temporal-Difference formulations: (1) Truncated Q-functions
which represent the return for the first n steps of a target-policy rollout
w.r.t. the full action-value and (2) Shifted Q-functions, acting as the
farsighted return after this truncated rollout. This decomposition allows us to
optimize both parts with their individual learning rates, achieving significant
learning speedup. We prove that the combination of these short- and long-term
predictions is a representation of the full return, leading to the Composite
Q-learning algorithm. We show the efficacy of Composite Q-learning in the
tabular case and compare Deep Composite Q-learning with TD3 and TD3(Delta),
which we introduce as an off-policy variant of TD(Delta). Moreover, we show
that Composite TD3 outperforms TD3 as well as state-of-the-art compositional
Q-learning approaches significantly in terms of data-efficiency in multiple
simulated robot tasks and that Composite Q-learning is robust to stochastic
environments and reward functions."
['stat.ML'],Synthesizing Property & Casualty Ratemaking Datasets using Generative Adversarial Networks,"Due to confidentiality issues, it can be difficult to access or share
interesting datasets for methodological development in actuarial science, or
other fields where personal data are important. We show how to design three
different types of generative adversarial networks (GANs) that can build a
synthetic insurance dataset from a confidential original dataset. The goal is
to obtain synthetic data that no longer contains sensitive information but
still has the same structure as the original dataset and retains the
multivariate relationships. In order to adequately model the specific
characteristics of insurance data, we use GAN architectures adapted for
multi-categorical data: a Wassertein GAN with gradient penalty (MC-WGAN-GP), a
conditional tabular GAN (CTGAN) and a Mixed Numerical and Categorical
Differentially Private GAN (MNCDP-GAN). For transparency, the approaches are
illustrated using a public dataset, the French motor third party liability
data. We compare the three different GANs on various aspects: ability to
reproduce the original data structure and predictive models, privacy, and ease
of use. We find that the MC-WGAN-GP synthesizes the best data, the CTGAN is the
easiest to use, and the MNCDP-GAN guarantees differential privacy."
['stat.ML'],PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning,"Direct policy gradient methods for reinforcement learning are a successful
approach for a variety of reasons: they are model free, they directly optimize
the performance metric of interest, and they allow for richly parameterized
policies. Their primary drawback is that, by being local in nature, they fail
to adequately explore the environment. In contrast, while model-based
approaches and Q-learning directly handle exploration through the use of
optimism, their ability to handle model misspecification and function
approximation is far less evident. This work introduces the the Policy
Cover-Policy Gradient (PC-PG) algorithm, which provably balances the
exploration vs. exploitation tradeoff using an ensemble of learned policies
(the policy cover). PC-PG enjoys polynomial sample complexity and run time for
both tabular MDPs and, more generally, linear MDPs in an infinite dimensional
RKHS. Furthermore, PC-PG also has strong guarantees under model
misspecification that go beyond the standard worst case $\ell_{\infty}$
assumptions; this includes approximation guarantees for state aggregation under
an average case error assumption, along with guarantees under a more general
assumption where the approximation error under distribution shift is
controlled. We complement the theory with empirical evaluation across a variety
of domains in both reward-free and reward-driven settings."
['stat.ML'],Null-sampling for Interpretable and Fair Representations,"We propose to learn invariant representations, in the data domain, to achieve
interpretability in algorithmic fairness. Invariance implies a selectivity for
high level, relevant correlations w.r.t. class label annotations, and a
robustness to irrelevant correlations with protected characteristics such as
race or gender. We introduce a non-trivial setup in which the training set
exhibits a strong bias such that class label annotations are irrelevant and
spurious correlations cannot be distinguished. To address this problem, we
introduce an adversarially trained model with a null-sampling procedure to
produce invariant representations in the data domain. To enable
disentanglement, a partially-labelled representative set is used. By placing
the representations into the data domain, the changes made by the model are
easily examinable by human auditors. We show the effectiveness of our method on
both image and tabular datasets: Coloured MNIST, the CelebA and the Adult
dataset."
['stat.ML'],A Generic and Model-Agnostic Exemplar Synthetization Framework for Explainable AI,"With the growing complexity of deep learning methods adopted in practical
applications, there is an increasing and stringent need to explain and
interpret the decisions of such methods. In this work, we focus on explainable
AI and propose a novel generic and model-agnostic framework for synthesizing
input exemplars that maximize a desired response from a machine learning model.
To this end, we use a generative model, which acts as a prior for generating
data, and traverse its latent space using a novel evolutionary strategy with
momentum updates. Our framework is generic because (i) it can employ any
underlying generator, e.g. Variational Auto-Encoders (VAEs) or Generative
Adversarial Networks (GANs), and (ii) it can be applied to any input data, e.g.
images, text samples or tabular data. Since we use a zero-order optimization
method, our framework is model-agnostic, in the sense that the machine learning
model that we aim to explain is a black-box. We stress out that our novel
framework does not require access or knowledge of the internal structure or the
training data of the black-box model. We conduct experiments with two
generative models, VAEs and GANs, and synthesize exemplars for various data
formats, image, text and tabular, demonstrating that our framework is generic.
We also employ our prototype synthetization framework on various black-box
models, for which we only know the input and the output formats, showing that
it is model-agnostic. Moreover, we compare our framework (available at
https://github.com/antoniobarbalau/exemplar) with a model-dependent approach
based on gradient descent, proving that our framework obtains equally-good
exemplars in a shorter computational time."
['stat.ML'],Momentum Q-learning with Finite-Sample Convergence Guarantee,"Existing studies indicate that momentum ideas in conventional optimization
can be used to improve the performance of Q-learning algorithms. However, the
finite-sample analysis for momentum-based Q-learning algorithms is only
available for the tabular case without function approximations. This paper
analyzes a class of momentum-based Q-learning algorithms with finite-sample
guarantee. Specifically, we propose the MomentumQ algorithm, which integrates
the Nesterov's and Polyak's momentum schemes, and generalizes the existing
momentum-based Q-learning algorithms. For the infinite state-action space case,
we establish the convergence guarantee for MomentumQ with linear function
approximations and Markovian sampling. In particular, we characterize the
finite-sample convergence rate which is provably faster than the vanilla
Q-learning. This is the first finite-sample analysis for momentum-based
Q-learning algorithms with function approximations. For the tabular case under
synchronous sampling, we also obtain a finite-sample convergence rate that is
slightly better than the SpeedyQ \citep{azar2011speedy} when choosing a special
family of step sizes. Finally, we demonstrate through various experiments that
the proposed MomentumQ outperforms other momentum-based Q-learning algorithms."
['stat.ML'],Towards Ground Truth Explainability on Tabular Data,"In data science, there is a long history of using synthetic data for method
development, feature selection and feature engineering. Our current interest in
synthetic data comes from recent work in explainability. Today's datasets are
typically larger and more complex - requiring less interpretable models. In the
setting of \textit{post hoc} explainability, there is no ground truth for
explanations. Inspired by recent work in explaining image classifiers that does
provide ground truth, we propose a similar solution for tabular data. Using
copulas, a concise specification of the desired statistical properties of a
dataset, users can build intuition around explainability using controlled data
sets and experimentation. The current capabilities are demonstrated on three
use cases: one dimensional logistic regression, impact of correlation from
informative features, impact of correlation from redundant variables."
['stat.ML'],Minority Class Oversampling for Tabular Data with Deep Generative Models,"In practice, machine learning experts are often confronted with imbalanced
data. Without accounting for the imbalance, common classifiers perform poorly
and standard evaluation metrics mislead the practitioners on the model's
performance. A common method to treat imbalanced datasets is under- and
oversampling. In this process, samples are either removed from the majority
class or synthetic samples are added to the minority class. In this paper, we
follow up on recent developments in deep learning. We take proposals of deep
generative models, including our own, and study the ability of these approaches
to provide realistic samples that improve performance on imbalanced
classification tasks via oversampling.
  Across 160K+ experiments, we show that all of the new methods tend to perform
better than simple baseline methods such as SMOTE, but require different under-
and oversampling ratios to do so. Our experiments show that the way the method
of sampling does not affect quality, but runtime varies widely. We also observe
that the improvements in terms of performance metric, while shown to be
significant when ranking the methods, often are minor in absolute terms,
especially compared to the required effort. Furthermore, we notice that a large
part of the improvement is due to undersampling, not oversampling. We make our
code and testing framework available."
['stat.ML'],Zap Q-Learning With Nonlinear Function Approximation,"Zap Q-learning is a recent class of reinforcement learning algorithms,
motivated primarily as a means to accelerate convergence. Stability theory has
been absent outside of two restrictive classes: the tabular setting, and
optimal stopping. This paper introduces a new framework for analysis of a more
general class of recursive algorithms known as stochastic approximation. Based
on this general theory, it is shown that Zap Q-learning is consistent under a
non-degeneracy assumption, even when the function approximation architecture is
nonlinear. Zap Q-learning with neural network function approximation emerges as
a special case, and is tested on examples from OpenAI Gym. Based on multiple
experiments with a range of neural network sizes, it is found that the new
algorithms converge quickly and are robust to choice of function approximation
architecture."
['stat.ML'],Near-Optimal Reinforcement Learning with Self-Play,"This paper considers the problem of designing optimal algorithms for
reinforcement learning in two-player zero-sum games. We focus on self-play
algorithms which learn the optimal policy by playing against itself without any
direct supervision. In a tabular episodic Markov game with $S$ states, $A$
max-player actions and $B$ min-player actions, the best existing algorithm for
finding an approximate Nash equilibrium requires $\tilde{\mathcal{O}}(S^2AB)$
steps of game playing, when only highlighting the dependency on $(S,A,B)$. In
contrast, the best existing lower bound scales as $\Omega(S(A+B))$ and has a
significant gap from the upper bound. This paper closes this gap for the first
time: we propose an optimistic variant of the \emph{Nash Q-learning} algorithm
with sample complexity $\tilde{\mathcal{O}}(SAB)$, and a new \emph{Nash
V-learning} algorithm with sample complexity $\tilde{\mathcal{O}}(S(A+B))$. The
latter result matches the information-theoretic lower bound in all
problem-dependent parameters except for a polynomial factor of the length of
each episode. In addition, we present a computational hardness result for
learning the best responses against a fixed opponent in Markov games---a
learning objective different from finding the Nash equilibrium."
['stat.ML'],Towards Robust Classification with Deep Generative Forests,"Decision Trees and Random Forests are among the most widely used machine
learning models, and often achieve state-of-the-art performance in tabular,
domain-agnostic datasets. Nonetheless, being primarily discriminative models
they lack principled methods to manipulate the uncertainty of predictions. In
this paper, we exploit Generative Forests (GeFs), a recent class of deep
probabilistic models that addresses these issues by extending Random Forests to
generative models representing the full joint distribution over the feature
space. We demonstrate that GeFs are uncertainty-aware classifiers, capable of
measuring the robustness of each prediction as well as detecting
out-of-distribution samples."
['stat.ML'],Provably-Efficient Double Q-Learning,"In this paper, we establish a theoretical comparison between the asymptotic
mean-squared error of Double Q-learning and Q-learning. Our result builds upon
an analysis for linear stochastic approximation based on Lyapunov equations and
applies to both tabular setting and with linear function approximation,
provided that the optimal policy is unique and the algorithms converge. We show
that the asymptotic mean-squared error of Double Q-learning is exactly equal to
that of Q-learning if Double Q-learning uses twice the learning rate of
Q-learning and outputs the average of its two estimators. We also present some
practical implications of this theoretical observation using simulations."
['stat.ML'],Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?,"Learning to plan for long horizons is a central challenge in episodic
reinforcement learning problems. A fundamental question is to understand how
the difficulty of the problem scales as the horizon increases. Here the natural
measure of sample complexity is a normalized one: we are interested in the
number of episodes it takes to provably discover a policy whose value is
$\varepsilon$ near to that of the optimal value, where the value is measured by
the normalized cumulative reward in each episode. In a COLT 2018 open problem,
Jiang and Agarwal conjectured that, for tabular, episodic reinforcement
learning problems, there exists a sample complexity lower bound which exhibits
a polynomial dependence on the horizon -- a conjecture which is consistent with
all known sample complexity upper bounds. This work refutes this conjecture,
proving that tabular, episodic reinforcement learning is possible with a sample
complexity that scales only logarithmically with the planning horizon. In other
words, when the values are appropriately normalized (to lie in the unit
interval), this results shows that long horizon RL is no more difficult than
short horizon RL, at least in a minimax sense. Our analysis introduces two
ideas: (i) the construction of an $\varepsilon$-net for optimal policies whose
log-covering number scales only logarithmically with the planning horizon, and
(ii) the Online Trajectory Synthesis algorithm, which adaptively evaluates all
policies in a given policy class using sample complexity that scales with the
log-covering number of the given policy class. Both may be of independent
interest."
['stat.ML'],Cognitive Radio Network Throughput Maximization with Deep Reinforcement Learning,"Radio Frequency powered Cognitive Radio Networks (RF-CRN) are likely to be
the eyes and ears of upcoming modern networks such as Internet of Things (IoT),
requiring increased decentralization and autonomous operation. To be considered
autonomous, the RF-powered network entities need to make decisions locally to
maximize the network throughput under the uncertainty of any network
environment. However, in complex and large-scale networks, the state and action
spaces are usually large, and existing Tabular Reinforcement Learning technique
is unable to find the optimal state-action policy quickly. In this paper, deep
reinforcement learning is proposed to overcome the mentioned shortcomings and
allow a wireless gateway to derive an optimal policy to maximize network
throughput. When benchmarked against advanced DQN techniques, our proposed DQN
configuration offers performance speedup of up to 1.8x with good overall
performance."
['stat.ML'],Discount Factor as a Regularizer in Reinforcement Learning,"Specifying a Reinforcement Learning (RL) task involves choosing a suitable
planning horizon, which is typically modeled by a discount factor. It is known
that applying RL algorithms with a lower discount factor can act as a
regularizer, improving performance in the limited data regime. Yet the exact
nature of this regularizer has not been investigated. In this work, we fill in
this gap. For several Temporal-Difference (TD) learning methods, we show an
explicit equivalence between using a reduced discount factor and adding an
explicit regularization term to the algorithm's loss. Motivated by the
equivalence, we empirically study this technique compared to standard $L_2$
regularization by extensive experiments in discrete and continuous domains,
using tabular and functional representations. Our experiments suggest the
regularization effectiveness is strongly related to properties of the available
data, such as size, distribution, and mixing rate."
['stat.ML'],A Unifying View of Optimism in Episodic Reinforcement Learning,"The principle of optimism in the face of uncertainty underpins many
theoretically successful reinforcement learning algorithms. In this paper we
provide a general framework for designing, analyzing and implementing such
algorithms in the episodic reinforcement learning problem. This framework is
built upon Lagrangian duality, and demonstrates that every model-optimistic
algorithm that constructs an optimistic MDP has an equivalent representation as
a value-optimistic dynamic programming algorithm. Typically, it was thought
that these two classes of algorithms were distinct, with model-optimistic
algorithms benefiting from a cleaner probabilistic analysis while
value-optimistic algorithms are easier to implement and thus more practical.
With the framework developed in this paper, we show that it is possible to get
the best of both worlds by providing a class of algorithms which have a
computationally efficient dynamic-programming implementation and also a simple
probabilistic analysis. Besides being able to capture many existing algorithms
in the tabular setting, our framework can also address largescale problems
under realizable function approximation, where it enables a simple model-based
analysis of some recently proposed methods."
['stat.ML'],Student-Teacher Curriculum Learning via Reinforcement Learning: Predicting Hospital Inpatient Admission Location,"Accurate and reliable prediction of hospital admission location is important
due to resource-constraints and space availability in a clinical setting,
particularly when dealing with patients who come from the emergency department.
In this work we propose a student-teacher network via reinforcement learning to
deal with this specific problem. A representation of the weights of the student
network is treated as the state and is fed as an input to the teacher network.
The teacher network's action is to select the most appropriate batch of data to
train the student network on from a training set sorted according to entropy.
By validating on three datasets, not only do we show that our approach
outperforms state-of-the-art methods on tabular data and performs competitively
on image recognition, but also that novel curricula are learned by the teacher
network. We demonstrate experimentally that the teacher network can actively
learn about the student network and guide it to achieve better performance than
if trained alone."
['stat.ML'],Model-based Asynchronous Hyperparameter and Neural Architecture Search,"We introduce a model-based asynchronous multi-fidelity method for
hyperparameter and neural architecture search that combines the strengths of
asynchronous Hyperband and Gaussian process-based Bayesian optimization. At the
heart of our method is a probabilistic model that can simultaneously reason
across hyperparameters and resource levels, and supports decision-making in the
presence of pending evaluations. We demonstrate the effectiveness of our method
on a wide range of challenging benchmarks, for tabular data, image
classification and language modelling, and report substantial speed-ups over
current state-of-the-art methods. Our new methods, along with asynchronous
baselines, are implemented in a distributed framework which will be open
sourced along with this publication."
['stat.ML'],"Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation","Automated machine learning (AutoML) can produce complex model ensembles by
stacking, bagging, and boosting many individual models like trees, deep
networks, and nearest neighbor estimators. While highly accurate, the resulting
predictors are large, slow, and opaque as compared to their constituents. To
improve the deployment of AutoML on tabular data, we propose FAST-DAD to
distill arbitrarily complex ensemble predictors into individual models like
boosted trees, random forests, and deep networks. At the heart of our approach
is a data augmentation strategy based on Gibbs sampling from a self-attention
pseudolikelihood estimator. Across 30 datasets spanning regression and
binary/multiclass classification tasks, FAST-DAD distillation produces
significantly better individual models than one obtains through standard
training on the original data. Our individual distilled models are over 10x
faster and more accurate than ensemble predictors produced by AutoML tools like
H2O/AutoSklearn."
['stat.ML'],Frequentist Regret Bounds for Randomized Least-Squares Value Iteration,"We consider the exploration-exploitation dilemma in finite-horizon
reinforcement learning (RL). When the state space is large or continuous,
traditional tabular approaches are unfeasible and some form of function
approximation is mandatory. In this paper, we introduce an
optimistically-initialized variant of the popular randomized least-squares
value iteration (RLSVI), a model-free algorithm where exploration is induced by
perturbing the least-squares approximation of the action-value function. Under
the assumption that the Markov decision process has low-rank transition
dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by
$\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $
is the horizon, and $ T $ is the total number of steps. To the best of our
knowledge, this is the first frequentist regret analysis for randomized
exploration with function approximation."
['stat.ML'],On Reward-Free Reinforcement Learning with Linear Function Approximation,"Reward-free reinforcement learning (RL) is a framework which is suitable for
both the batch RL setting and the setting where there are many reward functions
of interest. During the exploration phase, an agent collects samples without
using a pre-specified reward function. After the exploration phase, a reward
function is given, and the agent uses samples collected during the exploration
phase to compute a near-optimal policy. Jin et al. [2020] showed that in the
tabular setting, the agent only needs to collect polynomial number of samples
(in terms of the number states, the number of actions, and the planning
horizon) for reward-free RL. However, in practice, the number of states and
actions can be large, and thus function approximation schemes are required for
generalization. In this work, we give both positive and negative results for
reward-free RL with linear function approximation. We give an algorithm for
reward-free RL in the linear Markov decision process setting where both the
transition and the reward admit linear representations. The sample complexity
of our algorithm is polynomial in the feature dimension and the planning
horizon, and is completely independent of the number of states and actions. We
further give an exponential lower bound for reward-free RL in the setting where
only the optimal $Q$-function admits a linear representation. Our results imply
several interesting exponential separations on the sample complexity of
reward-free RL."
['stat.ML'],Optimistic Policy Optimization with Bandit Feedback,"Policy optimization methods are one of the most widely used classes of
Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been
mostly analyzed from an optimization perspective, without addressing the
problem of exploration, or by making strong assumptions on the interaction with
the environment. In this paper we consider model-based RL in the tabular
finite-horizon MDP setting with unknown transitions and bandit feedback. For
this setting, we propose an optimistic trust region policy optimization (TRPO)
algorithm for which we establish $\tilde O(\sqrt{S^2 A H^4 K})$ regret for
stochastic rewards. Furthermore, we prove $\tilde O( \sqrt{ S^2 A H^4 } K^{2/3}
) $ regret for adversarial rewards. Interestingly, this result matches previous
bounds derived for the bandit feedback case, yet with known transitions. To the
best of our knowledge, the two results are the first sub-linear regret bounds
obtained for policy optimization algorithms with unknown transitions and bandit
feedback."
['stat.ML'],No-regret Exploration in Contextual Reinforcement Learning,"We consider the recently proposed reinforcement learning (RL) framework of
Contextual Markov Decision Processes (CMDP), where the agent interacts with a
(potentially adversarial) sequence of episodic tabular MDPs. In addition, a
context vector determining the MDP parameters is available to the agent at the
start of each episode, thereby allowing it to learn a context-dependent
near-optimal policy. In this paper, we propose a no-regret online RL algorithm
in the setting where the MDP parameters are obtained from the context using
generalized linear mappings (GLMs). We propose and analyze optimistic and
randomized exploration methods which make (time and space) efficient online
updates. The GLM based model subsumes previous work in this area and also
improves previous known bounds in the special case where the contextual mapping
is linear. In addition, we demonstrate a generic template to derive confidence
sets using an online learning oracle and give a lower bound for the setting."
['stat.ML'],Anomaly Detection with Tensor Networks,"Originating from condensed matter physics, tensor networks are compact
representations of high-dimensional tensors. In this paper, the prowess of
tensor networks is demonstrated on the particular task of one-class anomaly
detection. We exploit the memory and computational efficiency of tensor
networks to learn a linear transformation over a space with dimension
exponential in the number of original features. The linearity of our model
enables us to ensure a tight fit around training instances by penalizing the
model's global tendency to a predict normality via its Frobenius norm---a task
that is infeasible for most deep learning models. Our method outperforms deep
and classical algorithms on tabular datasets and produces competitive results
on image datasets, despite not exploiting the locality of images."
['stat.ML'],Robust Variational Autoencoder for Tabular Data with Beta Divergence,"We propose a robust variational autoencoder with $\beta$ divergence for
tabular data (RTVAE) with mixed categorical and continuous features.
Variational autoencoders (VAE) and their variations are popular frameworks for
anomaly detection problems. The primary assumption is that we can learn
representations for normal patterns via VAEs and any deviation from that can
indicate anomalies. However, the training data itself can contain outliers. The
source of outliers in training data include the data collection process itself
(random noise) or a malicious attacker (data poisoning) who may target to
degrade the performance of the machine learning model. In either case, these
outliers can disproportionately affect the training process of VAEs and may
lead to wrong conclusions about what the normal behavior is. In this work, we
derive a novel form of a variational autoencoder for tabular data sets with
categorical and continuous features that is robust to outliers in training
data. Our results on the anomaly detection application for network traffic
datasets demonstrate the effectiveness of our approach."
['stat.ML'],Optimistic Distributionally Robust Policy Optimization,"Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization
(PPO), as the widely employed policy based reinforcement learning (RL) methods,
are prone to converge to a sub-optimal solution as they limit the policy
representation to a particular parametric distribution class. To address this
issue, we develop an innovative Optimistic Distributionally Robust Policy
Optimization (ODRPO) algorithm, which effectively utilizes Optimistic
Distributionally Robust Optimization (DRO) approach to solve the trust region
constrained optimization problem without parameterizing the policies. Our
algorithm improves TRPO and PPO with a higher sample efficiency and a better
performance of the final policy while attaining the learning stability.
Moreover, it achieves a globally optimal policy update that is not promised in
the prevailing policy based RL algorithms. Experiments across tabular domains
and robotic locomotion tasks demonstrate the effectiveness of our approach."
['stat.ML'],Actor-Critic Policy Optimization in Partially Observable Multiagent Environments,"Optimization of parameterized policies for reinforcement learning (RL) is an
important and challenging problem in artificial intelligence. Among the most
common approaches are algorithms based on gradient ascent of a score function
representing discounted return. In this paper, we examine the role of these
policy gradient and actor-critic algorithms in partially-observable multiagent
environments. We show several candidate policy update rules and relate them to
a foundation of regret minimization and multiagent learning techniques for the
one-shot and tabular cases, leading to previously unknown convergence
guarantees. We apply our method to model-free multiagent reinforcement learning
in adversarial sequential decision problems (zero-sum imperfect information
games), using RL-style function approximation. We evaluate on commonly used
benchmark Poker domains, showing performance against fixed policies and
empirical convergence to approximate Nash equilibria in self-play with rates
similar to or better than a baseline model-free algorithm for zero sum games,
without any domain-specific state space reductions."
['stat.ML'],DNF-Net: A Neural Architecture for Tabular Data,"A challenging open question in deep learning is how to handle tabular data.
Unlike domains such as image and natural language processing, where deep
architectures prevail, there is still no widely accepted neural architecture
that dominates tabular data. As a step toward bridging this gap, we present
DNF-Net a novel generic architecture whose inductive bias elicits models whose
structure corresponds to logical Boolean formulas in disjunctive normal form
(DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes
localized decisions that are taken over small subsets of the features. We
present an extensive empirical study showing that DNF-Nets significantly and
consistently outperform FCNs over tabular data. With relatively few
hyperparameters, DNF-Nets open the door to practical end-to-end handling of
tabular data using neural networks. We present ablation studies, which justify
the design choices of DNF-Net including the three inductive bias elements,
namely, Boolean formulation, locality, and feature selection."
['stat.ML'],Propositionalization and Embeddings: Two Sides of the Same Coin,"Data preprocessing is an important component of machine learning pipelines,
which requires ample time and resources. An integral part of preprocessing is
data transformation into the format required by a given learning algorithm.
This paper outlines some of the modern data processing techniques used in
relational learning that enable data fusion from different input data types and
formats into a single table data representation, focusing on the
propositionalization and embedding data transformation approaches. While both
approaches aim at transforming data into tabular data format, they use
different terminology and task definitions, are perceived to address different
goals, and are used in different contexts. This paper contributes a unifying
framework that allows for improved understanding of these two data
transformation techniques by presenting their unified definitions, and by
explaining the similarities and differences between the two approaches as
variants of a unified complex data transformation task. In addition to the
unifying framework, the novelty of this paper is a unifying methodology
combining propositionalization and embeddings, which benefits from the
advantages of both in solving complex data transformation and learning tasks.
We present two efficient implementations of the unifying methodology: an
instance-based PropDRM approach, and a feature-based PropStar approach to data
transformation and learning, together with their empirical evaluation on
several relational problems. The results show that the new algorithms can
outperform existing relational learners and can solve much larger problems."
['stat.ML'],Optimally Combining Classifiers for Semi-Supervised Learning,"This paper considers semi-supervised learning for tabular data. It is widely
known that Xgboost based on tree model works well on the heterogeneous features
while transductive support vector machine can exploit the low density
separation assumption. However, little work has been done to combine them
together for the end-to-end semi-supervised learning. In this paper, we find
these two methods have complementary properties and larger diversity, which
motivates us to propose a new semi-supervised learning method that is able to
adaptively combine the strengths of Xgboost and transductive support vector
machine. Instead of the majority vote rule, an optimization problem in terms of
ensemble weight is established, which helps to obtain more accurate pseudo
labels for unlabeled data. The experimental results on the UCI data sets and
real commercial data set demonstrate the superior classification performance of
our method over the five state-of-the-art algorithms improving test accuracy by
about $3\%-4\%$. The partial code can be found at
https://github.com/hav-cam-mit/CTO."
['stat.ML'],Network On Network for Tabular Data Classification in Real-world Applications,"Tabular data is the most common data format adopted by our customers ranging
from retail, finance to E-commerce, and tabular data classification plays an
essential role to their businesses. In this paper, we present Network On
Network (NON), a practical tabular data classification model based on deep
neural network to provide accurate predictions. Various deep methods have been
proposed and promising progress has been made. However, most of them use
operations like neural network and factorization machines to fuse the
embeddings of different features directly, and linearly combine the outputs of
those operations to get the final prediction. As a result, the intra-field
information and the non-linear interactions between those operations (e.g.
neural network and factorization machines) are ignored. Intra-field information
is the information that features inside each field belong to the same field.
NON is proposed to take full advantage of intra-field information and
non-linear interactions. It consists of three components: field-wise network at
the bottom to capture the intra-field information, across field network in the
middle to choose suitable operations data-drivenly, and operation fusion
network on the top to fuse outputs of the chosen operations deeply. Extensive
experiments on six real-world datasets demonstrate NON can outperform the
state-of-the-art models significantly. Furthermore, both qualitative and
quantitative study of the features in the embedding space show NON can capture
intra-field information effectively."
['stat.ML'],Uncertainty estimation for classification and risk prediction on medical tabular data,"In a data-scarce field such as healthcare, where models often deliver
predictions on patients with rare conditions, the ability to measure the
uncertainty of a model's prediction could potentially lead to improved
effectiveness of decision support tools and increased user trust. This work
advances the understanding of uncertainty estimation for classification and
risk prediction on medical tabular data, in a two-fold way. First, we expand
and refine the set of heuristics to select an uncertainty estimation technique,
introducing tests for clinically-relevant scenarios such as generalization to
uncommon pathologies, changes in clinical protocol and simulations of corrupted
data. We furthermore differentiate these heuristics depending on the clinical
use-case. Second, we observe that ensembles and related techniques perform
poorly when it comes to detecting out-of-domain examples, a critical task which
is carried out more successfully by auto-encoders. These remarks are enriched
by considerations of the interplay of uncertainty estimation with class
imbalance, post-modeling calibration and other modeling procedures. Our
findings are supported by an array of experiments on toy and real-world data."
['stat.ML'],Learning Model-Agnostic Counterfactual Explanations for Tabular Data,"Counterfactual explanations can be obtained by identifying the smallest
change made to a feature vector to qualitatively influence a prediction; for
example, from 'loan rejected' to 'awarded' or from 'high risk of cardiovascular
disease' to 'low risk'. Previous approaches often emphasized that
counterfactuals should be easily interpretable to humans, motivating sparse
solutions with few changes to the feature vectors. However, these approaches
would not ensure that the produced counterfactuals be proximate (i.e., not
local outliers) and connected to regions with substantial data density (i.e.,
close to correctly classified observations), two requirements known as
counterfactual faithfulness. These requirements are fundamental when making
suggestions to individuals that are indeed attainable. Our contribution is
twofold. On one hand, we suggest to complement the catalogue of counterfactual
quality measures [1] using a criterion to quantify the degree of difficulty for
a certain counterfactual suggestion. On the other hand, drawing ideas from the
manifold learning literature, we develop a framework that generates attainable
counterfactuals. We suggest the counterfactual conditional heterogeneous
variational autoencoder (C-CHVAE) to identify attainable counterfactuals that
lie within regions of high data density."
['stat.ML'],Composite Travel Generative Adversarial Networks for Tabular and Sequential Population Synthesis,"Agent-based transportation modelling has become the standard to simulate
travel behaviour, mobility choices and activity preferences using disaggregate
travel demand data for entire populations, data that are not typically readily
available. Various methods have been proposed to synthesize population data for
this purpose. We present a Composite Travel Generative Adversarial Network
(CTGAN), a novel deep generative model to estimate the underlying joint
distribution of a population, that is capable of reconstructing composite
synthetic agents having tabular (e.g. age and sex) as well as sequential
mobility data (e.g. trip trajectory and sequence). The CTGAN model is compared
with other recently proposed methods such as the Variational Autoencoders (VAE)
method, which has shown success in high dimensional tabular population
synthesis. We evaluate the performance of the synthesized outputs based on
distribution similarity, multi-variate correlations and spatio-temporal
metrics. The results show the consistent and accurate generation of synthetic
populations and their tabular and spatially sequential attributes, generated
over varying spatial scales and dimensions."
['stat.ML'],Contrastive Examples for Addressing the Tyranny of the Majority,"Computer vision algorithms, e.g. for face recognition, favour groups of
individuals that are better represented in the training data. This happens
because of the generalization that classifiers have to make. It is simpler to
fit the majority groups as this fit is more important to overall error. We
propose to create a balanced training dataset, consisting of the original
dataset plus new data points in which the group memberships are intervened,
minorities become majorities and vice versa. We show that current generative
adversarial networks are a powerful tool for learning these data points, called
contrastive examples. We experiment with the equalized odds bias measure on
tabular data as well as image data (CelebA and Diversity in Faces datasets).
Contrastive examples allow us to expose correlations between group membership
and other seemingly neutral features. Whenever a causal graph is available, we
can put those contrastive examples in the perspective of counterfactuals."
['stat.ML'],NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search,"One-shot neural architecture search (NAS) has played a crucial role in making
NAS methods computationally feasible in practice. Nevertheless, there is still
a lack of understanding on how these weight-sharing algorithms exactly work due
to the many factors controlling the dynamics of the process. In order to allow
a scientific study of these components, we introduce a general framework for
one-shot NAS that can be instantiated to many recently-introduced variants and
introduce a general benchmarking framework that draws on the recent large-scale
tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS
methods. To showcase the framework, we compare several state-of-the-art
one-shot NAS methods, examine how sensitive they are to their hyperparameters
and how they can be improved by tuning their hyperparameters, and compare their
performance to that of blackbox optimizers for NAS-Bench-101."
['stat.ML'],Human-like Time Series Summaries via Trend Utility Estimation,"In many scenarios, humans prefer a text-based representation of quantitative
data over numerical, tabular, or graphical representations. The attractiveness
of textual summaries for complex data has inspired research on data-to-text
systems. While there are several data-to-text tools for time series, few of
them try to mimic how humans summarize for time series. In this paper, we
propose a model to create human-like text descriptions for time series. Our
system finds patterns in time series data and ranks these patterns based on
empirical observations of human behavior using utility estimation. Our proposed
utility estimation model is a Bayesian network capturing interdependencies
between different patterns. We describe the learning steps for this network and
introduce baselines along with their performance for each step. The output of
our system is a natural language description of time series that attempts to
match a human's summary of the same data."
['stat.ML'],Deep Reinforcement Learning with Weighted Q-Learning,"Overestimation of the maximum action-value is a well-known problem that
hinders Q-Learning performance, leading to suboptimal policies and unstable
learning. Among several Q-Learning variants proposed to address this issue,
Weighted Q-Learning (WQL) effectively reduces the bias and shows remarkable
results in stochastic environments. WQL uses a weighted sum of the estimated
action-values, where the weights correspond to the probability of each
action-value being the maximum; however, the computation of these probabilities
is only practical in the tabular settings. In this work, we provide the
methodological advances to benefit from the WQL properties in Deep
Reinforcement Learning (DRL), by using neural networks with Dropout Variational
Inference as an effective approximation of deep Gaussian processes. In
particular, we adopt the Concrete Dropout variant to obtain calibrated
estimates of epistemic uncertainty in DRL. We show that model uncertainty in
DRL can be useful not only for action selection, but also action evaluation. We
analyze how the novel Weighted Deep Q-Learning algorithm reduces the bias
w.r.t. relevant baselines and provide empirical evidence of its advantages on
several representative benchmarks."
['stat.ML'],Teacher-Student Compression with Generative Adversarial Networks,"More accurate machine learning models often demand more computation and
memory at test time, making them difficult to deploy on CPU- or
memory-constrained devices. Teacher-student compression (TSC), also known as
distillation, alleviates this burden by training a less expensive student model
to mimic the expensive teacher model while maintaining most of the original
accuracy. However, when fresh data is unavailable for the compression task, the
teacher's training data is typically reused, leading to suboptimal compression.
In this work, we propose to augment the compression dataset with synthetic data
from a generative adversarial network (GAN) designed to approximate the
training data distribution. Our GAN-assisted TSC (GAN-TSC) significantly
improves student accuracy for expensive models such as large random forests and
deep neural networks on both tabular and image datasets. Building on these
results, we propose a comprehensive metric---the TSC Score---to evaluate the
quality of synthetic datasets based on their induced TSC performance. The TSC
Score captures both data diversity and class affinity, and we illustrate its
benefits over the popular Inception Score in the context of image
classification."
['stat.ML'],Parameterized Indexed Value Function for Efficient Exploration in Reinforcement Learning,"It is well known that quantifying uncertainty in the action-value estimates
is crucial for efficient exploration in reinforcement learning. Ensemble
sampling offers a relatively computationally tractable way of doing this using
randomized value functions. However, it still requires a huge amount of
computational resources for complex problems. In this paper, we present an
alternative, computationally efficient way to induce exploration using index
sampling. We use an indexed value function to represent uncertainty in our
action-value estimates. We first present an algorithm to learn parameterized
indexed value function through a distributional version of temporal difference
in a tabular setting and prove its regret bound. Then, in a computational point
of view, we propose a dual-network architecture, Parameterized Indexed Networks
(PINs), comprising one mean network and one uncertainty network to learn the
indexed value function. Finally, we show the efficacy of PINs through
computational experiments."
['stat.ML'],AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data,"We introduce AutoGluon-Tabular, an open-source AutoML framework that requires
only a single line of Python to train highly accurate machine learning models
on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML
frameworks that primarily focus on model/hyperparameter selection,
AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in
multiple layers. Experiments reveal that our multi-layer combination of many
models offers better use of allocated training time than seeking out the best.
A second contribution is an extensive evaluation of public and commercial
AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and
Google AutoML Tables. Tests on a suite of 50 classification and regression
tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is
faster, more robust, and much more accurate. We find that AutoGluon often even
outperforms the best-in-hindsight combination of all of its competitors. In two
popular Kaggle competitions, AutoGluon beat 99% of the participating data
scientists after merely 4h of training on the raw data."
['stat.ML'],Zooming for Efficient Model-Free Reinforcement Learning in Metric Spaces,"Despite the wealth of research into provably efficient reinforcement learning
algorithms, most works focus on tabular representation and thus struggle to
handle exponentially or infinitely large state-action spaces. In this paper, we
consider episodic reinforcement learning with a continuous state-action space
which is assumed to be equipped with a natural metric that characterizes the
proximity between different states and actions. We propose ZoomRL, an online
algorithm that leverages ideas from continuous bandits to learn an adaptive
discretization of the joint space by zooming in more promising and frequently
visited regions while carefully balancing the exploitation-exploration
trade-off. We show that ZoomRL achieves a worst-case regret
$\tilde{O}(H^{\frac{5}{2}} K^{\frac{d+1}{d+2}})$ where $H$ is the planning
horizon, $K$ is the number of episodes and $d$ is the covering dimension of the
space with respect to the metric. Moreover, our algorithm enjoys improved
metric-dependent guarantees that reflect the geometry of the underlying space.
Finally, we show that our algorithm is robust to small misspecification errors."
['stat.ML'],BasisVAE: Translation-invariant feature-level clustering with Variational Autoencoders,"Variational Autoencoders (VAEs) provide a flexible and scalable framework for
non-linear dimensionality reduction. However, in application domains such as
genomics where data sets are typically tabular and high-dimensional, a
black-box approach to dimensionality reduction does not provide sufficient
insights. Common data analysis workflows additionally use clustering techniques
to identify groups of similar features. This usually leads to a two-stage
process, however, it would be desirable to construct a joint modelling
framework for simultaneous dimensionality reduction and clustering of features.
In this paper, we propose to achieve this through the BasisVAE: a combination
of the VAE and a probabilistic clustering prior, which lets us learn a one-hot
basis function representation as part of the decoder network. Furthermore, for
scenarios where not all features are aligned, we develop an extension to handle
translation-invariant basis functions. We show how a collapsed variational
inference scheme leads to scalable and efficient inference for BasisVAE,
demonstrated on various toy examples as well as on single-cell gene expression
data."
['stat.ML'],Gaussianization Flows,"Iterative Gaussianization is a fixed-point iteration procedure that can
transform any continuous random vector into a Gaussian one. Based on iterative
Gaussianization, we propose a new type of normalizing flow model that enables
both efficient computation of likelihoods and efficient inversion for sample
generation. We demonstrate that these models, named Gaussianization flows, are
universal approximators for continuous probability distributions under some
regularity conditions. Because of this guaranteed expressivity, they can
capture multimodal target distributions without compromising the efficiency of
sample generation. Experimentally, we show that Gaussianization flows achieve
better or comparable performance on several tabular datasets compared to other
efficiently invertible flow models such as Real NVP, Glow and FFJORD. In
particular, Gaussianization flows are easier to initialize, demonstrate better
robustness with respect to different transformations of the training data, and
generalize better on small training sets."
['stat.ML'],Robust Variational Autoencoders for Outlier Detection and Repair of Mixed-Type Data,"We focus on the problem of unsupervised cell outlier detection and repair in
mixed-type tabular data. Traditional methods are concerned only with detecting
which rows in the dataset are outliers. However, identifying which cells are
corrupted in a specific row is an important problem in practice, and the very
first step towards repairing them. We introduce the Robust Variational
Autoencoder (RVAE), a deep generative model that learns the joint distribution
of the clean data while identifying the outlier cells, allowing their
imputation (repair). RVAE explicitly learns the probability of each cell being
an outlier, balancing different likelihood models in the row outlier score,
making the method suitable for outlier detection in mixed-type datasets. We
show experimentally that not only RVAE performs better than several
state-of-the-art methods in cell outlier detection and repair for tabular data,
but also that is robust against the initial hyper-parameter selection."
['stat.ML'],AP-Perf: Incorporating Generic Performance Metrics in Differentiable Learning,"We propose a method that enables practitioners to conveniently incorporate
custom non-decomposable performance metrics into differentiable learning
pipelines, notably those based upon neural network architectures. Our approach
is based on the recently developed adversarial prediction framework, a
distributionally robust approach that optimizes a metric in the worst case
given the statistical summary of the empirical distribution. We formulate a
marginal distribution technique to reduce the complexity of optimizing the
adversarial prediction formulation over a vast range of non-decomposable
metrics. We demonstrate how easy it is to write and incorporate complex custom
metrics using our provided tool. Finally, we show the effectiveness of our
approach various classification tasks on tabular datasets from the UCI
repository and benchmark datasets, as well as image classification tasks. The
code for our proposed method is available at
https://github.com/rizalzaf/AdversarialPrediction.jl."
['stat.ML'],Deep differentiable forest with sparse attention for the tabular data,"We present a general architecture of deep differentiable forest and its
sparse attention mechanism. The differentiable forest has the advantages of
both trees and neural networks. Its structure is a simple binary tree, easy to
use and understand. It has full differentiability and all variables are
learnable parameters. We would train it by the gradient-based optimization
method, which shows great power in the training of deep CNN. We find and
analyze the attention mechanism in the differentiable forest. That is, each
decision depends on only a few important features, and others are irrelevant.
The attention is always sparse. Based on this observation, we improve its
sparsity by data-aware initialization. We use the attribute importance to
initialize the attention weight. Then the learned weight is much sparse than
that from random initialization. Our experiment on some large tabular dataset
shows differentiable forest has higher accuracy than GBDT, which is the state
of art algorithm for tabular datasets. The source codes are available at
https://github.com/closest-git/QuantumForest"
['stat.ML'],ConQUR: Mitigating Delusional Bias in Deep Q-learning,"Delusional bias is a fundamental source of error in approximate Q-learning.
To date, the only techniques that explicitly address delusion require
comprehensive search using tabular value estimates. In this paper, we develop
efficient methods to mitigate delusional bias by training Q-approximators with
labels that are ""consistent"" with the underlying greedy policy class. We
introduce a simple penalization scheme that encourages Q-labels used across
training batches to remain (jointly) consistent with the expressible policy
class. We also propose a search framework that allows multiple Q-approximators
to be generated and tracked, thus mitigating the effect of premature (implicit)
policy commitments. Experimental results demonstrate that these methods can
improve the performance of Q-learning in a variety of Atari games, sometimes
dramatically."
['stat.ML'],Optimistic Exploration even with a Pessimistic Initialisation,"Optimistic initialisation is an effective strategy for efficient exploration
in reinforcement learning (RL). In the tabular case, all provably efficient
model-free algorithms rely on it. However, model-free deep RL algorithms do not
use optimistic initialisation despite taking inspiration from these provably
efficient tabular algorithms. In particular, in scenarios with only positive
rewards, Q-values are initialised at their lowest possible values due to
commonly used network initialisation schemes, a pessimistic initialisation.
Merely initialising the network to output optimistic Q-values is not enough,
since we cannot ensure that they remain optimistic for novel state-action
pairs, which is crucial for exploration. We propose a simple count-based
augmentation to pessimistically initialised Q-values that separates the source
of optimism from the neural network. We show that this scheme is provably
efficient in the tabular setting and extend it to the deep RL setting. Our
algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), augments
the Q-value estimates of a DQN-based agent with count-derived bonuses to ensure
optimism during both action selection and bootstrapping. We show that OPIQ
outperforms non-optimistic DQN variants that utilise a pseudocount-based
intrinsic motivation in hard exploration tasks, and that it predicts optimistic
estimates for novel state-action pairs."
['stat.ML'],Neural Replicator Dynamics,"Policy gradient and actor-critic algorithms form the basis of many commonly
used training techniques in deep reinforcement learning. Using these algorithms
in multiagent environments poses problems such as nonstationarity and
instability. In this paper, we first demonstrate that standard softmax-based
policy gradient can be prone to poor performance in the presence of even the
most benign nonstationarity. By contrast, it is known that the replicator
dynamics, a well-studied model from evolutionary game theory, eliminates
dominated strategies and exhibits convergence of the time-averaged trajectories
to interior Nash equilibria in zero-sum games. Thus, using the replicator
dynamics as a foundation, we derive an elegant one-line change to policy
gradient methods that simply bypasses the gradient step through the softmax,
yielding a new algorithm titled Neural Replicator Dynamics (NeuRD). NeuRD
reduces to the exponential weights/Hedge algorithm in the single-state
all-actions case. Additionally, NeuRD has formal equivalence to softmax
counterfactual regret minimization, which guarantees convergence in the
sequential tabular case. Importantly, our algorithm provides a straightforward
way of extending the replicator dynamics to the function approximation setting.
Empirical results show that NeuRD quickly adapts to nonstationarities,
outperforming policy gradient significantly in both tabular and function
approximation settings, when evaluated on the standard imperfect information
benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel."
['stat.ML'],Periodic Q-Learning,"The use of target networks is a common practice in deep reinforcement
learning for stabilizing the training; however, theoretical understanding of
this technique is still limited. In this paper, we study the so-called periodic
Q-learning algorithm (PQ-learning for short), which resembles the technique
used in deep Q-learning for solving infinite-horizon discounted Markov decision
processes (DMDP) in the tabular setting. PQ-learning maintains two separate
Q-value estimates - the online estimate and target estimate. The online
estimate follows the standard Q-learning update, while the target estimate is
updated periodically. In contrast to the standard Q-learning, PQ-learning
enjoys a simple finite time analysis and achieves better sample complexity for
finding an epsilon-optimal policy. Our result provides a preliminary
justification of the effectiveness of utilizing target estimates or networks in
Q-learning algorithms."
['stat.ML'],Prioritized Sequence Experience Replay,"Experience replay is widely used in deep reinforcement learning algorithms
and allows agents to remember and learn from experiences from the past. In an
effort to learn more efficiently, researchers proposed prioritized experience
replay (PER) which samples important transitions more frequently. In this
paper, we propose Prioritized Sequence Experience Replay (PSER) a framework for
prioritizing sequences of experience in an attempt to both learn more
efficiently and to obtain better performance. We compare the performance of PER
and PSER sampling techniques in a tabular Q-learning environment and in DQN on
the Atari 2600 benchmark. We prove theoretically that PSER is guaranteed to
converge faster than PER and empirically show PSER substantially improves upon
PER."
['stat.ML'],Interpretable Counterfactual Explanations Guided by Prototypes,"We propose a fast, model agnostic method for finding interpretable
counterfactual explanations of classifier predictions by using class
prototypes. We show that class prototypes, obtained using either an encoder or
through class specific k-d trees, significantly speed up the the search for
counterfactual instances and result in more interpretable explanations. We
introduce two novel metrics to quantitatively evaluate local interpretability
at the instance level. We use these metrics to illustrate the effectiveness of
our method on an image and tabular dataset, respectively MNIST and Breast
Cancer Wisconsin (Diagnostic). The method also eliminates the computational
bottleneck that arises because of numerical gradient evaluation for
$\textit{black box}$ models."
['stat.ML'],Feature Importance Estimation with Self-Attention Networks,"Black-box neural network models are widely used in industry and science, yet
are hard to understand and interpret. Recently, the attention mechanism was
introduced, offering insights into the inner workings of neural language
models. This paper explores the use of attention-based neural networks
mechanism for estimating feature importance, as means for explaining the models
learned from propositional (tabular) data. Feature importance estimates,
assessed by the proposed Self-Attention Network (SAN) architecture, are
compared with the established ReliefF, Mutual Information and Random
Forest-based estimates, which are widely used in practice for model
interpretation. For the first time we conduct scale-free comparisons of feature
importance estimates across algorithms on ten real and synthetic data sets to
study the similarities and differences of the resulting feature importance
estimates, showing that SANs identify similar high-ranked features as the other
methods. We demonstrate that SANs identify feature interactions which in some
cases yield better predictive performance than the baselines, suggesting that
attention extends beyond interactions of just a few key features and detects
larger feature subsets relevant for the considered learning task."
['stat.ML'],Lifting Interpretability-Performance Trade-off via Automated Feature Engineering,"Complex black-box predictive models may have high performance, but lack of
interpretability causes problems like lack of trust, lack of stability,
sensitivity to concept drift. On the other hand, achieving satisfactory
accuracy of interpretable models require more time-consuming work related to
feature engineering. Can we train interpretable and accurate models, without
timeless feature engineering? We propose a method that uses elastic black-boxes
as surrogate models to create a simpler, less opaque, yet still accurate and
interpretable glass-box models. New models are created on newly engineered
features extracted with the help of a surrogate model. We supply the analysis
by a large-scale benchmark on several tabular data sets from the OpenML
database. There are two results 1) extracting information from complex models
may improve the performance of linear models, 2) questioning a common myth that
complex machine learning models outperform linear models."
['stat.ML'],Stabilizing Deep Reinforcement Learning with Conservative Updates,"In recent years, advances in deep learning have enabled the application of
reinforcement learning algorithms in complex domains. However, they lack the
theoretical guarantees which are present in the tabular setting and suffer from
many stability and reproducibility problems \citep{henderson2018deep}. In this
work, we suggest a simple approach for improving stability and providing
probabilistic performance improvement in off-policy actor-critic deep
reinforcement learning regimes. Experiments on continuous action spaces, in the
MuJoCo control suite, show that our proposed method reduces the variance of the
process and improves the overall performance."
['stat.ML'],Learning State Abstractions for Transfer in Continuous Control,"Can simple algorithms with a good representation solve challenging
reinforcement learning problems? In this work, we answer this question in the
affirmative, where we take ""simple learning algorithm"" to be tabular
Q-Learning, the ""good representations"" to be a learned state abstraction, and
""challenging problems"" to be continuous control tasks. Our main contribution is
a learning algorithm that abstracts a continuous state-space into a discrete
one. We transfer this learned representation to unseen problems to enable
effective learning. We provide theory showing that learned abstractions
maintain a bounded value loss, and we report experiments showing that the
abstractions empower tabular Q-Learning to learn efficiently in unseen tasks."
['stat.ML'],On the Role of Weight Sharing During Deep Option Learning,"The options framework is a popular approach for building temporally extended
actions in reinforcement learning. In particular, the option-critic
architecture provides general purpose policy gradient theorems for learning
actions from scratch that are extended in time. However, past work makes the
key assumption that each of the components of option-critic has independent
parameters. In this work we note that while this key assumption of the policy
gradient theorems of option-critic holds in the tabular case, it is always
violated in practice for the deep function approximation setting. We thus
reconsider this assumption and consider more general extensions of
option-critic and hierarchical option-critic training that optimize for the
full architecture with each update. It turns out that not assuming parameter
independence challenges a belief in prior work that training the policy over
options can be disentangled from the dynamics of the underlying options. In
fact, learning can be sped up by focusing the policy over options on states
where options are actually likely to terminate. We put our new algorithms to
the test in application to sample efficient learning of Atari games, and
demonstrate significantly improved stability and faster convergence when
learning long options."
['stat.ML'],Scaling All-Goals Updates in Reinforcement Learning Using Convolutional Neural Networks,"Being able to reach any desired location in the environment can be a valuable
asset for an agent. Learning a policy to navigate between all pairs of states
individually is often not feasible. An all-goals updating algorithm uses each
transition to learn Q-values towards all goals simultaneously and off-policy.
However the expensive numerous updates in parallel limited the approach to
small tabular cases so far. To tackle this problem we propose to use
convolutional network architectures to generate Q-values and updates for a
large number of goals at once. We demonstrate the accuracy and generalization
qualities of the proposed method on randomly generated mazes and Sokoban
puzzles. In the case of on-screen goal coordinates the resulting mapping from
frames to distance-maps directly informs the agent about which places are
reachable and in how many steps. As an example of application we show that
replacing the random actions in epsilon-greedy exploration by several actions
towards feasible goals generates better exploratory trajectories on Montezuma's
Revenge and Super Mario All-Stars games."
['stat.ML'],Asymptotically Efficient Off-Policy Evaluation for Tabular Reinforcement Learning,"We consider the problem of off-policy evaluation for reinforcement learning,
where the goal is to estimate the expected reward of a target policy $\pi$
using offline data collected by running a logging policy $\mu$. Standard
importance-sampling based approaches for this problem suffer from a variance
that scales exponentially with time horizon $H$, which motivates a splurge of
recent interest in alternatives that break the ""Curse of Horizon"" (Liu et al.
2018, Xie et al. 2019). In particular, it was shown that a marginalized
importance sampling (MIS) approach can be used to achieve an estimation error
of order $O(H^3/ n)$ in mean square error (MSE) under an episodic Markov
Decision Process model with finite states and potentially infinite actions. The
MSE bound however is still a factor of $H$ away from a Cramer-Rao lower bound
of order $\Omega(H^2/n)$. In this paper, we prove that with a simple
modification to the MIS estimator, we can asymptotically attain the Cramer-Rao
lower bound, provided that the action space is finite. We also provide a
general method for constructing MIS estimators with high-probability error
bounds."
['stat.ML'],A Reduction from Reinforcement Learning to No-Regret Online Learning,"We present a reduction from reinforcement learning (RL) to no-regret online
learning based on the saddle-point formulation of RL, by which ""any"" online
algorithm with sublinear regret can generate policies with provable performance
guarantees. This new perspective decouples the RL problem into two parts:
regret minimization and function approximation. The first part admits a
standard online-learning analysis, and the second part can be quantified
independently of the learning algorithm. Therefore, the proposed reduction can
be used as a tool to systematically design new RL algorithms. We demonstrate
this idea by devising a simple RL algorithm based on mirror descent and the
generative-model oracle. For any $\gamma$-discounted tabular RL problem, with
probability at least $1-\delta$, it learns an $\epsilon$-optimal policy using
at most
$\tilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\log(\frac{1}{\delta})}{(1-\gamma)^4\epsilon^2}\right)$
samples. Furthermore, this algorithm admits a direct extension to linearly
parameterized function approximators for large-scale applications, with
computation and sample complexities independent of
$|\mathcal{S}|$,$|\mathcal{A}|$, though at the cost of potential approximation
bias."
['stat.ML'],Long-Term Visitation Value for Deep Exploration in Sparse Reward Reinforcement Learning,"Reinforcement learning with sparse rewards is still an open challenge.
Classic methods rely on getting feedback via extrinsic rewards to train the
agent, and in situations where this occurs very rarely the agent learns slowly
or cannot learn at all. Similarly, if the agent receives also rewards that
create suboptimal modes of the objective function, it will likely prematurely
stop exploring. More recent methods add auxiliary intrinsic rewards to
encourage exploration. However, auxiliary rewards lead to a non-stationary
target for the Q-function. In this paper, we present a novel approach that (1)
plans exploration actions far into the future by using a long-term visitation
count, and (2) decouples exploration and exploitation by learning a separate
function assessing the exploration value of the actions. Contrary to existing
methods which use models of reward and dynamics, our approach is off-policy and
model-free. We further propose new tabular environments for benchmarking
exploration in reinforcement learning. Empirical results on classic and novel
benchmarks show that the proposed approach outperforms existing methods in
environments with sparse rewards, especially in the presence of rewards that
create suboptimal modes of the objective function. Results also suggest that
our approach scales gracefully with the size of the environment. Source code is
available at https://github.com/sparisi/visit-value-explore"
['stat.ML'],Semi-Supervised Learning with Normalizing Flows,"Normalizing flows transform a latent distribution through an invertible
neural network for a flexible and pleasingly simple approach to generative
modelling, while preserving an exact likelihood. We propose FlowGMM, an
end-to-end approach to generative semi supervised learning with normalizing
flows, using a latent Gaussian mixture model. FlowGMM is distinct in its
simplicity, unified treatment of labelled and unlabelled data with an exact
likelihood, interpretability, and broad applicability beyond image data. We
show promising results on a wide range of applications, including AG-News and
Yahoo Answers text data, tabular data, and semi-supervised image
classification. We also show that FlowGMM can discover interpretable structure,
provide real-time optimization-free feature visualizations, and specify well
calibrated predictive distributions."
['stat.ML'],Imperceptible Adversarial Attacks on Tabular Data,"Security of machine learning models is a concern as they may face adversarial
attacks for unwarranted advantageous decisions. While research on the topic has
mainly been focusing on the image domain, numerous industrial applications, in
particular in finance, rely on standard tabular data. In this paper, we discuss
the notion of adversarial examples in the tabular domain. We propose a
formalization based on the imperceptibility of attacks in the tabular domain
leading to an approach to generate imperceptible adversarial examples.
Experiments show that we can generate imperceptible adversarial examples with a
high fooling rate."
['stat.ML'],Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,"Posterior sampling for reinforcement learning (PSRL) is an effective method
for balancing exploration and exploitation in reinforcement learning.
Randomised value functions (RVF) can be viewed as a promising approach to
scaling PSRL. However, we show that most contemporary algorithms combining RVF
with neural network function approximation do not possess the properties which
make PSRL effective, and provably fail in sparse reward problems. Moreover, we
find that propagation of uncertainty, a property of PSRL previously thought
important for exploration, does not preclude this failure. We use these
insights to design Successor Uncertainties (SU), a cheap and easy to implement
RVF algorithm that retains key properties of PSRL. SU is highly effective on
hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it
surpasses human performance on 38 of 49 games tested (achieving a median human
normalised score of 2.09), and outperforms its closest RVF competitor,
Bootstrapped DQN, on 36 of those."
['stat.ML'],Count-Based Exploration with the Successor Representation,"In this paper we introduce a simple approach for exploration in reinforcement
learning (RL) that allows us to develop theoretically justified algorithms in
the tabular case but that is also extendable to settings where function
approximation is required. Our approach is based on the successor
representation (SR), which was originally introduced as a representation
defining state generalization by the similarity of successor states. Here we
show that the norm of the SR, while it is being learned, can be used as a
reward bonus to incentivize exploration. In order to better understand this
transient behavior of the norm of the SR we introduce the substochastic
successor representation (SSR) and we show that it implicitly counts the number
of times each state (or feature) has been observed. We use this result to
introduce an algorithm that performs as well as some theoretically
sample-efficient approaches. Finally, we extend these ideas to a deep RL
algorithm and show that it achieves state-of-the-art performance in Atari 2600
games when in a low sample-complexity regime."
['stat.ML'],Information-Theoretic Confidence Bounds for Reinforcement Learning,"We integrate information-theoretic concepts into the design and analysis of
optimistic algorithms and Thompson sampling. By making a connection between
information-theoretic quantities and confidence bounds, we obtain results that
relate the per-period performance of the agent with its information gain about
the environment, thus explicitly characterizing the exploration-exploitation
tradeoff. The resulting cumulative regret bound depends on the agent's
uncertainty over the environment and quantifies the value of prior information.
We show applicability of this approach to several environments, including
linear bandits, tabular MDPs, and factored MDPs. These examples demonstrate the
potential of a general information-theoretic approach for the design and
analysis of reinforcement learning algorithms."
['stat.ML'],Scalable methods for computing state similarity in deterministic Markov Decision Processes,"We present new algorithms for computing and approximating bisimulation
metrics in Markov Decision Processes (MDPs). Bisimulation metrics are an
elegant formalism that capture behavioral equivalence between states and
provide strong theoretical guarantees on differences in optimal behaviour.
Unfortunately, their computation is expensive and requires a tabular
representation of the states, which has thus far rendered them impractical for
large problems. In this paper we present a new version of the metric that is
tied to a behavior policy in an MDP, along with an analysis of its theoretical
properties. We then present two new algorithms for approximating bisimulation
metrics in large, deterministic MDPs. The first does so via sampling and is
guaranteed to converge to the true metric. The second is a differentiable loss
which allows us to learn an approximation even for continuous state MDPs, which
prior to this work had not been possible."
['stat.ML'],Fair Adversarial Gradient Tree Boosting,"Fair classification has become an important topic in machine learning
research. While most bias mitigation strategies focus on neural networks, we
noticed a lack of work on fair classifiers based on decision trees even though
they have proven very efficient. In an up-to-date comparison of
state-of-the-art classification algorithms in tabular data, tree boosting
outperforms deep learning. For this reason, we have developed a novel approach
of adversarial gradient tree boosting. The objective of the algorithm is to
predict the output $Y$ with gradient tree boosting while minimizing the ability
of an adversarial neural network to predict the sensitive attribute $S$. The
approach incorporates at each iteration the gradient of the neural network
directly in the gradient tree boosting. We empirically assess our approach on 4
popular data sets and compare against state-of-the-art algorithms. The results
show that our algorithm achieves a higher accuracy while obtaining the same
level of fairness, as measured using a set of different common fairness
definitions."
['stat.ML'],Adversarial Robustness Toolbox v1.0.0,"Adversarial Robustness Toolbox (ART) is a Python library supporting
developers and researchers in defending Machine Learning models (Deep Neural
Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random
Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn
Pipelines, etc.) against adversarial threats and helps making AI systems more
secure and trustworthy. Machine Learning models are vulnerable to adversarial
examples, which are inputs (images, texts, tabular data, etc.) deliberately
modified to produce a desired response by the Machine Learning model. ART
provides the tools to build and deploy defences and test them with adversarial
attacks. Defending Machine Learning models involves certifying and verifying
model robustness and model hardening with approaches such as pre-processing
inputs, augmenting training data with adversarial samples, and leveraging
runtime detection methods to flag any inputs that might have been modified by
an adversary. The attacks implemented in ART allow creating adversarial attacks
against Machine Learning models which is required to test defenses with
state-of-the-art threat models. Supported Machine Learning Libraries include
TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM,
CatBoost, and GPy. The source code of ART is released with MIT license at
https://github.com/IBM/adversarial-robustness-toolbox. The release includes
code examples, notebooks with tutorials and documentation
(http://adversarial-robustness-toolbox.readthedocs.io)."
['stat.ML'],Graph Representation Learning via Multi-task Knowledge Distillation,"Machine learning on graph structured data has attracted much research
interest due to its ubiquity in real world data. However, how to efficiently
represent graph data in a general way is still an open problem. Traditional
methods use handcraft graph features in a tabular form but suffer from the
defects of domain expertise requirement and information loss. Graph
representation learning overcomes these defects by automatically learning the
continuous representations from graph structures, but they require abundant
training labels, which are often hard to fulfill for graph-level prediction
problems. In this work, we demonstrate that, if available, the domain expertise
used for designing handcraft graph features can improve the graph-level
representation learning when training labels are scarce. Specifically, we
proposed a multi-task knowledge distillation method. By incorporating
network-theory-based graph metrics as auxiliary tasks, we show on both
synthetic and real datasets that the proposed multi-task learning method can
improve the prediction performance of the original learning task, especially
when the training data size is small."
['stat.ML'],Preservation of Anomalous Subgroups On Machine Learning Transformed Data,"In this paper, we investigate the effect of machine learning based
anonymization on anomalous subgroup preservation. In particular, we train a
binary classifier to discover the most anomalous subgroup in a dataset by
maximizing the bias between the group's predicted odds ratio from the model and
observed odds ratio from the data. We then perform anonymization using a
variational autoencoder (VAE) to synthesize an entirely new dataset that would
ideally be drawn from the distribution of the original data. We repeat the
anomalous subgroup discovery task on the new data and compare it to what was
identified pre-anonymization. We evaluated our approach using publicly
available datasets from the financial industry. Our evaluation confirmed that
the approach was able to produce synthetic datasets that preserved a high level
of subgroup differentiation as identified initially in the original dataset.
Such a distinction was maintained while having distinctly different records
between the synthetic and original dataset. Finally, we packed the above end to
end process into what we call Utility Guaranteed Deep Privacy (UGDP) system.
UGDP can be easily extended to onboard alternative generative approaches such
as GANs to synthesize tabular data."
['stat.ML'],Advances in Machine Learning for the Behavioral Sciences,"The areas of machine learning and knowledge discovery in databases have
considerably matured in recent years. In this article, we briefly review recent
developments as well as classical algorithms that stood the test of time. Our
goal is to provide a general introduction into different tasks such as learning
from tabular data, behavioral data, or textual data, with a particular focus on
actual and potential applications in behavioral sciences. The supplemental
appendix to the article also provides practical guidance for using the methods
by pointing the reader to proven software implementations. The focus is on R,
but we also cover some libraries in other programming languages as well as
systems with easy-to-use graphical interfaces."
['stat.ML'],Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs,"In order to make good decision under uncertainty an agent must learn from
observations. To do so, two of the most common frameworks are Contextual
Bandits and Markov Decision Processes (MDPs). In this paper, we study whether
there exist algorithms for the more general framework (MDP) which automatically
provide the best performance bounds for the specific problem at hand without
user intervention and without modifying the algorithm. In particular, it is
found that a very minor variant of a recently proposed reinforcement learning
algorithm for MDPs already matches the best possible regret bound $\tilde O
(\sqrt{SAT})$ in the dominant term if deployed on a tabular Contextual Bandit
problem despite the agent being agnostic to such setting."
['stat.ML'],Policy Poisoning in Batch Reinforcement Learning and Control,"We study a security threat to batch reinforcement learning and control where
the attacker aims to poison the learned policy. The victim is a reinforcement
learner / controller which first estimates the dynamics and the rewards from a
batch data set, and then solves for the optimal policy with respect to the
estimates. The attacker can modify the data set slightly before learning
happens, and wants to force the learner into learning a target policy chosen by
the attacker. We present a unified framework for solving batch policy poisoning
attacks, and instantiate the attack on two standard victims: tabular certainty
equivalence learner in reinforcement learning and linear quadratic regulator in
control. We show that both instantiation result in a convex optimization
problem on which global optimality is guaranteed, and provide analysis on
attack feasibility and attack cost. Experiments show the effectiveness of
policy poisoning attacks."
['stat.ML'],Conditional out-of-sample generation for unpaired data using trVAE,"While generative models have shown great success in generating
high-dimensional samples conditional on low-dimensional descriptors (learning
e.g. stroke thickness in MNIST, hair color in CelebA, or speaker identity in
Wavenet), their generation out-of-sample poses fundamental problems. The
conditional variational autoencoder (CVAE) as a simple conditional generative
model does not explicitly relate conditions during training and, hence, has no
incentive of learning a compact joint distribution across conditions. We
overcome this limitation by matching their distributions using maximum mean
discrepancy (MMD) in the decoder layer that follows the bottleneck. This
introduces a strong regularization both for reconstructing samples within the
same condition and for transforming samples across conditions, resulting in
much improved generalization. We refer to the architecture as
\emph{transformer} VAE (trVAE). Benchmarking trVAE on high-dimensional image
and tabular data, we demonstrate higher robustness and higher accuracy than
existing approaches. In particular, we show qualitatively improved predictions
for cellular perturbation response to treatment and disease based on
high-dimensional single-cell gene expression data, by tackling previously
problematic minority classes and multiple conditions. For generic tasks, we
improve Pearson correlations of high-dimensional estimated means and variances
with their ground truths from 0.89 to 0.97 and 0.75 to 0.87, respectively."
['stat.ML'],bLIMEy: Surrogate Prediction Explanations Beyond LIME,"Surrogate explainers of black-box machine learning predictions are of
paramount importance in the field of eXplainable Artificial Intelligence since
they can be applied to any type of data (images, text and tabular), are
model-agnostic and are post-hoc (i.e., can be retrofitted). The Local
Interpretable Model-agnostic Explanations (LIME) algorithm is often mistakenly
unified with a more general framework of surrogate explainers, which may lead
to a belief that it is the solution to surrogate explainability. In this paper
we empower the community to ""build LIME yourself"" (bLIMEy) by proposing a
principled algorithmic framework for building custom local surrogate explainers
of black-box model predictions, including LIME itself. To this end, we
demonstrate how to decompose the surrogate explainers family into
algorithmically independent and interoperable modules and discuss the influence
of these component choices on the functional capabilities of the resulting
explainer, using the example of LIME."
"['stat.ML', 'stat.TH']",Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs,"This paper establishes that optimistic algorithms attain gap-dependent and
non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work,
our bounds do not suffer a dependence on diameter-like quantities or
ergodicity, and smoothly interpolate between the gap dependent
logarithmic-regret, and the $\widetilde{\mathcal{O}}(\sqrt{HSAT})$-minimax
rate. The key technique in our analysis is a novel ""clipped"" regret
decomposition which applies to a broad family of recent optimistic algorithms
for episodic MDPs."
['stat.ML'],Modeling Tabular data using Conditional GAN,"Modeling the probability distribution of rows in tabular data and generating
realistic synthetic data is a non-trivial task. Tabular data usually contains a
mix of discrete and continuous columns. Continuous columns may have multiple
modes whereas discrete columns are sometimes imbalanced making the modeling
difficult. Existing statistical and deep neural network models fail to properly
model this type of data. We design TGAN, which uses a conditional generative
adversarial network to address these challenges. To aid in a fair and thorough
comparison, we design a benchmark with 7 simulated and 8 real datasets and
several Bayesian network baselines. TGAN outperforms Bayesian methods on most
of the real datasets whereas other deep learning methods could not."
['stat.ML'],Contextual Local Explanation for Black Box Classifiers,"We introduce a new model-agnostic explanation technique which explains the
prediction of any classifier called CLE. CLE gives an faithful and
interpretable explanation to the prediction, by approximating the model locally
using an interpretable model. We demonstrate the flexibility of CLE by
explaining different models for text, tabular and image classification, and the
fidelity of it by doing simulated user experiments."
['stat.ML'],Deep Exploration via Randomized Value Functions,"We study the use of randomized value functions to guide deep exploration in
reinforcement learning. This offers an elegant means for synthesizing
statistically and computationally efficient exploration with common practical
approaches to value function learning. We present several reinforcement
learning algorithms that leverage randomized value functions and demonstrate
their efficacy through computational studies. We also prove a regret bound that
establishes statistical efficiency with a tabular representation."
['stat.ML'],Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data,"Nowadays, deep neural networks (DNNs) have become the main instrument for
machine learning tasks within a wide range of domains, including vision, NLP,
and speech. Meanwhile, in an important case of heterogenous tabular data, the
advantage of DNNs over shallow counterparts remains questionable. In
particular, there is no sufficient evidence that deep learning machinery allows
constructing methods that outperform gradient boosting decision trees (GBDT),
which are often the top choice for tabular problems. In this paper, we
introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning
architecture, designed to work with any tabular data. In a nutshell, the
proposed NODE architecture generalizes ensembles of oblivious decision trees,
but benefits from both end-to-end gradient-based optimization and the power of
multi-layer hierarchical representation learning. With an extensive
experimental comparison to the leading GBDT packages on a large number of
tabular datasets, we demonstrate the advantage of the proposed NODE
architecture, which outperforms the competitors on most of the tasks. We
open-source the PyTorch implementation of NODE and believe that it will become
a universal framework for machine learning on tabular data."
['stat.ML'],Mutual-Information Regularization in Markov Decision Processes and Actor-Critic Learning,"Cumulative entropy regularization introduces a regulatory signal to the
reinforcement learning (RL) problem that encourages policies with high-entropy
actions, which is equivalent to enforcing small deviations from a uniform
reference marginal policy. This has been shown to improve exploration and
robustness, and it tackles the value overestimation problem. It also leads to a
significant performance increase in tabular and high-dimensional settings, as
demonstrated via algorithms such as soft Q-learning (SQL) and soft actor-critic
(SAC). Cumulative entropy regularization has been extended to optimize over the
reference marginal policy instead of keeping it fixed, yielding a
regularization that minimizes the mutual information between states and
actions. While this has been initially proposed for Markov Decision Processes
(MDPs) in tabular settings, it was recently shown that a similar principle
leads to significant improvements over vanilla SQL in RL for high-dimensional
domains with discrete actions and function approximators.
  Here, we follow the motivation of mutual-information regularization from an
inference perspective and theoretically analyze the corresponding Bellman
operator. Inspired by this Bellman operator, we devise a novel
mutual-information regularized actor-critic learning (MIRACLE) algorithm for
continuous action spaces that optimizes over the reference marginal policy. We
empirically validate MIRACLE in the Mujoco robotics simulator, where we
demonstrate that it can compete with contemporary RL methods. Most notably, it
can improve over the model-free state-of-the-art SAC algorithm which implicitly
assumes a fixed reference policy."
['stat.ML'],LazyBum: Decision tree learning using lazy propositionalization,"Propositionalization is the process of summarizing relational data into a
tabular (attribute-value) format. The resulting table can next be used by any
propositional learner. This approach makes it possible to apply a wide variety
of learning methods to relational data. However, the transformation from
relational to propositional format is generally not lossless: different
relational structures may be mapped onto the same feature vector. At the same
time, features may be introduced that are not needed for the learning task at
hand. In general, it is hard to define a feature space that contains all and
only those features that are needed for the learning task. This paper presents
LazyBum, a system that can be considered a lazy version of the recently
proposed OneBM method for propositionalization. LazyBum interleaves OneBM's
feature construction method with a decision tree learner. This learner both
uses and guides the propositionalization process. It indicates when and where
to look for new features. This approach is similar to what has elsewhere been
called dynamic propositionalization. In an experimental comparison with the
original OneBM and with two other recently proposed propositionalization
methods (nFOIL and MODL, which respectively perform dynamic and static
propositionalization), LazyBum achieves a comparable accuracy with a lower
execution time on most of the datasets."
['stat.ML'],A Wide and Deep Neural Network for Survival Analysis from Anatomical Shape and Tabular Clinical Data,"We introduce a wide and deep neural network for prediction of progression
from patients with mild cognitive impairment to Alzheimer's disease.
Information from anatomical shape and tabular clinical data (demographics,
biomarkers) are fused in a single neural network. The network is invariant to
shape transformations and avoids the need to identify point correspondences
between shapes. To account for right censored time-to-event data, i.e., when it
is only known that a patient did not develop Alzheimer's disease up to a
particular time point, we employ a loss commonly used in survival analysis. Our
network is trained end-to-end to combine information from a patient's
hippocampus shape and clinical biomarkers. Our experiments on data from the
Alzheimer's Disease Neuroimaging Initiative demonstrate that our proposed model
is able to learn a shape descriptor that augments clinical biomarkers and
outperforms a deep neural network on shape alone and a linear model on common
clinical biomarkers."
['stat.ML'],"Gradient Q$(σ, λ)$: A Unified Algorithm with Function Approximation for Reinforcement Learning","Full-sampling (e.g., Q-learning) and pure-expectation (e.g., Expected Sarsa)
algorithms are efficient and frequently used techniques in reinforcement
learning. Q$(\sigma,\lambda)$ is the first approach unifies them with
eligibility trace through the sampling degree $\sigma$. However, it is limited
to the tabular case, for large-scale learning, the Q$(\sigma,\lambda)$ is too
expensive to require a huge volume of tables to accurately storage value
functions. To address above problem, we propose a GQ$(\sigma,\lambda)$ that
extends tabular Q$(\sigma,\lambda)$ with linear function approximation. We
prove the convergence of GQ$(\sigma,\lambda)$. Empirical results on some
standard domains show that GQ$(\sigma,\lambda)$ with a combination of
full-sampling with pure-expectation reach a better performance than
full-sampling and pure-expectation methods."
['stat.ML'],Expected Sarsa($λ$) with Control Variate for Variance Reduction,"Off-policy learning is powerful for reinforcement learning. However, the high
variance of off-policy evaluation is a critical challenge, which causes
off-policy learning falls into an uncontrolled instability. In this paper, for
reducing the variance, we introduce control variate technique to
$\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$) and propose a tabular
$\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ algorithm. We prove that if a proper
estimator of value function reaches, the proposed
$\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ enjoys a lower variance than
$\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$). Furthermore, to extend
$\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ to be a convergent algorithm with linear
function approximation, we propose the $\mathtt{GES}$($\lambda$) algorithm
under the convex-concave saddle-point formulation. We prove that the
convergence rate of $\mathtt{GES}$($\lambda$) achieves $\mathcal{O}(1/T)$,
which matches or outperforms lots of state-of-art gradient-based algorithms,
but we use a more relaxed condition. Numerical experiments show that the
proposed algorithm performs better with lower variance than several
state-of-art gradient-based TD learning algorithms: $\mathtt{GQ}$($\lambda$),
$\mathtt{GTB}$($\lambda$) and $\mathtt{ABQ}$($\zeta$)."
['stat.ML'],Worst-Case Regret Bounds for Exploration via Randomized Value Functions,"This paper studies a recent proposal to use randomized value functions to
drive exploration in reinforcement learning. These randomized value functions
are generated by injecting random noise into the training data, making the
approach compatible with many popular methods for estimating parameterized
value functions. By providing a worst-case regret bound for tabular
finite-horizon Markov decision processes, we show that planning with respect to
these randomized value functions can induce provably efficient exploration."
['stat.ML'],Graph Node Embeddings using Domain-Aware Biased Random Walks,"The recent proliferation of publicly available graph-structured data has
sparked an interest in machine learning algorithms for graph data. Since most
traditional machine learning algorithms assume data to be tabular, embedding
algorithms for mapping graph data to real-valued vector spaces has become an
active area of research. Existing graph embedding approaches are based purely
on structural information and ignore any semantic information from the
underlying domain. In this paper, we demonstrate that semantic information can
play a useful role in computing graph embeddings. Specifically, we present a
framework for devising embedding strategies aware of domain-specific
interpretations of graph nodes and edges, and use knowledge of downstream
machine learning tasks to identify relevant graph substructures. Using two
real-life domains, we show that our framework yields embeddings that are simple
to implement and yet achieve equal or greater accuracy in machine learning
tasks compared to domain independent approaches."
['stat.ML'],Multimodal Machine Learning for Automated ICD Coding,"This study presents a multimodal machine learning model to predict ICD-10
diagnostic codes. We developed separate machine learning models that can handle
data from different modalities, including unstructured text, semi-structured
text and structured tabular data. We further employed an ensemble method to
integrate all modality-specific models to generate ICD-10 codes. Key evidence
was also extracted to make our prediction more convincing and explainable. We
used the Medical Information Mart for Intensive Care III (MIMIC -III) dataset
to validate our approach. For ICD code prediction, our best-performing model
(micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other
baseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and
Text-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability,
our approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text
data and 0.3105 on tabular data, where well-trained physicians achieve 0.2780
and 0.5002 respectively."
['stat.ML'],Meta-learning of textual representations,"Recent progress in AutoML has lead to state-of-the-art methods (e.g.,
AutoSKLearn) that can be readily used by non-experts to approach any supervised
learning problem. Whereas these methods are quite effective, they are still
limited in the sense that they work for tabular (matrix formatted) data only.
This paper describes one step forward in trying to automate the design of
supervised learning methods in the context of text mining. We introduce a meta
learning methodology for automatically obtaining a representation for text
mining tasks starting from raw text. We report experiments considering 60
different textual representations and more than 80 text mining datasets
associated to a wide variety of tasks. Experimental results show the proposed
methodology is a promising solution to obtain highly effective off the shell
text classification pipelines."
['stat.ML'],AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications,"Feature crossing captures interactions among categorical features and is
useful to enhance learning from tabular data in real-world businesses. In this
paper, we present AutoCross, an automatic feature crossing tool provided by
4Paradigm to its customers, ranging from banks, hospitals, to Internet
corporations. By performing beam search in a tree-structured space, AutoCross
enables efficient generation of high-order cross features, which is not yet
visited by existing works. Additionally, we propose successive mini-batch
gradient descent and multi-granularity discretization to further improve
efficiency and effectiveness, while ensuring simplicity so that no machine
learning expertise or tedious hyper-parameter tuning is required. Furthermore,
the algorithms are designed to reduce the computational, transmitting, and
storage costs involved in distributed computing. Experimental results on both
benchmark and real-world business datasets demonstrate the effectiveness and
efficiency of AutoCross. It is shown that AutoCross can significantly enhance
the performance of both linear and deep models."
['stat.ML'],Composing Entropic Policies using Divergence Correction,"Composing previously mastered skills to solve novel tasks promises dramatic
improvements in the data efficiency of reinforcement learning. Here, we analyze
two recent works composing behaviors represented in the form of action-value
functions and show that they perform poorly in some situations. As part of this
analysis, we extend an important generalization of policy improvement to the
maximum entropy framework and introduce an algorithm for the practical
implementation of successor features in continuous action spaces. Then we
propose a novel approach which addresses the failure cases of prior work and,
in principle, recovers the optimal policy during transfer. This method works by
explicitly learning the (discounted, future) divergence between base policies.
We study this approach in the tabular case and on non-trivial continuous
control problems with compositional structure and show that it outperforms or
matches existing methods across all tasks considered."
['stat.ML'],Directed Exploration for Reinforcement Learning,"Efficient exploration is necessary to achieve good sample efficiency for
reinforcement learning in general. From small, tabular settings such as
gridworlds to large, continuous and sparse reward settings such as robotic
object manipulation tasks, exploration through adding an uncertainty bonus to
the reward function has been shown to be effective when the uncertainty is able
to accurately drive exploration towards promising states. However reward
bonuses can still be inefficient since they are non-stationary, which means
that we must wait for function approximators to catch up and converge again
when uncertainties change. We propose the idea of directed exploration, that is
learning a goal-conditioned policy where goals are simply other states, and
using that to directly try to reach states with large uncertainty. The
goal-conditioned policy is independent of uncertainty and is thus stationary.
We show in our experiments how directed exploration is more efficient at
exploration and more robust to how the uncertainty is computed than adding
bonuses to rewards."
['stat.ML'],Provably Efficient Imitation Learning from Observation Alone,"We study Imitation Learning (IL) from Observations alone (ILFO) in
large-scale MDPs. While most IL algorithms rely on an expert to directly
provide actions to the learner, in this setting the expert only supplies
sequences of observations. We design a new model-free algorithm for ILFO,
Forward Adversarial Imitation Learning (FAIL), which learns a sequence of
time-dependent policies by minimizing an Integral Probability Metric between
the observation distributions of the expert policy and the learner. FAIL is the
first provably efficient algorithm in ILFO setting, which learns a near-optimal
policy with a number of samples that is polynomial in all relevant parameters
but independent of the number of unique observations. The resulting theory
extends the domain of provably sample efficient learning algorithms beyond
existing results, which typically only consider tabular reinforcement learning
settings or settings that require access to a near-optimal reset distribution.
We also investigate the extension of FAIL in a model-based setting. Finally we
demonstrate the efficacy of FAIL on multiple OpenAI Gym control tasks."
['stat.ML'],Concept Tree: High-Level Representation of Variables for More Interpretable Surrogate Decision Trees,"Interpretable surrogates of black-box predictors trained on high-dimensional
tabular datasets can struggle to generate comprehensible explanations in the
presence of correlated variables. We propose a model-agnostic interpretable
surrogate that provides global and local explanations of black-box classifiers
to address this issue. We introduce the idea of concepts as intuitive groupings
of variables that are either defined by a domain expert or automatically
discovered using correlation coefficients. Concepts are embedded in a surrogate
decision tree to enhance its comprehensibility. First experiments on FRED-MD, a
macroeconomic database with 134 variables, show improvement in
human-interpretability while accuracy and fidelity of the surrogate model are
preserved."
['stat.ML'],Policy Certificates: Towards Accountable Reinforcement Learning,"The performance of a reinforcement learning algorithm can vary drastically
during learning because of exploration. Existing algorithms provide little
information about the quality of their current policy before executing it, and
thus have limited use in high-stakes applications like healthcare. We address
this lack of accountability by proposing that algorithms output policy
certificates. These certificates bound the sub-optimality and return of the
policy in the next episode, allowing humans to intervene when the certified
quality is not satisfactory. We further introduce two new algorithms with
certificates and present a new framework for theoretical analysis that
guarantees the quality of their policies and certificates. For tabular MDPs, we
show that computing certificates can even improve the sample-efficiency of
optimism-based exploration. As a result, one of our algorithms is the first to
achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm
also matches (and in some settings slightly improves upon) existing minimax
regret bounds."
['stat.ML'],Recurrent Value Functions,"Despite recent successes in Reinforcement Learning, value-based methods often
suffer from high variance hindering performance. In this paper, we illustrate
this in a continuous control setting where state of the art methods perform
poorly whenever sensor noise is introduced. To overcome this issue, we
introduce Recurrent Value Functions (RVFs) as an alternative to estimate the
value function of a state. We propose to estimate the value function of the
current state using the value function of past states visited along the
trajectory. Due to the nature of their formulation, RVFs have a natural way of
learning an emphasis function that selectively emphasizes important states.
First, we establish RVF's asymptotic convergence properties in tabular
settings. We then demonstrate their robustness on a partially observable domain
and continuous control tasks. Finally, we provide a qualitative interpretation
of the learned emphasis function."
['stat.ML'],Dynamic Weights in Multi-Objective Deep Reinforcement Learning,"Many real-world decision problems are characterized by multiple conflicting
objectives which must be balanced based on their relative importance. In the
dynamic weights setting the relative importance changes over time and
specialized algorithms that deal with such change, such as a tabular
Reinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are
required. However, this earlier work is not feasible for RL settings that
necessitate the use of function approximators. We generalize across weight
changes and high-dimensional inputs by proposing a multi-objective Q-network
whose outputs are conditioned on the relative importance of objectives and we
introduce Diverse Experience Replay (DER) to counter the inherent
non-stationarity of the Dynamic Weights setting. We perform an extensive
experimental evaluation and compare our methods to adapted algorithms from Deep
Multi-Task/Multi-Objective Reinforcement Learning and show that our proposed
network in combination with DER dominates these adapted algorithms across
weight change scenarios and problem domains."
['stat.ML'],Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization,"Due to the high computational demands executing a rigorous comparison between
hyperparameter optimization (HPO) methods is often cumbersome. The goal of this
paper is to facilitate a better empirical evaluation of HPO methods by
providing benchmarks that are cheap to evaluate, but still represent realistic
use cases. We believe these benchmarks provide an easy and efficient way to
conduct reproducible experiments for neural hyperparameter search. Our
benchmarks consist of a large grid of configurations of a feed forward neural
network on four different regression datasets including architectural
hyperparameters and hyperparameters concerning the training pipeline. Based on
this data, we performed an in-depth analysis to gain a better understanding of
the properties of the optimization problem, as well as of the importance of
different types of hyperparameters. Second, we exhaustively compared various
different state-of-the-art methods from the hyperparameter optimization
literature on these benchmarks in terms of performance and robustness."
['stat.ML'],Exploration Conscious Reinforcement Learning Revisited,"The Exploration-Exploitation tradeoff arises in Reinforcement Learning when
one cannot tell if a policy is optimal. Then, there is a constant need to
explore new actions instead of exploiting past experience. In practice, it is
common to resolve the tradeoff by using a fixed exploration mechanism, such as
$\epsilon$-greedy exploration or by adding Gaussian noise, while still trying
to learn an optimal policy. In this work, we take a different approach and
study exploration-conscious criteria, that result in optimal policies with
respect to the exploration mechanism. Solving these criteria, as we establish,
amounts to solving a surrogate Markov Decision Process. We continue and analyze
properties of exploration-conscious optimal policies and characterize two
general approaches to solve such criteria. Building on the approaches, we apply
simple changes in existing tabular and deep Reinforcement Learning algorithms
and empirically demonstrate superior performance relatively to their
non-exploration-conscious counterparts, both for discrete and continuous action
spaces."
['stat.ML'],Action Robust Reinforcement Learning and Applications in Continuous Control,"A policy is said to be robust if it maximizes the reward while considering a
bad, or even adversarial, model. In this work we formalize two new criteria of
robustness to action uncertainty. Specifically, we consider two scenarios in
which the agent attempts to perform an action $a$, and (i) with probability
$\alpha$, an alternative adversarial action $\bar a$ is taken, or (ii) an
adversary adds a perturbation to the selected action in the case of continuous
action space. We show that our criteria are related to common forms of
uncertainty in robotics domains, such as the occurrence of abrupt forces, and
suggest algorithms in the tabular case. Building on the suggested algorithms,
we generalize our approach to deep reinforcement learning (DRL) and provide
extensive experiments in the various MuJoCo domains. Our experiments show that
not only does our approach produce robust policies, but it also improves the
performance in the absence of perturbations. This generalization indicates that
action-robustness can be thought of as implicit regularization in RL problems."
['stat.ML'],Efficient Model-free Reinforcement Learning in Metric Spaces,"Model-free Reinforcement Learning (RL) algorithms such as Q-learning
[Watkins, Dayan 92] have been widely used in practice and can achieve human
level performance in applications such as video games [Mnih et al. 15].
Recently, equipped with the idea of optimism in the face of uncertainty,
Q-learning algorithms [Jin, Allen-Zhu, Bubeck, Jordan 18] can be proven to be
sample efficient for discrete tabular Markov Decision Processes (MDPs) which
have finite number of states and actions. In this work, we present an efficient
model-free Q-learning based algorithm in MDPs with a natural metric on the
state-action space--hence extending efficient model-free Q-learning algorithms
to continuous state-action space. Compared to previous model-based RL
algorithms for metric spaces [Kakade, Kearns, Langford 03], our algorithm does
not require access to a black-box planning oracle."
['stat.ML'],Discovering Fair Representations in the Data Domain,"Interpretability and fairness are critical in computer vision and machine
learning applications, in particular when dealing with human outcomes, e.g.
inviting or not inviting for a job interview based on application materials
that may include photographs. One promising direction to achieve fairness is by
learning data representations that remove the semantics of protected
characteristics, and are therefore able to mitigate unfair outcomes. All
available models however learn latent embeddings which comes at the cost of
being uninterpretable. We propose to cast this problem as data-to-data
translation, i.e. learning a mapping from an input domain to a fair target
domain, where a fairness definition is being enforced. Here the data domain can
be images, or any tabular data representation. This task would be
straightforward if we had fair target data available, but this is not the case.
To overcome this, we learn a highly unconstrained mapping by exploiting
statistics of residuals - the difference between input data and its translated
version - and the protected characteristics. When applied to the CelebA dataset
of face images with gender attribute as the protected characteristic, our model
enforces equality of opportunity by adjusting the eyes and lips regions.
Intriguingly, on the same dataset we arrive at similar conclusions when using
semantic attribute representations of images for translation. On face images of
the recent DiF dataset, with the same gender attribute, our method adjusts nose
regions. In the Adult income dataset, also with protected gender attribute, our
model achieves equality of opportunity by, among others, obfuscating the wife
and husband relationship. Analyzing those systematic changes will allow us to
scrutinize the interplay of fairness criterion, chosen protected
characteristics, and prediction performance."
"['stat.ME', 'stat.ML']",SAFE ML: Surrogate Assisted Feature Extraction for Model Learning,"Complex black-box predictive models may have high accuracy, but opacity
causes problems like lack of trust, lack of stability, sensitivity to concept
drift. On the other hand, interpretable models require more work related to
feature engineering, which is very time consuming. Can we train interpretable
and accurate models, without timeless feature engineering? In this article, we
show a method that uses elastic black-boxes as surrogate models to create a
simpler, less opaque, yet still accurate and interpretable glass-box models.
New models are created on newly engineered features extracted/learned with the
help of a surrogate model. We show applications of this method for model level
explanations and possible extensions for instance level explanations. We also
present an example implementation in Python and benchmark this method on a
number of tabular data sets."
['stat.ML'],A Comparative Analysis of Expected and Distributional Reinforcement Learning,"Since their introduction a year ago, distributional approaches to
reinforcement learning (distributional RL) have produced strong results
relative to the standard approach which models expected values (expected RL).
However, aside from convergence guarantees, there have been few theoretical
results investigating the reasons behind the improvements distributional RL
provides. In this paper we begin the investigation into this fundamental
question by analyzing the differences in the tabular, linear approximation, and
non-linear approximation settings. We prove that in many realizations of the
tabular and linear approximation settings, distributional RL behaves exactly
the same as expected RL. In cases where the two methods behave differently,
distributional RL can in fact hurt performance when it does not induce
identical behaviour. We then continue with an empirical analysis comparing
distributional and expected RL methods in control settings with non-linear
approximators to tease apart where the improvements from distributional RL
methods are coming from."
['stat.ML'],Automatic Bayesian Density Analysis,"Making sense of a dataset in an automatic and unsupervised fashion is a
challenging problem in statistics and AI. Classical approaches for {exploratory
data analysis} are usually not flexible enough to deal with the uncertainty
inherent to real-world data: they are often restricted to fixed latent
interaction models and homogeneous likelihoods; they are sensitive to missing,
corrupt and anomalous data; moreover, their expressiveness generally comes at
the price of intractable inference. As a result, supervision from statisticians
is usually needed to find the right model for the data. However, since domain
experts are not necessarily also experts in statistics, we propose Automatic
Bayesian Density Analysis (ABDA) to make exploratory data analysis accessible
at large. Specifically, ABDA allows for automatic and efficient missing value
estimation, statistical data type and likelihood discovery, anomaly detection
and dependency structure mining, on top of providing accurate density
estimation. Extensive empirical evidence shows that ABDA is a suitable tool for
automatic exploratory analysis of mixed continuous and discrete tabular data."
['stat.ML'],Distributional reinforcement learning with linear function approximation,"Despite many algorithmic advances, our theoretical understanding of practical
distributional reinforcement learning methods remains limited. One exception is
Rowland et al. (2018)'s analysis of the C51 algorithm in terms of the Cram\'er
distance, but their results only apply to the tabular setting and ignore C51's
use of a softmax to produce normalized distributions. In this paper we adapt
the Cram\'er distance to deal with arbitrary vectors. From it we derive a new
distributional algorithm which is fully Cram\'er-based and can be combined to
linear function approximation, with formal guarantees in the context of policy
evaluation. In allowing the model's prediction to be any real vector, we lose
the probabilistic interpretation behind the method, but otherwise maintain the
appealing properties of distributional approaches. To the best of our
knowledge, ours is the first proof of convergence of a distributional algorithm
combined with function approximation. Perhaps surprisingly, our results provide
evidence that Cram\'er-based distributional methods may perform worse than
directly approximating the value function."
['stat.ML'],Source Traces for Temporal Difference Learning,"This paper motivates and develops source traces for temporal difference (TD)
learning in the tabular setting. Source traces are like eligibility traces, but
model potential histories rather than immediate ones. This allows TD errors to
be propagated to potential causal states and leads to faster generalization.
Source traces can be thought of as the model-based, backward view of successor
representations (SR), and share many of the same benefits. This view, however,
suggests several new ideas. First, a TD($\lambda$)-like source learning
algorithm is proposed and its convergence is proven. Then, a novel algorithm
for learning the source map (or SR matrix) is developed and shown to outperform
the previous algorithm. Finally, various approaches to using the source/SR
model are explored, and it is shown that source traces can be effectively
combined with other model-based methods like Dyna and experience replay."
['stat.ML'],Provably Efficient Maximum Entropy Exploration,"Suppose an agent is in a (possibly unknown) Markov Decision Process in the
absence of a reward signal, what might we hope that an agent can efficiently
learn to do? This work studies a broad class of objectives that are defined
solely as functions of the state-visitation frequencies that are induced by how
the agent behaves. For example, one natural, intrinsically defined, objective
problem is for the agent to learn a policy which induces a distribution over
state space that is as uniform as possible, which can be measured in an
entropic sense. We provide an efficient algorithm to optimize such such
intrinsically defined objectives, when given access to a black box planning
oracle (which is robust to function approximation). Furthermore, when
restricted to the tabular setting where we have sample based access to the MDP,
our proposed algorithm is provably efficient, both in terms of its sample and
computational complexities. Key to our algorithmic methodology is utilizing the
conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes
an approximate MDP solver."
['stat.ML'],Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to Reach New Goals,"Consider mutli-goal tasks that involve static environments and dynamic goals.
Examples of such tasks, such as goal-directed navigation and pick-and-place in
robotics, abound. Two types of Reinforcement Learning (RL) algorithms are used
for such tasks: model-free or model-based. Each of these approaches has
limitations. Model-free RL struggles to transfer learned information when the
goal location changes, but achieves high asymptotic accuracy in single goal
tasks. Model-based RL can transfer learned information to new goal locations by
retaining the explicitly learned state-dynamics, but is limited by the fact
that small errors in modelling these dynamics accumulate over long-term
planning. In this work, we improve upon the limitations of model-free RL in
multi-goal domains. We do this by adapting the Floyd-Warshall algorithm for RL
and call the adaptation Floyd-Warshall RL (FWRL). The proposed algorithm learns
a goal-conditioned action-value function by constraining the value of the
optimal path between any two states to be greater than or equal to the value of
paths via intermediary states. Experimentally, we show that FWRL is more
sample-efficient and learns higher reward strategies in multi-goal tasks as
compared to Q-learning, model-based RL and other relevant baselines in a
tabular domain."
['stat.ML'],NIPS - Not Even Wrong? A Systematic Review of Empirically Complete Demonstrations of Algorithmic Effectiveness in the Machine Learning and Artificial Intelligence Literature,"Objective: To determine the completeness of argumentative steps necessary to
conclude effectiveness of an algorithm in a sample of current ML/AI supervised
learning literature.
  Data Sources: Papers published in the Neural Information Processing Systems
(NeurIPS, n\'ee NIPS) journal where the official record showed a 2017 year of
publication.
  Eligibility Criteria: Studies reporting a (semi-)supervised model, or
pre-processing fused with (semi-)supervised models for tabular data.
  Study Appraisal: Three reviewers applied the assessment criteria to determine
argumentative completeness. The criteria were split into three groups,
including: experiments (e.g real and/or synthetic data), baselines (e.g
uninformed and/or state-of-art) and quantitative comparison (e.g. performance
quantifiers with confidence intervals and formal comparison of the algorithm
against baselines).
  Results: Of the 121 eligible manuscripts (from the sample of 679 abstracts),
99\% used real-world data and 29\% used synthetic data. 91\% of manuscripts did
not report an uninformed baseline and 55\% reported a state-of-art baseline.
32\% reported confidence intervals for performance but none provided references
or exposition for how these were calculated. 3\% reported formal comparisons.
  Limitations: The use of one journal as the primary information source may not
be representative of all ML/AI literature. However, the NeurIPS conference is
recognised to be amongst the top tier concerning ML/AI studies, so it is
reasonable to consider its corpus to be representative of high-quality
research.
  Conclusion: Using the 2017 sample of the NeurIPS supervised learning corpus
as an indicator for the quality and trustworthiness of current ML/AI research,
it appears that complete argumentative chains in demonstrations of algorithmic
effectiveness are rare."
['stat.ML'],Scalable Coordinated Exploration in Concurrent Reinforcement Learning,"We consider a team of reinforcement learning agents that concurrently operate
in a common environment, and we develop an approach to efficient coordinated
exploration that is suitable for problems of practical scale. Our approach
builds on seed sampling (Dimakopoulou and Van Roy, 2018) and randomized value
function learning (Osband et al., 2016). We demonstrate that, for simple
tabular contexts, the approach is competitive with previously proposed tabular
model learning methods (Dimakopoulou and Van Roy, 2018). With a
higher-dimensional problem and a neural network value function representation,
the approach learns quickly with far fewer agents than alternative exploration
schemes."
['stat.ML'],Adversarially Learned Anomaly Detection,"Anomaly detection is a significant and hence well-studied problem. However,
developing effective anomaly detection methods for complex and high-dimensional
data remains a challenge. As Generative Adversarial Networks (GANs) are able to
model the complex high-dimensional distributions of real-world data, they offer
a promising approach to address this challenge. In this work, we propose an
anomaly detection method, Adversarially Learned Anomaly Detection (ALAD) based
on bi-directional GANs, that derives adversarially learned features for the
anomaly detection task. ALAD then uses reconstruction errors based on these
adversarially learned features to determine if a data sample is anomalous. ALAD
builds on recent advances to ensure data-space and latent-space
cycle-consistencies and stabilize GAN training, which results in significantly
improved anomaly detection performance. ALAD achieves state-of-the-art
performance on a range of image and tabular datasets while being several
hundred-fold faster at test time than the only published GAN-based method."
['stat.ML'],Synthesizing Tabular Data using Generative Adversarial Networks,"Generative adversarial networks (GANs) implicitly learn the probability
distribution of a dataset and can draw samples from the distribution. This
paper presents, Tabular GAN (TGAN), a generative adversarial network which can
generate tabular data like medical or educational records. Using the power of
deep neural networks, TGAN generates high-quality and fully synthetic tables
while simultaneously generating discrete and continuous variables. When we
evaluate our model on three datasets, we find that TGAN outperforms
conventional statistical generative models in both capturing the correlation
between columns and scaling up for large datasets."
['stat.ML'],Performance Guarantees for Homomorphisms Beyond Markov Decision Processes,"Most real-world problems have huge state and/or action spaces. Therefore, a
naive application of existing tabular solution methods is not tractable on such
problems. Nonetheless, these solution methods are quite useful if an agent has
access to a relatively small state-action space homomorphism of the true
environment and near-optimal performance is guaranteed by the map. A plethora
of research is focused on the case when the homomorphism is a Markovian
representation of the underlying process. However, we show that near-optimal
performance is sometimes guaranteed even if the homomorphism is non-Markovian.
Moreover, we can aggregate significantly more states by lifting the Markovian
requirement without compromising on performance. In this work, we expand
Extreme State Aggregation (ESA) framework to joint state-action aggregations.
We also lift the policy uniformity condition for aggregation in ESA that allows
even coarser modeling of the true environment."
['stat.ML'],Reward Estimation for Variance Reduction in Deep Reinforcement Learning,"Reinforcement Learning (RL) agents require the specification of a reward
signal for learning behaviours. However, introduction of corrupt or stochastic
rewards can yield high variance in learning. Such corruption may be a direct
result of goal misspecification, randomness in the reward signal, or
correlation of the reward with external factors that are not known to the
agent. Corruption or stochasticity of the reward signal can be especially
problematic in robotics, where goal specification can be particularly difficult
for complex tasks. While many variance reduction techniques have been studied
to improve the robustness of the RL process, handling such stochastic or
corrupted reward structures remains difficult. As an alternative for handling
this scenario in model-free RL methods, we suggest using an estimator for both
rewards and value functions. We demonstrate that this improves performance
under corrupted stochastic rewards in both the tabular and non-linear function
approximation settings for a variety of noise types and environments. The use
of reward estimation is a robust and easy-to-implement improvement for handling
corrupted reward signals in model-free RL."
['stat.ML'],Regularization Learning Networks: Deep Learning for Tabular Datasets,"Despite their impressive performance, Deep Neural Networks (DNNs) typically
underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning
tasks. We propose that applying a different regularization coefficient to each
weight might boost the performance of DNNs by allowing them to make more use of
the more relevant inputs. However, this will lead to an intractable number of
hyperparameters. Here, we introduce Regularization Learning Networks (RLNs),
which overcome this challenge by introducing an efficient hyperparameter tuning
scheme which minimizes a new Counterfactual Loss. Our results show that RLNs
significantly improve DNNs on tabular datasets, and achieve comparable results
to GBTs, with the best performance achieved with an ensemble that combines GBTs
and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of
the network edges and 82% of the input features, thus providing more
interpretable models and reveal the importance that the network assigns to
different inputs. RLNs could efficiently learn a single network in datasets
that comprise both tabular and unstructured data, such as in the setting of
medical imaging accompanied by electronic health records. An open source
implementation of RLN can be found at
https://github.com/irashavitt/regularization_learning_networks."
['stat.ML'],The Laplacian in RL: Learning Representations with Efficient Approximations,"The smallest eigenvectors of the graph Laplacian are well-known to provide a
succinct representation of the geometry of a weighted graph. In reinforcement
learning (RL), where the weighted graph may be interpreted as the state
transition process induced by a behavior policy acting on the environment,
approximating the eigenvectors of the Laplacian provides a promising approach
to state representation learning. However, existing methods for performing this
approximation are ill-suited in general RL settings for two main reasons:
First, they are computationally expensive, often requiring operations on large
matrices. Second, these methods lack adequate justification beyond simple,
tabular, finite-state settings. In this paper, we present a fully general and
scalable method for approximating the eigenvectors of the Laplacian in a
model-free RL context. We systematically evaluate our approach and empirically
show that it generalizes beyond the tabular, finite-state setting. Even in
tabular, finite-state settings, its ability to approximate the eigenvectors
outperforms previous proposals. Finally, we show the potential benefits of
using a Laplacian representation learned using our method in goal-achieving RL
tasks, providing evidence that our technique can be used to significantly
improve the performance of an RL agent."
['stat.ML'],Detecting Potential Local Adversarial Examples for Human-Interpretable Defense,"Machine learning models are increasingly used in the industry to make
decisions such as credit insurance approval. Some people may be tempted to
manipulate specific variables, such as the age or the salary, in order to get
better chances of approval. In this ongoing work, we propose to discuss, with a
first proposition, the issue of detecting a potential local adversarial example
on classical tabular data by providing to a human expert the locally critical
features for the classifier's decision, in order to control the provided
information and avoid a fraud."
['stat.ML'],Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models,"Interpretation and diagnosis of machine learning models have gained renewed
interest in recent years with breakthroughs in new approaches. We present
Manifold, a framework that utilizes visual analysis techniques to support
interpretation, debugging, and comparison of machine learning models in a more
transparent and interactive manner. Conventional techniques usually focus on
visualizing the internal logic of a specific model type (i.e., deep neural
networks), lacking the ability to extend to a more complex scenario where
different model types are integrated. To this end, Manifold is designed as a
generic framework that does not rely on or access the internal logic of the
model and solely observes the input (i.e., instances or features) and the
output (i.e., the predicted result and probability distribution). We describe
the workflow of Manifold as an iterative process consisting of three major
phases that are commonly involved in the model development and diagnosis
process: inspection (hypothesis), explanation (reasoning), and refinement
(verification). The visual components supporting these tasks include a
scatterplot-based visual summary that overviews the models' outcome and a
customizable tabular view that reveals feature discrimination. We demonstrate
current applications of the framework on the classification and regression
tasks and discuss other potential machine learning use scenarios where Manifold
can be applied."
['stat.ML'],Learning to Play Pong using Policy Gradient Learning,"Activities in reinforcement learning (RL) revolve around learning the Markov
decision process (MDP) model, in particular, the following parameters: state
values, V; state-action values, Q; and policy, pi. These parameters are
commonly implemented as an array. Scaling up the problem means scaling up the
size of the array and this will quickly lead to a computational bottleneck. To
get around this, the RL problem is commonly formulated to learn a specific task
using hand-crafted input features to curb the size of the array. In this
report, we discuss an alternative end-to-end Deep Reinforcement Learning (DRL)
approach where the DRL attempts to learn general task representations which in
our context refers to learning to play the Pong game from a sequence of screen
snapshots without game-specific hand-crafted features. We apply artificial
neural networks (ANN) to approximate a policy of the RL model. The policy
network, via Policy Gradients (PG) method, learns to play the Pong game from a
sequence of frames without any extra semantics apart from the pixel information
and the score. In contrast to the traditional tabular RL approach where the
contents in the array have clear interpretations such as V or Q, the
interpretation of knowledge content from the weights of the policy network is
more illusive. In this work, we experiment with various Deep ANN architectures
i.e., Feed forward ANN (FFNN), Convolution ANN (CNN) and Asynchronous Advantage
Actor-Critic (A3C). We also examine the activation of hidden nodes and the
weights between the input and the hidden layers, before and after the DRL has
successfully learnt to play the Pong game. Insights into the internal learning
mechanisms and future research directions are then discussed."
['stat.ML'],GAN Q-learning,"Distributional reinforcement learning (distributional RL) has seen empirical
success in complex Markov Decision Processes (MDPs) in the setting of nonlinear
function approximation. However, there are many different ways in which one can
leverage the distributional approach to reinforcement learning. In this paper,
we propose GAN Q-learning, a novel distributional RL method based on generative
adversarial networks (GANs) and analyze its performance in simple tabular
environments, as well as OpenAI Gym. We empirically show that our algorithm
leverages the flexibility and blackbox approach of deep learning models while
providing a viable alternative to traditional methods."
['stat.ML'],Equivalence Between Wasserstein and Value-Aware Loss for Model-based Reinforcement Learning,"Learning a generative model is a key component of model-based reinforcement
learning. Though learning a good model in the tabular setting is a simple task,
learning a useful model in the approximate setting is challenging. In this
context, an important question is the loss function used for model learning as
varying the loss function can have a remarkable impact on effectiveness of
planning. Recently Farahmand et al. (2017) proposed a value-aware model
learning (VAML) objective that captures the structure of value function during
model learning. Using tools from Asadi et al. (2018), we show that minimizing
the VAML objective is in fact equivalent to minimizing the Wasserstein metric.
This equivalence improves our understanding of value-aware models, and also
creates a theoretical foundation for applications of Wasserstein in model-based
reinforcement~learning."
['stat.ML'],Many-Goals Reinforcement Learning,"All-goals updating exploits the off-policy nature of Q-learning to update all
possible goals an agent could have from each transition in the world, and was
introduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work
this was mostly explored in small-state RL problems that allowed tabular
representations and where all possible goals could be explicitly enumerated and
learned separately. In this paper we empirically explore 3 different extensions
of the idea of updating many (instead of all) goals in the context of RL with
deep neural networks (or DeepRL for short). First, in a direct adaptation of
Kaelbling's approach we explore if many-goals updating can be used to achieve
mastery in non-tabular visual-observation domains. Second, we explore whether
many-goals updating can be used to pre-train a network to subsequently learn
faster and better on a single main task of interest. Third, we explore whether
many-goals updating can be used to provide auxiliary task updates in training a
network to learn faster and better on a single main task of interest. We
provide comparisons to baselines for each of the 3 extensions."
['stat.ML'],Deep Neural Decision Trees,"Deep neural networks have been proven powerful at processing perceptual data,
such as images and audio. However for tabular data, tree-based models are more
popular. A nice property of tree-based models is their natural
interpretability. In this work, we present Deep Neural Decision Trees (DNDT) --
tree models realised by neural networks. A DNDT is intrinsically interpretable,
as it is a tree. Yet as it is also a neural network (NN), it can be easily
implemented in NN toolkits, and trained with gradient descent rather than
greedy splitting. We evaluate DNDT on several tabular datasets, verify its
efficacy, and investigate similarities and differences between DNDT and vanilla
decision trees. Interestingly, DNDT self-prunes at both split and
feature-level."
['stat.ML'],Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation,"Modern reinforcement learning algorithms reach super-human performance on
many board and video games, but they are sample inefficient, i.e. they
typically require significantly more playing experience than humans to reach an
equal performance level. To improve sample efficiency, an agent may build a
model of the environment and use planning methods to update its policy. In this
article we introduce Variational State Tabulation (VaST), which maps an
environment with a high-dimensional state space (e.g. the space of visual
inputs) to an abstract tabular model. Prioritized sweeping with small backups,
a highly efficient planning method, can then be used to update state-action
values. We show how VaST can rapidly learn to maximize reward in tasks like 3D
navigation and efficiently adapt to sudden changes in rewards or transition
probabilities."
['stat.ML'],Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,"This paper considers the problem of inverse reinforcement learning in
zero-sum stochastic games when expert demonstrations are known to be not
optimal. Compared to previous works that decouple agents in the game by
assuming optimality in expert strategies, we introduce a new objective function
that directly pits experts against Nash Equilibrium strategies, and we design
an algorithm to solve for the reward function in the context of inverse
reinforcement learning with deep neural networks as model approximations. In
our setting the model and algorithm do not decouple by agent. In order to find
Nash Equilibrium in large-scale games, we also propose an adversarial training
algorithm for zero-sum stochastic games, and show the theoretical appeal of
non-existence of local optima in its objective function. In our numerical
experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement
learning algorithms address games that are not amenable to previous approaches
using tabular representations. Moreover, with sub-optimal expert demonstrations
our algorithms recover both reward functions and strategies with good quality."
['stat.ML'],TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,"Our understanding of reinforcement learning (RL) has been shaped by
theoretical and empirical results that were obtained decades ago using tabular
representations and linear function approximators. These results suggest that
RL methods that use temporal differencing (TD) are superior to direct Monte
Carlo estimation (MC). How do these results hold up in deep RL, which deals
with perceptually complex environments and deep nonlinear models? In this
paper, we re-examine the role of TD in modern deep RL, using specially designed
environments that control for specific factors that affect performance, such as
reward sparsity, reward delay, and the perceptual complexity of the task. When
comparing TD with infinite-horizon MC, we are able to reproduce classic results
in modern settings. Yet we also find that finite-horizon MC is not inferior to
TD, even when rewards are sparse or delayed. This makes MC a viable alternative
to TD in deep RL."
['stat.ML'],Multi-Layered Gradient Boosting Decision Trees,"Multi-layered representation is believed to be the key ingredient of deep
neural networks especially in cognitive tasks like computer vision. While
non-differentiable models such as gradient boosting decision trees (GBDTs) are
the dominant methods for modeling discrete or tabular data, they are hard to
incorporate with such representation learning ability. In this work, we propose
the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring
the ability to learn hierarchical representations by stacking several layers of
regression GBDTs as its building block. The model can be jointly trained by a
variant of target propagation across layers, without the need to derive
back-propagation nor differentiability. Experiments and visualizations
confirmed the effectiveness of the model in terms of performance and
representation learning ability."
['stat.ML'],Benchmarking projective simulation in navigation problems,"Projective simulation (PS) is a model for intelligent agents with a
deliberation capacity that is based on episodic memory. The model has been
shown to provide a flexible framework for constructing reinforcement-learning
agents, and it allows for quantum mechanical generalization, which leads to a
speed-up in deliberation time. PS agents have been applied successfully in the
context of complex skill learning in robotics, and in the design of
state-of-the-art quantum experiments. In this paper, we study the performance
of projective simulation in two benchmarking problems in navigation, namely the
grid world and the mountain car problem. The performance of PS is compared to
standard tabular reinforcement learning approaches, Q-learning and SARSA. Our
comparison demonstrates that the performance of PS and standard learning
approaches are qualitatively and quantitatively similar, while it is much
easier to choose optimal model parameters in case of projective simulation,
with a reduced computational effort of one to two orders of magnitude. Our
results show that the projective simulation model stands out for its simplicity
in terms of the number of model parameters, which makes it simple to set up the
learning agent in unknown task environments."
['stat.ML'],Active Reinforcement Learning with Monte-Carlo Tree Search,"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes
reward information only if it pays a cost. This subtle change makes exploration
substantially more challenging. Powerful principles in RL like optimism,
Thompson sampling, and random exploration do not help with ARL. We relate ARL
in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm
using Monte-Carlo Tree Search that is asymptotically Bayes optimal.
Experimentally, this algorithm is near-optimal on small Bandit problems and
MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised
heuristics for ARL. By analysing exploration behaviour in detail, we uncover
obstacles to scaling up simulation-based algorithms for ARL."
['stat.ML'],An experimental study of graph-based semi-supervised classification with additional node information,"The volume of data generated by internet and social networks is increasing
every day, and there is a clear need for efficient ways of extracting useful
information from them. As those data can take different forms, it is important
to use all the available data representations for prediction.
  In this paper, we focus our attention on supervised classification using both
regular plain, tabular, data and structural information coming from a network
structure. 14 techniques are investigated and compared in this study and can be
divided in three classes: the first one uses only the plain data to build a
classification model, the second uses only the graph structure and the last
uses both information sources. The relative performances in these three cases
are investigated. Furthermore, the effect of using a graph embedding and
well-known indicators in spatial statistics is also studied.
  Possible applications are automatic classification of web pages or other
linked documents, of people in a social network or of proteins in a biological
complex system, to name a few.
  Based on our comparison, we draw some general conclusions and advices to
tackle this particular classification task: some datasets can be better
explained by their graph structure (graph-driven), or by their feature set
(features-driven). The most efficient methods are discussed in both cases."
['stat.ML'],Neural Episodic Control,"Deep reinforcement learning methods attain super-human performance in a wide
range of environments. Such methods are grossly inefficient, often taking
orders of magnitudes more data than humans to achieve reasonable performance.
We propose Neural Episodic Control: a deep reinforcement learning agent that is
able to rapidly assimilate new experiences and act upon them. Our agent uses a
semi-tabular representation of the value function: a buffer of past experience
containing slowly changing state representations and rapidly updated estimates
of the value function. We show across a wide range of environments that our
agent learns significantly faster than other state-of-the-art, general purpose
deep reinforcement learning agents."
['stat.ML'],Bayesian Differential Privacy through Posterior Sampling,"Differential privacy formalises privacy-preserving mechanisms that provide
access to a database. We pose the question of whether Bayesian inference itself
can be used directly to provide private access to data, with no modification.
The answer is affirmative: under certain conditions on the prior, sampling from
the posterior distribution can be used to achieve a desired level of privacy
and utility. To do so, we generalise differential privacy to arbitrary dataset
metrics, outcome spaces and distribution families. This allows us to also deal
with non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of
the posterior to the data, which gives a measure of robustness. We also show
how to use posterior sampling to provide differentially private responses to
queries, within a decision-theoretic framework. Finally, we provide bounds on
the utility and on the distinguishability of datasets. The latter are
complemented by a novel use of Le Cam's method to obtain lower bounds. All our
general results hold for arbitrary database metrics, including those for the
common definition of differential privacy. For specific choices of the metric,
we give a number of examples satisfying our assumptions."
['stat.ML'],Generalized Low Rank Models,"Principal components analysis (PCA) is a well-known technique for
approximating a tabular data set by a low rank matrix. Here, we extend the idea
of PCA to handle arbitrary data sets consisting of numerical, Boolean,
categorical, ordinal, and other data types. This framework encompasses many
well known techniques in data analysis, such as nonnegative matrix
factorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,
and maximum margin matrix factorization. The method handles heterogeneous data
sets, and leads to coherent schemes for compressing, denoising, and imputing
missing entries across all data types simultaneously. It also admits a number
of interesting interpretations of the low rank factors, which allow clustering
of examples or of features. We propose several parallel algorithms for fitting
generalized low rank models, and describe implementations and numerical
results."
['stat.ML'],Chi-square Tests Driven Method for Learning the Structure of Factored MDPs,"SDYNA is a general framework designed to address large stochastic
reinforcement learning problems. Unlike previous model based methods in FMDPs,
it incrementally learns the structure and the parameters of a RL problem using
supervised learning techniques. Then, it integrates decision-theoric planning
algorithms based on FMDPs to compute its policy. SPITI is an instanciation of
SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the
reward function and the Dynamic Bayesian Networks with local structures
representing the transition function of the problem. These representations are
used by an incremental version of the Structured Value Iteration algorithm. In
order to learn the structure, SPITI uses Chi-Square tests to detect the
independence between two probability distributions. Thus, we study the relation
between the threshold used in the Chi-Square test, the size of the model built
and the relative error of the value function of the induced policy with respect
to the optimal value. We show that, on stochastic problems, one can tune the
threshold so as to generate both a compact model and an efficient policy. Then,
we show that SPITI, while keeping its model compact, uses the generalization
property of its learning method to perform better than a stochastic classical
tabular algorithm in large RL problem with an unknown structure. We also
introduce a new measure based on Chi-Square to qualify the accuracy of the
model learned by SPITI. We qualitatively show that the generalization property
in SPITI within the FMDP framework may prevent an exponential growth of the
time required to learn the structure of large stochastic RL problems."
[],Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities,"Graph neural networks (GNNs) have been widely used to learn vector
representation of graph-structured data and achieved better task performance
than conventional methods. The foundation of GNNs is the message passing
procedure, which propagates the information in a node to its neighbors. Since
this procedure proceeds one step per layer, the range of the information
propagation among nodes is small in the lower layers, and it expands toward the
higher layers. Therefore, a GNN model has to be deep enough to capture global
structural information in a graph. On the other hand, it is known that deep GNN
models suffer from performance degradation because they lose nodes' local
information, which would be essential for good model performance, through many
message passing steps. In this study, we propose multi-level attention pooling
(MLAP) for graph-level classification tasks, which can adapt to both local and
global structural information in a graph. It has an attention pooling layer for
each message passing step and computes the final graph representation by
unifying the layer-wise graph representations. The MLAP architecture allows
models to utilize the structural information of graphs with multiple levels of
localities because it preserves layer-wise information before losing them due
to oversmoothing. Results of our experiments show that the MLAP architecture
improves the graph classification performance compared to the baseline
architectures. In addition, analyses on the layer-wise graph representations
suggest that aggregating information from multiple levels of localities indeed
has the potential to improve the discriminability of learned graph
representations."
[],Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes,"Deep networks and decision forests (such as random forests and gradient
boosted trees) are the leading machine learning methods for structured and
tabular data, respectively. Many papers have empirically compared large numbers
of classifiers on one or two different domains (e.g., on 100 different tabular
data settings). However, a careful conceptual and empirical comparison of these
two strategies using the most contemporary best practices has yet to be
performed. Conceptually, we illustrate that both can be profitably viewed as
""partition and vote"" schemes. Specifically, the representation space that they
both learn is a partitioning of feature space into a union of convex polytopes.
For inference, each decides on the basis of votes from the activated nodes.
This formulation allows for a unified basic understanding of the relationship
between these methods. Empirically, we compare these two strategies on hundreds
of tabular data settings, as well as several vision and auditory settings. Our
focus is on datasets with at most 10,000 samples, which represent a large
fraction of scientific and biomedical datasets. In general, we found forests to
excel at tabular and structured data (vision and audition) with small sample
sizes, whereas deep nets performed better on structured data with larger sample
sizes. This suggests that further gains in both scenarios may be realized via
further combining aspects of forests and networks. We will continue revising
this technical report in the coming months with updated results."
[],Releasing Graph Neural Networks with Differential Privacy Guarantees,"With the increasing popularity of Graph Neural Networks (GNNs) in several
sensitive applications like healthcare and medicine, concerns have been raised
over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to
privacy attacks, such as membership inference attacks, even if only blackbox
access to the trained model is granted. To build defenses, differential privacy
has emerged as a mechanism to disguise the sensitive data in training datasets.
Following the strategy of Private Aggregation of Teacher Ensembles (PATE),
recent methods leverage a large ensemble of teacher models. These teachers are
trained on disjoint subsets of private data and are employed to transfer
knowledge to a student model, which is then released with privacy guarantees.
However, splitting graph data into many disjoint training sets may destroy the
structural information and adversely affect accuracy. We propose a new
graph-specific scheme of releasing a student GNN, which avoids splitting
private training data altogether. The student GNN is trained using public data,
partly labeled privately using the teacher GNN models trained exclusively for
each query node. We theoretically analyze our approach in the R\`{e}nyi
differential privacy framework and provide privacy guarantees. Besides, we show
the solid experimental performance of our method compared to several baselines,
including the PATE baseline adapted for graph-structured data. Our anonymized
code is available."
[],Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification,"Machine learning solutions for pattern classification problems are nowadays
widely deployed in society and industry. However, the lack of transparency and
accountability of most accurate models often hinders their safe use. Thus,
there is a clear need for developing explainable artificial intelligence
mechanisms. There exist model-agnostic methods that summarize feature
contributions, but their interpretability is limited to predictions made by
black-box models. An open challenge is to develop models that have intrinsic
interpretability and produce their own explanations, even for classes of models
that are traditionally considered black boxes like (recurrent) neural networks.
In this paper, we propose a Long-Term Cognitive Network for interpretable
pattern classification of structured data. Our method brings its own mechanism
for providing explanations by quantifying the relevance of each feature in the
decision process. For supporting the interpretability without affecting the
performance, the model incorporates more flexibility through a quasi-nonlinear
reasoning rule that allows controlling nonlinearity. Besides, we propose a
recurrence-aware decision model that evades the issues posed by unique fixed
points while introducing a deterministic learning method to compute the tunable
parameters. The simulations show that our interpretable model obtains
competitive results when compared to the state-of-the-art white and black-box
models."
[],Bayesian graph convolutional neural networks via tempered MCMC,"Deep learning models, such as convolutional neural networks, have long been
applied to image and multi-media tasks, particularly those with structured
data. More recently, there has been more attention to unstructured data that
can be represented via graphs. These types of data are often found in health
and medicine, social networks, and research data repositories. Graph
convolutional neural networks have recently gained attention in the field of
deep learning that takes advantage of graph-based data representation with
automatic feature extraction via convolutions. Given the popularity of these
methods in a wide range of applications, robust uncertainty quantification is
vital. This remains a challenge for large models and unstructured datasets.
Bayesian inference provides a principled approach to uncertainty quantification
of model parameters for deep learning models. Although Bayesian inference has
been used extensively elsewhere, its application to deep learning remains
limited due to the computational requirements of the Markov Chain Monte Carlo
(MCMC) methods. Recent advances in parallel computing and advanced proposal
schemes in MCMC sampling methods has opened the path for Bayesian deep
learning. In this paper, we present Bayesian graph convolutional neural
networks that employ tempered MCMC sampling with Langevin-gradient proposal
distribution implemented via parallel computing. Our results show that the
proposed method can provide accuracy similar to advanced optimisers while
providing uncertainty quantification for key benchmark problems."
[],Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs,"Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty of
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. It allows us to use global
self-attention, the key element of transformers, directly for graphs and comes
with the benefit of long-range interaction among nodes. Moreover, the edge
channels allow the structural information to evolve from layer to layer, and
prediction tasks on edges/links can be performed directly from the output
embeddings of these channels. In addition, we introduce a generalized
positional encoding scheme for graphs based on Singular Value Decomposition
which can improve the performance of EGT. Our framework, which relies on global
node feature aggregation, achieves better performance compared to
Convolutional/Message-Passing Graph Neural Networks, which rely on local
feature aggregation within a neighborhood. We verify the performance of EGT in
a supervised learning setting on a wide range of experiments on benchmark
datasets. Our findings indicate that convolutional aggregation is not an
essential inductive bias for graphs and global self-attention can serve as a
flexible and adaptive alternative."
[],Variational Graph Normalized Auto-Encoders,"Link prediction is one of the key problems for graph-structured data. With
the advancement of graph neural networks, graph autoencoders (GAEs) and
variational graph autoencoders (VGAEs) have been proposed to learn graph
embeddings in an unsupervised way. It has been shown that these methods are
effective for link prediction tasks. However, they do not work well in link
predictions when a node whose degree is zero (i.g., isolated node) is involved.
We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero
regardless of their content features. In this paper, we propose a novel
Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization
to derive better embeddings for isolated nodes. We show that our VGNAEs
outperform the existing state-of-the-art models for link prediction tasks. The
code is available at https://github.com/SeongJinAhn/VGNAE."
[],Local Augmentation for Graph Neural Networks,"Data augmentation has been widely used in image data and linguistic data but
remains under-explored on graph-structured data. Existing methods focus on
augmenting the graph data from a global perspective and largely fall into two
genres: structural manipulation and adversarial training with feature noise
injection. However, the structural manipulation approach suffers information
loss issues while the adversarial training approach may downgrade the feature
quality by injecting noise. In this work, we introduce the local augmentation,
which enhances node features by its local subgraph structures. Specifically, we
model the data argumentation as a feature generation process. Given the central
node's feature, our local augmentation approach learns the conditional
distribution of its neighbors' features and generates the neighbors' optimal
feature to boost the performance of downstream tasks. Based on the local
augmentation, we further design a novel framework: LA-GNN, which can apply to
any GNN models in a plug-and-play manner. Extensive experiments and analyses
show that local augmentation consistently yields performance improvement for
various GNN architectures across a diverse set of benchmarks. Code is available
at https://github.com/Soughing0823/LAGNN."
[],FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks,"Graph Neural Network (GNN) research is rapidly growing thanks to the capacity
of GNNs in learning distributed representations from graph-structured data.
However, centralizing a massive amount of real-world graph data for GNN
training is prohibitive due to privacy concerns, regulation restrictions, and
commercial competitions. Federated learning (FL), a trending distributed
learning paradigm, provides possibilities to solve this challenge while
preserving data privacy. Despite recent advances in vision and language
domains, there is no suitable platform for the FL of GNNs. To this end, we
introduce FedGraphNN, an open FL benchmark system that can facilitate research
on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and
contains a wide range of datasets from different domains, popular GNN models,
and FL algorithms, with secure and efficient system support. Particularly for
the datasets, we collect, preprocess, and partition 36 datasets from 7 domains,
including both publicly available ones and specifically obtained ones such as
hERG and Tencent. Our empirical analysis showcases the utility of our benchmark
system, while exposing significant challenges in graph FL: federated GNNs
perform worse in most datasets with a non-IID split than centralized GNNs; the
GNN model that attains the best result in the centralized setting may not
maintain its advantage in the FL setting. These results imply that more
research efforts are needed to unravel the mystery behind federated GNNs.
Moreover, our system performance analysis demonstrates that the FedGraphNN
system is computationally efficient and secure to large-scale graphs datasets.
We maintain the source code at https://github.com/FedML-AI/FedGraphNN."
[],Diff-ResNets for Few-shot Learning -- an ODE Perspective,"Interpreting deep neural networks from the ordinary differential equations
(ODEs) perspective has inspired many efficient and robust network
architectures. However, existing ODE based approaches ignore the relationship
among data points, which is a critical component in many problems including
few-shot learning and semi-supervised learning. In this paper, inspired by the
diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to
strengthen the interactions among data points. Under the structured data
assumption, it is proved that the diffusion mechanism can decrease the
distance-diameter ratio that improves the separability of inter-class points
and reduces the distance among local intra-class points. This property can be
easily adopted by the residual networks for constructing the separable
hyperplanes. The synthetic binary classification experiments demonstrate the
effectiveness of the proposed diffusion mechanism. Moreover, extensive
experiments of few-shot image classification and semi-supervised graph node
classification in various datasets validate the advantages of the proposed
Diff-ResNet over existing few-shot learning methods."
[],Scale-invariant representation of machine learning,"The success of machine learning stems from its structured data
representation. Similar data have close representation as compressed codes for
classification or emerged labels for clustering. We observe that the frequency
of the internal representation follows power laws in both supervised and
unsupervised learning. The scale-invariant distribution implies that machine
learning largely compresses frequent typical data, and at the same time,
differentiates many atypical data as outliers. In this study, we derive how the
power laws can naturally arise in machine learning. In terms of information
theory, the scale-invariant representation corresponds to a maximally uncertain
data grouping among possible representations that guarantee pre-specified
learning accuracy."
[],Generation of Synthetic Electronic Health Records Using a Federated GAN,"Sensitive medical data is often subject to strict usage constraints. In this
paper, we trained a generative adversarial network (GAN) on real-world
electronic health records (EHR). It was then used to create a data-set of
""fake"" patients through synthetic data generation (SDG) to circumvent usage
constraints. This real-world data was tabular, binary, intensive care unit
(ICU) patient diagnosis data. The entire data-set was split into separate data
silos to mimic real-world scenarios where multiple ICU units across different
hospitals may have similarly structured data-sets within their own
organisations but do not have access to each other's data-sets. We implemented
federated learning (FL) to train separate GANs locally at each organisation,
using their unique data silo and then combining the GANs into a single central
GAN, without any siloed data ever being exposed. This global, central GAN was
then used to generate the synthetic patients data-set. We performed an
evaluation of these synthetic patients with statistical measures and through a
structured review by a group of medical professionals. It was shown that there
was no significant reduction in the quality of the synthetic EHR when we moved
between training a single central model and training on separate data silos
with individual models before combining them into a central model. This was
true for both the statistical evaluation (Root Mean Square Error (RMSE) of
0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also
for the medical professionals' evaluation (no quality difference between EHR
generated from a single source and EHR generated from multiple sources)."
[],Sparsifying the Update Step in Graph Neural Networks,"Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural
Network (GNN) framework, celebrate much success in the analysis of
graph-structured data. Concurrently, the sparsification of Neural Network
models attracts a great amount of academic and industrial interest. In this
paper, we conduct a structured study of the effect of sparsification on the
trainable part of MPNNs known as the Update step. To this end, we design a
series of models to successively sparsify the linear transform in the Update
step. Specifically, we propose the ExpanderGNN model with a tuneable
sparsification rate and the Activation-Only GNN, which has no linear transform
in the Update step. In agreement with a growing trend in the literature, the
sparsification paradigm is changed by initialising sparse neural network
architectures rather than expensively sparsifying already trained
architectures. Our novel benchmark models enable a better understanding of the
influence of the Update step on model performance and outperform existing
simplified benchmark models such as the Simple Graph Convolution. The
ExpanderGNNs, and in some cases the Activation-Only models, achieve performance
on par with their vanilla counterparts on several downstream tasks while
containing significantly fewer trainable parameters. In experiments with
matching parameter numbers, our benchmark models outperform the
state-of-the-art GNN models. Our code is publicly available at:
https://github.com/ChangminWu/ExpanderGNN."
[],Computing Graph Descriptors on Edge Streams,"Graph feature extraction is a fundamental task in graphs analytics. Using
feature vectors (graph descriptors) in tandem with data mining algorithms that
operate on Euclidean data, one can solve problems such as classification,
clustering, and anomaly detection on graph-structured data. This idea has
proved fruitful in the past, with spectral-based graph descriptors providing
state-of-the-art classification accuracy on benchmark datasets. However, these
algorithms do not scale to large graphs since: 1) they require storing the
entire graph in memory, and 2) the end-user has no control over the algorithm's
runtime. In this paper, we present single-pass streaming algorithms to
approximate structural features of graphs (counts of subgraphs of order $k \geq
4$). Operating on edge streams allows us to avoid keeping the entire graph in
memory, and controlling the sample size enables us to control the time taken by
the algorithm. We demonstrate the efficacy of our descriptors by analyzing the
approximation error, classification accuracy, and scalability to massive
graphs. Our experiments showcase the effect of the sample size on approximation
error and predictive accuracy. The proposed descriptors are applicable on
graphs with millions of edges within minutes and outperform the
state-of-the-art descriptors in classification accuracy."
[],Deep Dual Support Vector Data Description for Anomaly Detection on Attributed Networks,"Networks are ubiquitous in the real world such as social networks and
communication networks, and anomaly detection on networks aims at finding nodes
whose structural or attributed patterns deviate significantly from the majority
of reference nodes. However, most of the traditional anomaly detection methods
neglect the relation structure information among data points and therefore
cannot effectively generalize to the graph structure data. In this paper, we
propose an end-to-end model of Deep Dual Support Vector Data description based
Autoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which
considers both the structure and attribute for attributed networks.
Specifically, Dual-SVDAE consists of a structure autoencoder and an attribute
autoencoder to learn the latent representation of the node in the structure
space and attribute space respectively. Then, a dual-hypersphere learning
mechanism is imposed on them to learn two hyperspheres of normal nodes from the
structure and attribute perspectives respectively. Moreover, to achieve joint
learning between the structure and attribute of the network, we fuse the
structure embedding and attribute embedding as the final input of the feature
decoder to generate the node attribute. Finally, abnormal nodes can be detected
by measuring the distance of nodes to the learned center of each hypersphere in
the latent structure space and attribute space respectively. Extensive
experiments on the real-world attributed networks show that Dual-SVDAE
consistently outperforms the state-of-the-arts, which demonstrates the
effectiveness of the proposed method."
[],Learning Fair Graph Neural Networks with Limited and Private Sensitive Attribute Information,"Graph neural networks (GNNs) have shown great power in modeling graph
structured data. However, similar to other machine learning models, GNNs may
make biased predictions w.r.t protected sensitive attributes, e.g., skin color
and gender. This is because the training data often contains historical bias
towards sensitive attributes. In addition, we empirically show that the
discrimination in GNNs can be magnified by graph structures and the
message-passing mechanism of GNNs. As a result, the applications of GNNs in
high-stake domains such as crime rate prediction would be largely limited.
Though extensive studies of fair classification have been conducted on i.i.d
data, methods to address the problem of discrimination on non-i.i.d data are
rather limited. Generally, learning fair models require abundant sensitive
attributes to regularize the model. However, for many graphs such as social
networks, users are reluctant to share sensitive attributes. Thus, only limited
sensitive attributes are available for fair GNN training in practice. Moreover,
directly collecting and applying the sensitive attributes in fair model
training may cause privacy issues, because the sensitive information can be
leaked in data breach or attacks on the trained model. Therefore, we study a
novel and crucial problem of learning fair GNNs with limited and private
sensitive attribute information. In an attempt to address these problems,
FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high
accuracy by leveraging graph structures and limited sensitive information. We
further extend FairGNN to NT-FairGNN which can achieve both fairness and
privacy on sensitive attributes by using limited and private sensitive
attributes. Theoretical analysis and extensive experiments on real-world
datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving
fair and high-accurate classification."
[],Adversarial Stein Training for Graph Energy Models,"Learning distributions over graph-structured data is a challenging task with
many applications in biology and chemistry. In this work we use an energy-based
model (EBM) based on multi-channel graph neural networks (GNN) to learn
permutation invariant unnormalized density functions on graphs. Unlike standard
EBM training methods our approach is to learn the model via minimizing
adversarial stein discrepancy. Samples from the model can be obtained via
Langevin dynamics based MCMC. We find that this approach achieves competitive
results on graph generation compared to benchmark models."
[],Towards Self-Explainable Graph Neural Network,"Graph Neural Networks (GNNs), which generalize the deep neural networks to
graph-structured data, have achieved great success in modeling graphs. However,
as an extension of deep learning for graphs, GNNs lack explainability, which
largely limits their adoption in scenarios that demand the transparency of
models. Though many efforts are taken to improve the explainability of deep
learning, they mainly focus on i.i.d data, which cannot be directly applied to
explain the predictions of GNNs because GNNs utilize both node features and
graph topology to make predictions. There are only very few work on the
explainability of GNNs and they focus on post-hoc explanations. Since post-hoc
explanations are not directly obtained from the GNNs, they can be biased and
misrepresent the true explanations. Therefore, in this paper, we study a novel
problem of self-explainable GNNs which can simultaneously give predictions and
explanations. We propose a new framework which can find $K$-nearest labeled
nodes for each unlabeled node to give explainable node classification, where
nearest labeled nodes are found by interpretable similarity module in terms of
both node similarity and local structure similarity. Extensive experiments on
real-world and synthetic datasets demonstrate the effectiveness of the proposed
framework for explainable node classification."
[],Temporal Network Embedding via Tensor Factorization,"Representation learning on static graph-structured data has shown a
significant impact on many real-world applications. However, less attention has
been paid to the evolving nature of temporal networks, in which the edges are
often changing over time. The embeddings of such temporal networks should
encode both graph-structured information and the temporally evolving pattern.
Existing approaches in learning temporally evolving network representations
fail to capture the temporal interdependence. In this paper, we propose Toffee,
a novel approach for temporal network representation learning based on tensor
decomposition. Our method exploits the tensor-tensor product operator to encode
the cross-time information, so that the periodic changes in the evolving
networks can be captured. Experimental results demonstrate that Toffee
outperforms existing methods on multiple real-world temporal networks in
generating effective embeddings for the link prediction tasks."
[],LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis,"Graph structured data have enabled several successful applications such as
recommendation systems and traffic prediction, given the rich node features and
edges information. However, these high-dimensional features and high-order
adjacency information are usually heterogeneous and held by different data
holders in practice. Given such vertical data partition (e.g., one data holder
will only own either the node features or edge information), different data
holders have to develop efficient joint training protocols rather than directly
transfer data to each other due to privacy concerns. In this paper, we focus on
the edge privacy, and consider a training scenario where Bob with node features
will first send training node features to Alice who owns the adjacency
information. Alice will then train a graph neural network (GNN) with the joint
information and release an inference API. During inference, Bob is able to
provide test node features and query the API to obtain the predictions for test
nodes. Under this setting, we first propose a privacy attack LinkTeller via
influence analysis to infer the private edge information held by Alice via
designing adversarial queries for Bob. We then empirically show that LinkTeller
is able to recover a significant amount of private edges, outperforming
existing baselines. To further evaluate the privacy leakage, we adapt an
existing algorithm for differentially private graph convolutional network (DP
GCN) training and propose a new DP GCN mechanism LapGraph. We show that these
DP GCN mechanisms are not always resilient against LinkTeller empirically under
mild privacy guarantees ($\varepsilon>5$). Our studies will shed light on
future research towards designing more resilient privacy-preserving GCN models;
in the meantime, provide an in-depth understanding of the tradeoff between GCN
model utility and robustness against potential privacy attacks."
[],Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems,"Spatio-temporal forecasting is of great importance in a wide range of
dynamical systems applications from atmospheric science, to recent COVID-19
spread modeling. These applications rely on accurate predictions of
spatio-temporal structured data reflecting real-world phenomena. A stunning
characteristic is that the dynamical system is not only driven by some physics
laws but also impacted by the localized factor in spatial and temporal regions.
One of the major challenges is to infer the underlying causes, which generate
the perceived data stream and propagate the involved causal dynamics through
the distributed observing units. Another challenge is that the success of
machine learning based predictive models requires massive annotated data for
model training. However, the acquisition of high-quality annotated data is
objectively manual and tedious as it needs a considerable amount of human
intervention, making it infeasible in fields that require high levels of
expertise. To tackle these challenges, we advocate a spatio-temporal
physics-coupled neural networks (ST-PCNN) model to learn the underlying physics
of the dynamical system and further couple the learned physics to assist the
learning of the recurring dynamics. To deal with data-acquisition constraints,
an active learning mechanism with Kriging for actively acquiring the most
informative data is proposed for ST-PCNN training in a partially observable
environment. Our experiments on both synthetic and real-world datasets exhibit
that the proposed ST-PCNN with active learning converges to near optimal
accuracy with substantially fewer instances."
[],GIPA: General Information Propagation Algorithm for Graph Learning,"Graph neural networks (GNNs) have been popularly used in analyzing
graph-structured data, showing promising results in various applications such
as node classification, link prediction and network recommendation. In this
paper, we present a new graph attention neural network, namely GIPA, for
attributed graph data learning. GIPA consists of three key components:
attention, feature propagation and aggregation. Specifically, the attention
component introduces a new multi-layer perceptron based multi-head to generate
better non-linear feature mapping and representation than conventional
implementations such as dot-product. The propagation component considers not
only node features but also edge features, which differs from existing GNNs
that merely consider node features. The aggregation component uses a residual
connection to generate the final embedding. We evaluate the performance of GIPA
using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The
experimental results reveal that GIPA can beat the state-of-the-art models in
terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of
$0.8700\pm 0.0010$ and outperforms all the previous methods listed in the
ogbn-proteins leaderboard."
[],StrucTexT: Structured Text Understanding with Multi-Modal Transformers,"Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets."
[],LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices,"Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation."
[],On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization,"Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP."
[],Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks,"Deep learning's performance has been extensively recognized recently. Graph
neural networks (GNNs) are designed to deal with graph-structural data that
classical deep learning does not easily manage. Since most GNNs were created
using distinct theories, direct comparisons are impossible. Prior research has
primarily concentrated on categorizing existing models, with little attention
paid to their intrinsic connections. The purpose of this study is to establish
a unified framework that integrates GNNs based on spectral graph and
approximation theory. The framework incorporates a strong integration between
spatial- and spectral-based GNNs while tightly associating approaches that
exist within each respective domain."
[],DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs,"Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions of nodes and several billions
of edges. To tackle this challenge, we develop DistDGL, a system for training
GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the
Deep Graph Library (DGL), a popular GNN development framework. DistDGL
distributes the graph and its associated data (initial features and embeddings)
across the machines and uses this distribution to derive a computational
decomposition by following an owner-compute rule. DistDGL follows a synchronous
training approach and allows ego-networks forming the mini-batches to include
non-local nodes. To minimize the overheads associated with distributed
computations, DistDGL uses a high-quality and light-weight min-cut graph
partitioning algorithm along with multiple balancing constraints. This allows
it to reduce communication overheads and statically balance the computations.
It further reduces the communication by replicating halo nodes and by using
sparse embedding updates. The combination of these design choices allows
DistDGL to train high-quality models while achieving high parallel efficiency
and memory scalability. We demonstrate our optimizations on both inductive and
transductive GNN models. Our results show that DistDGL achieves linear speedup
without compromising model accuracy and requires only 13 seconds to complete a
training epoch for a graph with 100 million nodes and 3 billion edges on a
cluster with 16 machines. DistDGL is now publicly available as part of
DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed."
[],Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization,"Data selection methods, such as active learning and core-set selection, are
useful tools for improving the data efficiency of deep learning models on
large-scale datasets. However, recent deep learning models have moved forward
from independent and identically distributed data to graph-structured data,
such as social networks, e-commerce user-item graphs, and knowledge graphs.
This evolution has led to the emergence of Graph Neural Networks (GNNs) that go
beyond the models existing data selection methods are designed for. Therefore,
we present Grain, an efficient framework that opens up a new perspective
through connecting data selection in GNNs with social influence maximization.
By exploiting the common patterns of GNNs, Grain introduces a novel feature
propagation concept, a diversified influence maximization objective with novel
influence and diversity functions, and a greedy algorithm with an approximation
guarantee into a unified framework. Empirical studies on public datasets
demonstrate that Grain significantly improves both the performance and
efficiency of data selection (including active learning and core-set selection)
for GNNs. To the best of our knowledge, this is the first attempt to bridge two
largely parallel threads of research, data selection, and social influence
maximization, in the setting of GNNs, paving new ways for improving data
efficiency."
[],CKConv: Learning Feature Voxelization for Point Cloud Analysis,"Despite the remarkable success of deep learning, optimal convolution
operation on point cloud remains indefinite due to its irregular data
structure. In this paper, we present Cubic Kernel Convolution (CKConv) that
learns to voxelize the features of local points by exploiting both continuous
and discrete convolutions. Our continuous convolution uniquely employs a 3D
cubic form of kernel weight representation that splits a feature into voxels in
embedding space. By consecutively applying discrete 3D convolutions on the
voxelized features in a spatial manner, preceding continuous convolution is
forced to learn spatial feature mapping, i.e., feature voxelization. In this
way, geometric information can be detailed by encoding with subdivided
features, and our 3D convolutions on these fixed structured data do not suffer
from discretization artifacts thanks to voxelization in embedding space.
Furthermore, we propose a spatial attention module, Local Set Attention (LSA),
to provide comprehensive structure awareness within the local point set and
hence produce representative features. By learning feature voxelization with
LSA, CKConv can extract enriched features for effective point cloud analysis.
We show that CKConv has great applicability to point cloud processing tasks
including object classification, object part segmentation, and scene semantic
segmentation with state-of-the-art results."
[],Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks,"Graph neural networks (GNNs) have achieved remarkable success as a framework
for deep learning on graph-structured data. However, GNNs are fundamentally
limited by their tree-structured inductive bias: the WL-subtree kernel
formulation bounds the representational capacity of GNNs, and polynomial-time
GNNs are provably incapable of recognizing triangles in a graph. In this work,
we propose to augment the GNN message-passing operations with information
defined on ego graphs (i.e., the induced subgraph surrounding each node). We
term these approaches Ego-GNNs and show that Ego-GNNs are provably more
powerful than standard message-passing GNNs. In particular, we show that
Ego-GNNs are capable of recognizing closed triangles, which is essential given
the prominence of transitivity in real-world graphs. We also motivate our
approach from the perspective of graph signal processing as a form of multiplex
graph convolution. Experimental results on node classification using synthetic
and real data highlight the achievable performance gains using this approach."
[],Adaptive Transfer Learning on Graph Neural Networks,"Graph neural networks (GNNs) is widely used to learn a powerful
representation of graph-structured data. Recent work demonstrates that
transferring knowledge from self-supervised tasks to downstream tasks could
further improve graph representation. However, there is an inherent gap between
self-supervised tasks and downstream tasks in terms of optimization objective
and training data. Conventional pre-training methods may be not effective
enough on knowledge transfer since they do not make any adaptation for
downstream tasks. To solve such problems, we propose a new transfer learning
paradigm on GNNs which could effectively leverage self-supervised tasks as
auxiliary tasks to help the target task. Our methods would adaptively select
and combine different auxiliary tasks with the target task in the fine-tuning
stage. We design an adaptive auxiliary loss weighting model to learn the
weights of auxiliary tasks by quantifying the consistency between auxiliary
tasks and the target task. In addition, we learn the weighting model through
meta-learning. Our methods can be applied to various transfer learning
approaches, it performs well not only in multi-task learning but also in
pre-training and fine-tuning. Comprehensive experiments on multiple downstream
tasks demonstrate that the proposed methods can effectively combine auxiliary
tasks with the target task and significantly improve the performance compared
to state-of-the-art methods."
[],Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning,"Graph representation learning plays a vital role in processing
graph-structured data. However, prior arts on graph representation learning
heavily rely on labeling information. To overcome this problem, inspired by the
recent success of graph contrastive learning and Siamese networks in visual
representation learning, we propose a novel self-supervised approach in this
paper to learn node representations by enhancing Siamese self-distillation with
multi-scale contrastive learning. Specifically, we first generate two augmented
views from the input graph based on local and global perspectives. Then, we
employ two objectives called cross-view and cross-network contrastiveness to
maximize the agreement between node representations across different views and
networks. To demonstrate the effectiveness of our approach, we perform
empirical experiments on five real-world datasets. Our method not only achieves
new state-of-the-art results but also surpasses some semi-supervised
counterparts by large margins. Code is made available at
https://github.com/GRAND-Lab/MERIT"
[],Deep Neural Networks and End-to-End Learning for Audio Compression,"Recent achievements in end-to-end deep learning have encouraged the
exploration of tasks dealing with highly structured data with unified deep
network models. Having such models for compressing audio signals has been
challenging since it requires discrete representations that are not easy to
train with end-to-end backpropagation. In this paper, we present an end-to-end
deep learning approach that combines recurrent neural networks (RNNs) within
the training strategy of variational autoencoders (VAEs) with a binary
representation of the latent space. We apply a reparametrization trick for the
Bernoulli distribution for the discrete representations, which allows smooth
backpropagation. In addition, our approach allows the separation of the encoder
and decoder, which is necessary for compression tasks. To our best knowledge,
this is the first end-to-end learning for a single audio compression model with
RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54."
[],Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering,"Graph convolutional networks are becoming indispensable for deep learning
from graph-structured data. Most of the existing graph convolutional networks
share two big shortcomings. First, they are essentially low-pass filters, thus
the potentially useful middle and high frequency band of graph signals are
ignored. Second, the bandwidth of existing graph convolutional filters is
fixed. Parameters of a graph convolutional filter only transform the graph
inputs without changing the curvature of a graph convolutional filter function.
In reality, we are uncertain about whether we should retain or cut off the
frequency at a certain point unless we have expert domain knowledge. In this
paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture
the full spectrum of graph signals and automatically update the bandwidth of
graph convolutional filters. While it is based on graph spectral theory, our
AutoGCN is also localized in space and has a spatial form. Experimental results
show that AutoGCN achieves significant improvement over baseline methods which
only work as low-pass filters."
[],Inter-domain Multi-relational Link Prediction,"Multi-relational graph is a ubiquitous and important data structure, allowing
flexible representation of multiple types of interactions and relations between
entities. Similar to other graph-structured data, link prediction is one of the
most important tasks on multi-relational graphs and is often used for knowledge
completion. When related graphs coexist, it is of great benefit to build a
larger graph via integrating the smaller ones. The integration requires
predicting hidden relational connections between entities belonged to different
graphs (inter-domain link prediction). However, this poses a real challenge to
existing methods that are exclusively designed for link prediction between
entities of the same graph only (intra-domain link prediction). In this study,
we propose a new approach to tackle the inter-domain link prediction problem by
softly aligning the entity distributions between different domains with optimal
transport and maximum mean discrepancy regularizers. Experiments on real-world
datasets show that optimal transport regularizer is beneficial and considerably
improves the performance of baseline methods."
[],Multi-Level Graph Contrastive Learning,"Graph representation learning has attracted a surge of interest recently,
whose target at learning discriminant embedding for each node in the graph.
Most of these representation methods focus on supervised learning and heavily
depend on label information. However, annotating graphs are expensive to obtain
in the real world, especially in specialized domains (i.e. biology), as it
needs the annotator to have the domain knowledge to label the graph. To
approach this problem, self-supervised learning provides a feasible solution
for graph representation learning. In this paper, we propose a Multi-Level
Graph Contrastive Learning (MLGCL) framework for learning robust representation
of graph data by contrasting space views of graphs. Specifically, we introduce
a novel contrastive view - topological and feature space views. The original
graph is first-order approximation structure and contains uncertainty or error,
while the $k$NN graph generated by encoding features preserves high-order
proximity. Thus $k$NN graph generated by encoding features not only provide a
complementary view, but is more suitable to GNN encoder to extract discriminant
representation. Furthermore, we develop a multi-level contrastive mode to
preserve the local similarity and semantic similarity of graph-structured data
simultaneously. Extensive experiments indicate MLGCL achieves promising results
compared with the existing state-of-the-art graph representation learning
methods on seven datasets."
[],ARM-Net: Adaptive Relation Modeling Network for Structured Data,"Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
  In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making."
[],Learning deep autoregressive models for hierarchical data,"We propose a model for hierarchical structured data as an extension to the
stochastic temporal convolutional network. The proposed model combines an
autoregressive model with a hierarchical variational autoencoder and
downsampling to achieve superior computational complexity. We evaluate the
proposed model on two different types of sequential data: speech and
handwritten text. The results are promising with the proposed model achieving
state-of-the-art performance."
[],Edge Representation Learning with Hypergraphs,"Graph neural networks have recently achieved remarkable success in
representing graph-structured data, with rapid progress in both the node
embedding and graph pooling methods. Yet, they mostly focus on capturing
information from the nodes considering their connectivity, and not much work
has been done in representing the edges, which are essential components of a
graph. However, for tasks such as graph reconstruction and generation, as well
as graph classification tasks for which the edges are important for
discrimination, accurately representing edges of a given graph is crucial to
the success of the graph representation learning. To this end, we propose a
novel edge representation learning framework based on Dual Hypergraph
Transformation (DHT), which transforms the edges of a graph into the nodes of a
hypergraph. This dual hypergraph construction allows us to apply message
passing techniques for node representations to edges. After obtaining edge
representations from the hypergraphs, we then cluster or drop edges to obtain
holistic graph-level edge representations. We validate our edge representation
learning method with hypergraphs on diverse graph datasets for graph
representation and generation performance, on which our method largely
outperforms existing graph representation learning methods. Moreover, our edge
representation learning and pooling method also largely outperforms
state-of-the-art graph pooling methods on graph classification, not only
because of its accurate edge representation learning, but also due to its
lossless compression of the nodes and removal of irrelevant edges for effective
message passing."
[],Domain adaptation for person re-identification on new unlabeled data using AlignedReID++,"In the world where big data reigns and there is plenty of hardware prepared
to gather a huge amount of non structured data, data acquisition is no longer a
problem. Surveillance cameras are ubiquitous and they capture huge numbers of
people walking across different scenes. However, extracting value from this
data is challenging, specially for tasks that involve human images, such as
face recognition and person re-identification. Annotation of this kind of data
is a challenging and expensive task. In this work we propose a domain
adaptation workflow to allow CNNs that were trained in one domain to be applied
to another domain without the need for new annotation of the target data. Our
method uses AlignedReID++ as the baseline, trained using a Triplet loss with
batch hard. Domain adaptation is done by using pseudo-labels generated using an
unsupervised learning strategy. Our results show that domain adaptation
techniques really improve the performance of the CNN when applied in the target
domain."
[],GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily,"In representation learning on the graph-structured data, under heterophily
(or low homophily), many popular GNNs may fail to capture long-range
dependencies, which leads to their performance degradation. To solve the
above-mentioned issue, we propose a graph convolutional networks with structure
learning (GCN-SL), and furthermore, the proposed approach can be applied to
node classification. The proposed GCN-SL contains two improvements:
corresponding to node features and edges, respectively. In the aspect of node
features, we propose an efficient-spectral-clustering (ESC) and an ESC with
anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations
from all similar nodes. In the aspect of edges, we build a re-connected
adjacency matrix by using a special data preprocessing technique and similarity
learning, and the re-connected adjacency matrix can be optimized directly along
with GCN-SL parameters. Considering that the original adjacency matrix may
provide misleading information for aggregation in GCN, especially the graphs
being with a low level of homophily. The proposed GCN-SL can aggregate feature
representations from nearby nodes via re-connected adjacency matrix and is
applied to graphs with various levels of homophily. Experimental results on a
wide range of benchmark datasets illustrate that the proposed GCN-SL
outperforms the stateof-the-art GNN counterparts."
[],Graph Contrastive Learning Automated,"Self-supervised learning on graph-structured data has drawn recent interest
for learning generalizable, transferable and robust representations from
unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged
with promising representation learning performance. Unfortunately, unlike its
counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data
augmentations, which have to be manually picked per dataset, by either rules of
thumb or trial-and-errors, owing to the diverse nature of graph data. That
significantly limits the more general applicability of GraphCL. Aiming to fill
in this crucial gap, this paper proposes a unified bi-level optimization
framework to automatically, adaptively and dynamically select data
augmentations when performing GraphCL on specific graph data. The general
framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as
min-max optimization. The selections of augmentations made by JOAO are shown to
be in general aligned with previous ""best practices"" observed from handcrafted
tuning: yet now being automated, more flexible and versatile. Moreover, we
propose a new augmentation-aware projection head mechanism, which will route
output features through different projection heads corresponding to different
augmentations chosen at each training step. Extensive experiments demonstrate
that JOAO performs on par with or sometimes better than the state-of-the-art
competitors including GraphCL, on multiple graph datasets of various scales and
types, yet without resorting to any laborious dataset-specific tuning on
augmentation selection. We release the code at
https://github.com/Shen-Lab/GraphCL_Automated."
[],CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals,"Data augmentation is a key element of deep learning pipelines, as it informs
the network during training about transformations of the input data that keep
the label unchanged. Manually finding adequate augmentation methods and
parameters for a given pipeline is however rapidly cumbersome. In particular,
while intuition can guide this decision for images, the design and choice of
augmentation policies remains unclear for more complex types of data, such as
neuroscience signals. Moreover, label independent strategies might not be
suitable for such structured data and class-dependent augmentations might be
necessary. This idea has been surprisingly unexplored in the literature, while
it is quite intuitive: changing the color of a car image does not change the
object class to be predicted, but doing the same to the picture of an orange
does. This paper aims to increase the generalization power added through
class-wise data augmentation. Yet, as seeking transformations depending on the
class largely increases the complexity of the task, using gradient-free
optimization techniques as done by most existing automatic approaches becomes
intractable for real-world datasets. For this reason we propose to use
differentiable data augmentation amenable to gradient-based learning. EEG
signals are a perfect example of data for which good augmentation policies are
mostly unknown. In this work, we demonstrate the relevance of our approach on
the clinically relevant sleep staging classification task, for which we also
propose differentiable transformations."
[],Temporal Graph Signal Decomposition,"Temporal graph signals are multivariate time series with individual
components associated with nodes of a fixed graph structure. Data of this kind
arises in many domains including activity of social network users, sensor
network readings over time, and time course gene expression within the
interaction network of a model organism. Traditional matrix decomposition
methods applied to such data fall short of exploiting structural regularities
encoded in the underlying graph and also in the temporal patterns of the
signal. How can we take into account such structure to obtain a succinct and
interpretable representation of temporal graph signals?
  We propose a general, dictionary-based framework for temporal graph signal
decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of
the data via a combination of graph and time dictionaries. We propose a highly
scalable decomposition algorithm for both complete and incomplete data, and
demonstrate its advantage for matrix decomposition, imputation of missing
values, temporal interpolation, clustering, period estimation, and rank
estimation in synthetic and real-world data ranging from traffic patterns to
social media activity. Our framework achieves 28% reduction in RMSE compared to
baselines for temporal interpolation when as many as 75% of the observations
are missing. It scales best among baselines taking under 20 seconds on 3.5
million data points and produces the most parsimonious models. To the best of
our knowledge, TGSD is the first framework to jointly model graph signals by
temporal and graph dictionaries."
[],You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks,"Hypergraphs are used to model higher-order interactions amongst agents and
there exist many practically relevant instances of hypergraph datasets. To
enable efficient processing of hypergraph-structured data, several hypergraph
neural network platforms have been proposed for learning hypergraph properties
and structure, with a special focus on node classification. However, almost all
existing methods use heuristic propagation rules and offer suboptimal
performance on many datasets. We propose AllSet, a new hypergraph neural
network paradigm that represents a highly general framework for (hyper)graph
neural networks and for the first time implements hypergraph neural network
layers as compositions of two multiset functions that can be efficiently
learned for each task and each dataset. Furthermore, AllSet draws on new
connections between hypergraph neural networks and recent advances in deep
learning of multiset functions. In particular, the proposed architecture
utilizes Deep Sets and Set Transformer architectures that allow for significant
modeling flexibility and offer high expressive power. To evaluate the
performance of AllSet, we conduct the most extensive experiments to date
involving ten known benchmarking datasets and three newly curated datasets that
represent significant challenges for hypergraph node classification. The
results demonstrate that AllSet has the unique ability to consistently either
match or outperform all other hypergraph neural networks across the tested
datasets. Our implementation and dataset will be released upon acceptance."
[],Graph Attention Networks with LSTM-based Path Reweighting,"Graph Neural Networks (GNNs) have been extensively used for mining
graph-structured data with impressive performance. However, traditional GNNs
suffer from over-smoothing, non-robustness and over-fitting problems. To solve
these weaknesses, we design a novel GNN solution, namely Graph Attention
Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically
aggregate multi-hop information, highlight important paths and filter out
noises. In addition, we utilize random path sampling in PR-GAT for data
augmentation. The augmented data is used for predicting the distribution of
corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the
issues of over-smoothing, non-robustness and overfitting. We achieve
state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for
other 2 datasets. The average accuracy of 7 datasets have been improved by
0.5\% than the best SOTA from literature."
[],How Framelets Enhance Graph Neural Networks,"This paper presents a new approach for assembling graph neural networks based
on framelet transforms. The latter provides a multi-scale representation for
graph-structured data. We decompose an input graph into low-pass and high-pass
frequencies coefficients for network training, which then defines a
framelet-based graph convolution. The framelet decomposition naturally induces
a graph pooling strategy by aggregating the graph feature into low-pass and
high-pass spectra, which considers both the feature values and geometry of the
graph data and conserves the total information. The graph neural networks with
the proposed framelet convolution and pooling achieve state-of-the-art
performance in many node and graph prediction tasks. Moreover, we propose
shrinkage as a new activation for the framelet convolution, which thresholds
high-frequency information at different scales. Compared to ReLU, shrinkage
activation improves model performance on denoising and signal compression:
noises in both node and structure can be significantly reduced by accurately
cutting off the high-pass coefficients from framelet decomposition, and the
signal can be compressed to less than half its original size with
well-preserved prediction performance."
[],Do Transformers Really Perform Bad for Graph Representation?,"The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer."
[],Comparison of Outlier Detection Techniques for Structured Data,"An outlier is an observation or a data point that is far from rest of the
data points in a given dataset or we can be said that an outlier is away from
the center of mass of observations. Presence of outliers can skew statistical
measures and data distributions which can lead to misleading representation of
the underlying data and relationships. It is seen that the removal of outliers
from the training dataset before modeling can give better predictions. With the
advancement of machine learning, the outlier detection models are also
advancing at a good pace. The goal of this work is to highlight and compare
some of the existing outlier detection techniques for the data scientists to
use that information for outlier algorithm selection while building a machine
learning model."
[],Graph Domain Adaptation: A Generative View,"Recent years have witnessed tremendous interest in deep learning on
graph-structured data. Due to the high cost of collecting labeled
graph-structured data, domain adaptation is important to supervised graph
learning tasks with limited samples. However, current graph domain adaptation
methods are generally adopted from traditional domain adaptation tasks, and the
properties of graph-structured data are not well utilized. For example, the
observed social networks on different platforms are controlled not only by the
different crowd or communities but also by the domain-specific policies and the
background noise. Based on these properties in graph-structured data, we first
assume that the graph-structured data generation process is controlled by three
independent types of latent variables, i.e., the semantic latent variables, the
domain latent variables, and the random latent variables. Based on this
assumption, we propose a disentanglement-based unsupervised domain adaptation
method for the graph-structured data, which applies variational graph
auto-encoders to recover these latent variables and disentangles them via three
supervised learning modules. Extensive experimental results on two real-world
datasets in the graph classification task reveal that our method not only
significantly outperforms the traditional domain adaptation methods and the
disentangled-based domain adaptation methods but also outperforms the
state-of-the-art graph domain adaptation algorithms."
[],Link Prediction with Persistent Homology: An Interactive View,"Link prediction is an important learning task for graph-structured data. In
this paper, we propose a novel topological approach to characterize
interactions between two nodes. Our topological feature, based on the extended
persistent homology, encodes rich structural information regarding the
multi-hop paths connecting nodes. Based on this feature, we propose a graph
neural network method that outperforms state-of-the-arts on different
benchmarks. As another contribution, we propose a novel algorithm to more
efficiently compute the extended persistence diagrams for graphs. This
algorithm can be generally applied to accelerate many other topological methods
for graph learning tasks."
[],A Review of Graph Neural Networks and Their Applications in Power Systems,"Deep neural networks have revolutionized many machine learning tasks in power
systems, ranging from pattern recognition to signal processing. The data in
these tasks is typically represented in Euclidean domains. Nevertheless, there
is an increasing number of applications in power systems, where data are
collected from non-Euclidean domains and represented as graph-structured data
with high dimensional features and interdependency among nodes. The complexity
of graph-structured data has brought significant challenges to the existing
deep neural networks defined in Euclidean domains. Recently, many publications
generalizing deep neural networks for graph-structured data in power systems
have emerged. In this paper, a comprehensive overview of graph neural networks
(GNNs) in power systems is proposed. Specifically, several classical paradigms
of GNNs structures (e.g., graph convolutional networks) are summarized, and key
applications in power systems, such as fault scenario application, time series
prediction, power flow calculation, and data generation are reviewed in detail.
Furthermore, main issues and some research trends about the applications of
GNNs in power systems are discussed."
[],Learnable Hypergraph Laplacian for Hypergraph Learning,"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability."
[],What Can Knowledge Bring to Machine Learning? -- A Survey of Low-shot Learning for Structured Data,"Supervised machine learning has several drawbacks that make it difficult to
use in many situations. Drawbacks include: heavy reliance on massive training
data, limited generalizability and poor expressiveness of high-level semantics.
Low-shot Learning attempts to address these drawbacks. Low-shot learning allows
the model to obtain good predictive power with very little or no training data,
where structured knowledge plays a key role as a high-level semantic
representation of human. This article will review the fundamental factors of
low-shot learning technologies, with a focus on the operation of structured
knowledge under different low-shot conditions. We also introduce other
techniques relevant to low-shot learning. Finally, we point out the limitations
of low-shot learning, the prospects and gaps of industrial applications, and
future research directions."
[],Graph Transformer Networks: Learning Meta-path Graphs to Improve GNNs,"Graph Neural Networks (GNNs) have been widely applied to various fields due
to their powerful representations of graph-structured data. Despite the success
of GNNs, most existing GNNs are designed to learn node representations on the
fixed and homogeneous graphs. The limitations especially become problematic
when learning representations on a misspecified graph or a heterogeneous graph
that consists of various types of nodes and edges. To address this limitations,
we propose Graph Transformer Networks (GTNs) that are capable of generating new
graph structures, which preclude noisy connections and include useful
connections (e.g., meta-paths) for tasks, while learning effective node
representations on the new graphs in an end-to-end fashion. We further propose
enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that
improve scalability of graph transformations. Compared to GTNs, FastGTNs are
230x faster and use 100x less memory while allowing the identical graph
transformations as GTNs. In addition, we extend graph transformations to the
semantic proximity of nodes allowing non-local operations beyond meta-paths.
Extensive experiments on both homogeneous graphs and heterogeneous graphs show
that GTNs and FastGTNs with non-local operations achieve the state-of-the-art
performance for node classification tasks. The code is available:
https://github.com/seongjunyun/Graph_Transformer_Networks"
[],Learning to Pool in Graph Neural Networks for Extrapolation,"Graph neural networks (GNNs) are one of the most popular approaches to using
deep learning on graph-structured data, and they have shown state-of-the-art
performances on a variety of tasks. However, according to a recent study, a
careful choice of pooling functions, which are used for the aggregation or
readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without
the ideal combination of pooling functions, which varies across tasks, GNNs
completely fail to generalize to out-of-distribution data, while the number of
possible combinations grows exponentially with the number of layers. In this
paper, we present GNP, a $L^p$ norm-like pooling function that is trainable
end-to-end for any given task. Notably, GNP generalizes most of the widely-used
pooling functions. We verify experimentally that simply replacing all pooling
functions with GNP enables GNNs to extrapolate well on many node-level,
graph-level, and set-related tasks; and GNP sometimes performs even better than
optimal combinations of existing pooling functions."
[],Learnable Hypergraph Laplacian for Hypergraph Learning,"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their
potential in modeling high-order relations preserved in graph structured data.
However, most existing convolution filters are localized and determined by the
pre-defined initial hypergraph topology, neglecting to explore implicit and
long-ange relations in real-world data. In this paper, we propose the first
learning-based method tailored for constructing adaptive hypergraph structure,
termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic
plug-in-play module for improving the representational power of HGCNNs.
Specifically, HERALD adaptively optimizes the adjacency relationship between
hypernodes and hyperedges in an end-to-end manner and thus the task-aware
hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism
to capture the non-local paired-nodes relation. Extensive experiments on
various popular hypergraph datasets for node classification and graph
classification tasks demonstrate that our approach obtains consistent and
considerable performance enhancement, proving its effectiveness and
generalization ability."
[],Learning subtree pattern importance for Weisfeiler-Lehmanbased graph kernels,"Graph is an usual representation of relational data, which are ubiquitous in
manydomains such as molecules, biological and social networks. A popular
approach to learningwith graph structured data is to make use of graph kernels,
which measure the similaritybetween graphs and are plugged into a kernel
machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph
kernels, which employ WL labeling scheme to extract subtree patterns and
perform node embedding, are demonstrated to achieve great performance while
being efficiently computable. However, one of the main drawbacks of ageneral
kernel is the decoupling of kernel construction and learning process. For
moleculargraphs, usual kernels such as WL subtree, based on substructures of
the molecules, consider all available substructures having the same importance,
which might not be suitable inpractice. In this paper, we propose a method to
learn the weights of subtree patterns in the framework of WWL kernels, the
state of the art method for graph classification task [14]. To overcome the
computational issue on large scale data sets, we present an efficient learning
algorithm and also derive a generalization gap bound to show its convergence.
Finally, through experiments on synthetic and real-world data sets, we
demonstrate the effectiveness of our proposed method for learning the weights
of subtree patterns."
[],Graph-MLP: Node Classification without Message Passing in Graph,"Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing
with non-Euclidean structural data. Both spatial-based and spectral-based GNNs
are relying on adjacency matrix to guide message passing among neighbors during
feature aggregation. Recent works have mainly focused on powerful message
passing modules, however, in this paper, we show that none of the message
passing modules is necessary. Instead, we propose a pure
multilayer-perceptron-based framework, Graph-MLP with the supervision signal
leveraging graph structure, which is sufficient for learning discriminative
node representation. In model-level, Graph-MLP only includes multi-layer
perceptrons, activation function, and layer normalization. In the loss level,
we design a neighboring contrastive (NContrast) loss to bridge the gap between
GNNs and MLPs by utilizing the adjacency information implicitly. This design
allows our model to be lighter and more robust when facing large-scale graph
data and corrupted adjacency information. Extensive experiments prove that even
without adjacency information in testing phase, our framework can still reach
comparable and even superior performance against the state-of-the-art models in
the graph node classification task."
[],Generative Causal Explanations for Graph Neural Networks,"This paper presents Gem, a model-agnostic approach for providing
interpretable explanations for any GNNs on various graph learning tasks.
Specifically, we formulate the problem of providing explanations for the
decisions of GNNs as a causal learning task. Then we train a causal explanation
model equipped with a loss function based on Granger causality. Different from
existing explainers for GNNs, Gem explains GNNs on graph-structured data from a
causal perspective. It has better generalization ability as it has no
requirements on the internal structure of the GNNs or prior knowledge on the
graph learning tasks. In addition, Gem, once trained, can be used to explain
the target GNN very quickly. Our theoretical analysis shows that several recent
explainers fall into a unified framework of additive feature attribution
methods. Experimental results on synthetic and real-world datasets show that
Gem achieves a relative increase of the explanation accuracy by up to $30\%$
and speeds up the explanation process by up to $110\times$ as compared to its
state-of-the-art alternatives."
[],Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space,"Learning the representation of data with hierarchical structures in the
hyperbolic space attracts increasing attention in recent years. Due to the
constant negative curvature, the hyperbolic space resembles tree metrics and
captures the tree-like properties naturally, which enables the hyperbolic
embeddings to improve over traditional Euclidean models. However, many
real-world hierarchically structured data such as taxonomies and multitree
networks have varying local structures and they are not trees, thus they do not
ubiquitously match the constant curvature property of the hyperbolic space. To
address this limitation of hyperbolic embeddings, we explore the complex
hyperbolic space, which has the variable negative curvature, for representation
learning. Specifically, we propose to learn the embeddings of hierarchically
structured data in the unit ball model of the complex hyperbolic space. The
unit ball model based embeddings have a more powerful representation capacity
to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the
hyperbolic embedding models significantly."
[],ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks,"Imbalanced classification on graphs is ubiquitous yet challenging in many
real-world applications, such as fraudulent node detection. Recently, graph
neural networks (GNNs) have shown promising performance on many network
analysis tasks. However, most existing GNNs have almost exclusively focused on
the balanced networks, and would get unappealing performance on the imbalanced
networks. To bridge this gap, in this paper, we present a generative
adversarial graph network model, called ImGAGN to address the imbalanced
classification problem on graphs. It introduces a novel generator for graph
structure data, named GraphGenerator, which can simulate both the minority
class nodes' attribute distribution and network topological structure
distribution by generating a set of synthetic minority nodes such that the
number of nodes in different classes can be balanced. Then a graph
convolutional network (GCN) discriminator is trained to discriminate between
real nodes and fake (i.e., generated) nodes, and also between minority nodes
and majority nodes on the synthetic balanced network. To validate the
effectiveness of the proposed method, extensive experiments are conducted on
four real-world imbalanced network datasets. Experimental results demonstrate
that the proposed method ImGAGN outperforms state-of-the-art algorithms for
semi-supervised imbalanced node classification task."
[],SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks,"Graph Neural Networks (GNNs) are the first choice methods for graph machine
learning problems thanks to their ability to learn state-of-the-art level
representations from graph-structured data. However, centralizing a massive
amount of real-world graph data for GNN training is prohibitive due to
user-side privacy concerns, regulation restrictions, and commercial
competition. Federated Learning is the de-facto standard for collaborative
training of machine learning models over many distributed edge devices without
the need for centralization. Nevertheless, training graph neural networks in a
federated setting is vaguely defined and brings statistical and systems
challenges. This work proposes SpreadGNN, a novel multi-task federated training
framework capable of operating in the presence of partial labels and absence of
a central server for the first time in the literature. SpreadGNN extends
federated multi-task learning to realistic serverless settings for GNNs, and
utilizes a novel optimization algorithm with a convergence guarantee,
Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized
multi-task learning problems. We empirically demonstrate the efficacy of our
framework on a variety of non-I.I.D. distributed graph-level molecular property
prediction datasets with partial labels. Our results show that SpreadGNN
outperforms GNN models trained over a central server-dependent federated
learning system, even in constrained topologies. The source code is publicly
available at https://github.com/FedML-AI/SpreadGNN"
[],Node-Variant Graph Filters in Graph Neural Networks,"Graph neural networks (GNNs) have been successfully employed in a myriad of
applications involving graph-structured data. Theoretical findings establish
that GNNs use nonlinear activation functions to create low-eigenvalue frequency
content that can be processed in a stable manner by subsequent graph
convolutional filters. However, the exact shape of the frequency content
created by nonlinear functions is not known, and thus, it cannot be learned nor
controlled. In this work, node-variant graph filters (NVGFs) are shown to be
capable of creating frequency content and are thus used in lieu of nonlinear
activation functions. This results in a novel GNN architecture that, although
linear, is capable of creating frequency content as well. Furthermore, this new
frequency content can be either designed or learned from data. In this way, the
role of frequency creation is separated from the nonlinear nature of
traditional GNNs. Extensive simulations are carried out to differentiate the
contributions of frequency creation from those of the nonlinearity."
[],Hashing-Accelerated Graph Neural Networks for Link Prediction,"Networks are ubiquitous in the real world. Link prediction, as one of the key
problems for network-structured data, aims to predict whether there exists a
link between two nodes. The traditional approaches are based on the explicit
similarity computation between the compact node representation by embedding
each node into a low-dimensional space. In order to efficiently handle the
intensive similarity computation in link prediction, the hashing technique has
been successfully used to produce the node representation in the Hamming space.
However, the hashing-based link prediction algorithms face accuracy loss from
the randomized hashing techniques or inefficiency from the learning to hash
techniques in the embedding process. Currently, the Graph Neural Network (GNN)
framework has been widely applied to the graph-related tasks in an end-to-end
manner, but it commonly requires substantial computational resources and memory
costs due to massive parameter learning, which makes the GNN-based algorithms
impractical without the help of a powerful workhorse. In this paper, we propose
a simple and effective model called #GNN, which balances the trade-off between
accuracy and efficiency. #GNN is able to efficiently acquire node
representation in the Hamming space for link prediction by exploiting the
randomized hashing technique to implement message passing and capture
high-order proximity in the GNN framework. Furthermore, we characterize the
discriminative power of #GNN in probability. The extensive experimental results
demonstrate that the proposed #GNN algorithm achieves accuracy comparable to
the learning-based algorithms and outperforms the randomized algorithm, while
running significantly faster than the learning-based algorithms. Also, the
proposed algorithm shows excellent scalability on a large-scale network with
the limited resources."
[],Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data,"Estimating the amount of electricity that can be produced by rooftop
photovoltaic systems is a time-consuming process that requires on-site
measurements, a difficult task to achieve on a large scale. In this paper, we
present an approach to estimate the solar potential of rooftops based on their
location and architectural characteristics, as well as the amount of solar
radiation they receive annually. Our technique uses computer vision to achieve
semantic segmentation of roof sections and roof objects on the one hand, and a
machine learning model based on structured building features to predict roof
pitch on the other hand. We then compute the azimuth and maximum number of
solar panels that can be installed on a rooftop with geometric approaches.
Finally, we compute precise shading masks and combine them with solar
irradiation data that enables us to estimate the yearly solar potential of a
rooftop."
[],Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention,"Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at https://github.com/egyptdj/stagin"
[],An Explainable Probabilistic Classifier for Categorical Data Inspired to Quantum Physics,"This paper presents Sparse Tensor Classifier (STC), a supervised
classification algorithm for categorical data inspired by the notion of
superposition of states in quantum physics. By regarding an observation as a
superposition of features, we introduce the concept of wave-particle duality in
machine learning and propose a generalized framework that unifies the classical
and the quantum probability. We show that STC possesses a wide range of
desirable properties not available in most other machine learning methods but
it is at the same time exceptionally easy to comprehend and use. Empirical
evaluation of STC on structured data and text classification demonstrates that
our methodology achieves state-of-the-art performances compared to both
standard classifiers and deep learning, at the additional benefit of requiring
minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides
a native explanation of its predictions both for single instances and for each
target label globally."
[],Dynamic Filters in Graph Convolutional Neural Networks,"Over the last few years, we have seen increasing data generated from
non-Euclidean domains, which are usually represented as graphs with complex
relationships, and Graph Neural Networks (GNN) have gained a high interest
because of their potential in processing graph-structured data. In particular,
there is a strong interest in exploring the possibilities in performing
convolution on graphs using an extension of the GNN architecture, generally
referred to as Graph Convolutional Neural Networks (GCNN). Convolution on
graphs has been achieved mainly in two forms: spectral and spatial
convolutions. Due to the higher flexibility in exploring and exploiting the
graph structure of data, recently, there is an increasing interest in
investigating the possibilities that the spatial approach can offer. The idea
of finding a way to adapt the network behaviour to the inputs they process to
maximize the total performances has aroused much interest in the neural
networks literature over the years. This paper presents a novel method to adapt
the behaviour of a GCNN to the input proposing two ways to perform spatial
convolution on graphs using input-based filters which are dynamically
generated. Our model also investigates the problem of discovering and refining
relations among nodes. The experimental assessment confirms the capabilities of
the proposed approach, which achieves satisfying results using simple
architectures with a low number of filters."
[],Graph Convolutional Networks in Feature Space for Image Deblurring and Super-resolution,"Graph convolutional networks (GCNs) have achieved great success in dealing
with data of non-Euclidean structures. Their success directly attributes to
fitting graph structures effectively to data such as in social media and
knowledge databases. For image processing applications, the use of graph
structures and GCNs have not been fully explored. In this paper, we propose a
novel encoder-decoder network with added graph convolutions by converting
feature maps to vertexes of a pre-generated graph to synthetically construct
graph-structured data. By doing this, we inexplicitly apply graph Laplacian
regularization to the feature maps, making them more structured. The
experiments show that it significantly boosts performance for image restoration
tasks, including deblurring and super-resolution. We believe it opens up
opportunities for GCN-based approaches in more applications."
[],rx-anon -- A Novel Approach on the De-Identification of Heterogeneous Data based on a Modified Mondrian Algorithm,"Traditional approaches for data anonymization consider relational data and
textual data independently. We propose rx-anon, an anonymization approach for
heterogeneous semi-structured documents composed of relational and textual
attributes. We map sensitive terms extracted from the text to the structured
data. This allows us to use concepts like k-anonymity to generate a joined,
privacy-preserved version of the heterogeneous data input. We introduce the
concept of redundant sensitive information to consistently anonymize the
heterogeneous data. To control the influence of anonymization over unstructured
textual data versus structured data attributes, we introduce a modified,
parameterized Mondrian algorithm. The parameter $\lambda$ allows to give
different weight on the relational and textual attributes during the
anonymization process. We evaluate our approach with two real-world datasets
using a Normalized Certainty Penalty score, adapted to the problem of jointly
anonymizing relational and textual data. The results show that our approach is
capable of reducing information loss by using the tuning parameter to control
the Mondrian partitioning while guaranteeing k-anonymity for relational
attributes as well as for sensitive terms. As rx-anon is a framework approach,
it can be reused and extended by other anonymization algorithms, privacy
models, and textual similarity metrics."
[],VoxelContext-Net: An Octree based Framework for Point Cloud Compression,"In this paper, we propose a two-stage deep learning framework called
VoxelContext-Net for both static and dynamic point cloud compression. Taking
advantages of both octree based methods and voxel based schemes, our approach
employs the voxel context to compress the octree structured data. Specifically,
we first extract the local voxel representation that encodes the spatial
neighbouring context information for each node in the constructed octree. Then,
in the entropy coding stage, we propose a voxel context based deep entropy
model to compress the symbols of non-leaf nodes in a lossless way. Furthermore,
for dynamic point cloud compression, we additionally introduce the local voxel
representations from the temporal neighbouring point clouds to exploit temporal
dependency. More importantly, to alleviate the distortion from the octree
construction procedure, we propose a voxel context based 3D coordinate
refinement method to produce more accurate reconstructed point cloud at the
decoder side, which is applicable to both static and dynamic point cloud
compression. The comprehensive experiments on both static and dynamic point
cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate
the effectiveness of our newly proposed method VoxelContext-Net for 3D point
cloud geometry compression."
[],VersaGNN: a Versatile accelerator for Graph neural networks,"\textit{Graph Neural Network} (GNN) is a promising approach for analyzing
graph-structured data that tactfully captures their dependency information via
node-level message passing. It has achieved state-of-the-art performances in
many tasks, such as node classification, graph matching, clustering, and graph
generation. As GNNs operate on non-Euclidean data, their irregular data access
patterns cause considerable computational costs and overhead on conventional
architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid
computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase
performs vector additions where vectors are fetched with irregular strides. The
\textit{Transformation} (or \textit{Node Embedding}) phase can be either dense
or sparse-dense matrix multiplication. In this work, We propose
\textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware
accelerator that unifies dense and sparse matrix multiplication. By applying
this single optimized systolic array to both aggregation and transformation
phases, we have significantly reduced chip sizes and energy consumption. We
then divide the computing engine into blocked systolic arrays to support the
\textit{Strassen}'s algorithm for dense matrix multiplication, dramatically
scaling down the number of multiplications and enabling high-throughput
computation of GNNs. To balance the workload of sparse-dense matrix
multiplication, we also introduced a greedy algorithm to combine sparse
sub-matrices of compressed format into condensed ones to reduce computational
cycles. Compared with current state-of-the-art GNN software frameworks,
\textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$
energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy
reduction on GPU."
[],An Energy-Based View of Graph Neural Networks,"Graph neural networks are a popular variant of neural networks that work with
graph-structured data. In this work, we consider combining graph neural
networks with the energy-based view of Grathwohl et al. (2019) with the aim of
obtaining a more robust classifier. We successfully implement this framework by
proposing a novel method to ensure generation over features as well as the
adjacency matrix and evaluate our method against the standard graph
convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach
obtains comparable discriminative performance while improving robustness,
opening promising new directions for future research for energy-based graph
neural networks."
[],Network Embedding via Deep Prediction Model,"Network-structured data becomes ubiquitous in daily life and is growing at a
rapid pace. It presents great challenges to feature engineering due to the high
non-linearity and sparsity of the data. The local and global structure of the
real-world networks can be reflected by dynamical transfer behaviors among
nodes. This paper proposes a network embedding framework to capture the
transfer behaviors on structured networks via deep prediction models. We first
design a degree-weight biased random walk model to capture the transfer
behaviors on the network. Then a deep network embedding method is introduced to
preserve the transfer possibilities among the nodes. A network structure
embedding layer is added into conventional deep prediction models, including
Long Short-Term Memory Network and Recurrent Neural Network, to utilize the
sequence prediction ability. To keep the local network neighborhood, we further
perform a Laplacian supervised space optimization on the embedding feature
representations. Experimental studies are conducted on various datasets
including social networks, citation networks, biomedical network, collaboration
network and language network. The results show that the learned representations
can be effectively used as features in a variety of tasks, such as clustering,
visualization, classification, reconstruction and link prediction, and achieve
promising performance compared with state-of-the-arts."
[],VID-WIN: Fast Video Event Matching with Query-Aware Windowing at the Edge for the Internet of Multimedia Things,"Efficient video processing is a critical component in many IoMT applications
to detect events of interest. Presently, many window optimization techniques
have been proposed in event processing with an underlying assumption that the
incoming stream has a structured data model. Videos are highly complex due to
the lack of any underlying structured data model. Video stream sources such as
CCTV cameras and smartphones are resource-constrained edge nodes. At the same
time, video content extraction is expensive and requires computationally
intensive Deep Neural Network (DNN) models that are primarily deployed at
high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage
allied windowing approach to accelerate video event analytics in an edge-cloud
paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the
query and resource-aware optimization for state-based complex event matching.
VID-WIN exploits the video content and DNN input knobs to accelerate the video
inference process across nodes. The paper proposes a novel content-driven
micro-batch resizing, queryaware caching and micro-batch based utility
filtering strategy of video frames under resource-constrained edge nodes to
improve the overall system throughput, latency, and network usage. Extensive
evaluations are performed over five real-world datasets. The experimental
results show that VID-WIN video event matching achieves ~2.3X higher throughput
with minimal latency and ~99% bandwidth reduction compared to other baselines
while maintaining query-level accuracy and resource bounds."
[],Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural Networks,"Graph neural networks (GNNs), an emerging deep learning model class, can
extract meaningful representations from highly expressive graph-structured data
and are therefore gaining popularity for wider ranges of applications. However,
current GNNs suffer from the poor performance of their sparse-dense matrix
multiplication (SpMM) operator, even when using powerful GPUs. Our analysis
shows that 95% of the inference time could be spent on SpMM when running
popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance
bottleneck hinders GNNs' applicability to large-scale problems or the
development of more sophisticated GNN models. To address this inference time
bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and
codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit
into GPU's shared memory. It thus reduces the computation cost and improves
SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with
a popular GNN framework, DGL, and tested it using representative GNN models and
datasets. Our results show that ES-SpMM outperforms the highly optimized
cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with
less than a 1% accuracy loss."
[],Scalable and Flexible Deep Bayesian Optimization with Auxiliary Information for Scientific Problems,"Bayesian optimization (BO) is a popular paradigm for global optimization of
expensive black-box functions, but there are many domains where the function is
not completely black-box. The data may have some known structure, e.g.
symmetries, and the data generation process can yield useful intermediate or
auxiliary information in addition to the value of the optimization objective.
However, surrogate models traditionally employed in BO, such as Gaussian
Processes (GPs), scale poorly with dataset size and struggle to incorporate
known structure or auxiliary information. Instead, we propose performing BO on
complex, structured problems by using Bayesian Neural Networks (BNNs), a class
of scalable surrogate models that have the representation power and flexibility
to handle structured data and exploit auxiliary information. We demonstrate BO
on a number of realistic problems in physics and chemistry, including topology
optimization of photonic crystal materials using convolutional neural networks,
and chemical property optimization of molecules using graph neural networks. On
these complex tasks, we show that BNNs often outperform GPs as surrogate models
for BO in terms of both sampling efficiency and computational cost."
[],Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning,"Recently, there has been great success in applying deep neural networks on
graph structured data. Most work, however, focuses on either node- or
graph-level supervised learning, such as node, link or graph classification or
node-level unsupervised learning (e.g. node clustering). Despite its wide range
of possible applications, graph-level unsupervised learning has not received
much attention yet. This might be mainly attributed to the high representation
complexity of graphs, which can be represented by n! equivalent adjacency
matrices, where n is the number of nodes. In this work we address this issue by
proposing a permutation-invariant variational autoencoder for graph structured
data. Our proposed model indirectly learns to match the node ordering of input
and output graph, without imposing a particular node ordering or performing
expensive graph matching. We demonstrate the effectiveness of our proposed
model on various graph reconstruction and generation tasks and evaluate the
expressive power of extracted representations for downstream graph-level
classification and regression."
[],Ranking Structured Objects with Graph Neural Networks,"Graph neural networks (GNNs) have been successfully applied in many
structured data domains, with applications ranging from molecular property
prediction to the analysis of social networks. Motivated by the broad
applicability of GNNs, we propose the family of so-called RankGNNs, a
combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are
trained with a set of pair-wise preferences between graphs, suggesting that one
of them is preferred over the other. One practical application of this problem
is drug screening, where an expert wants to find the most promising molecules
in a large collection of drug candidates. We empirically demonstrate that our
proposed pair-wise RankGNN approach either significantly outperforms or at
least matches the ranking performance of the naive point-wise baseline
approach, in which the LtR problem is solved via GNN-based graph regression."
[],Hierarchical Adaptive Pooling by Capturing High-order Dependency for Graph Representation Learning,"Graph neural networks (GNN) have been proven to be mature enough for handling
graph-structured data on node-level graph representation learning tasks.
However, the graph pooling technique for learning expressive graph-level
representation is critical yet still challenging. Existing pooling methods
either struggle to capture the local substructure or fail to effectively
utilize high-order dependency, thus diminishing the expression capability. In
this paper we propose HAP, a hierarchical graph-level representation learning
framework, which is adaptively sensitive to graph structures, i.e., HAP
clusters local substructures incorporating with high-order dependencies. HAP
utilizes a novel cross-level attention mechanism MOA to naturally focus more on
close neighborhood while effectively capture higher-order dependency that may
contain crucial information. It also learns a global graph content GCont that
extracts the graph pattern properties to make the pre- and post-coarsening
graph content maintain stable, thus providing global guidance in graph
coarsening. This novel innovation also facilitates generalization across graphs
with the same form of features. Extensive experiments on fourteen datasets show
that HAP significantly outperforms twelve popular graph pooling methods on
graph classification task with an maximum accuracy improvement of 22.79%, and
exceeds the performance of state-of-the-art graph matching and graph similarity
learning algorithms by over 3.5% and 16.7%."
[],Self-supervised Auxiliary Learning for Graph Neural Networks via Meta-Learning,"In recent years, graph neural networks (GNNs) have been widely adopted in the
representation learning of graph-structured data and provided state-of-the-art
performance in various applications such as link prediction, node
classification, and recommendation. Motivated by recent advances of
self-supervision for representation learning in natural language processing and
computer vision, self-supervised learning has been recently studied to leverage
unlabeled graph-structured data. However, employing self-supervision tasks as
auxiliary tasks to assist a primary task has been less explored in the
literature on graphs. In this paper, we propose a novel self-supervised
auxiliary learning framework to effectively learn graph neural networks.
Moreover, this work is the first study showing that a meta-path prediction is
beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our
method is learning to learn a primary task with various auxiliary tasks to
improve generalization performance. The proposed method identifies an effective
combination of auxiliary tasks and automatically balances them to improve the
primary task. Our methods can be applied to any graph neural network in a
plug-in manner without manual labeling or additional data. Also, it can be
extended to any other auxiliary tasks. Our experiments demonstrate that the
proposed method consistently improves the performance of node classification
and link prediction."
[],Attentional Graph Neural Network for Parking-slot Detection,"Deep learning has recently demonstrated its promising performance for
vision-based parking-slot detection. However, very few existing methods
explicitly take into account learning the link information of the
marking-points, resulting in complex post-processing and erroneous detection.
In this paper, we propose an attentional graph neural network based
parking-slot detection method, which refers the marking-points in an
around-view image as graph-structured data and utilize graph neural network to
aggregate the neighboring information between marking-points. Without any
manually designed post-processing, the proposed method is end-to-end trainable.
Extensive experiments have been conducted on public benchmark dataset, where
the proposed method achieves state-of-the-art accuracy. Code is publicly
available at \url{https://github.com/Jiaolong/gcn-parking-slot}."
[],Graph Contrastive Learning with Augmentations,"Generalizable, transferrable, and robust representation learning on
graph-structured data remains a challenge for current graph neural networks
(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)
for image data, self-supervised learning and pre-training are less explored for
GNNs. In this paper, we propose a graph contrastive learning (GraphCL)
framework for learning unsupervised representations of graph data. We first
design four types of graph augmentations to incorporate various priors. We then
systematically study the impact of various combinations of graph augmentations
on multiple datasets, in four different settings: semi-supervised,
unsupervised, and transfer learning as well as adversarial attacks. The results
show that, even without tuning augmentation extents nor using sophisticated GNN
architectures, our GraphCL framework can produce graph representations of
similar or better generalizability, transferrability, and robustness compared
to state-of-the-art methods. We also investigate the impact of parameterized
graph augmentation extents and patterns, and observe further performance gains
in preliminary experiments. Our codes are available at
https://github.com/Shen-Lab/GraphCL."
[],SetVAE: Learning Hierarchical Composition for Generative Modeling of Set-Structured Data,"Generative modeling of set-structured data, such as point clouds, requires
reasoning over local and global structures at various scales. However, adopting
multi-scale frameworks for ordinary sequential data to a set-structured data is
nontrivial as it should be invariant to the permutation of its elements. In
this paper, we propose SetVAE, a hierarchical variational autoencoder for sets.
Motivated by recent progress in set encoding, we build SetVAE upon attentive
modules that first partition the set and project the partition back to the
original cardinality. Exploiting this module, our hierarchical VAE learns
latent variables at multiple scales, capturing coarse-to-fine dependency of the
set elements while achieving permutation invariance. We evaluate our model on
point cloud generation task and achieve competitive performance to the prior
arts with substantially smaller model capacity. We qualitatively demonstrate
that our model generalizes to unseen set sizes and learns interesting subset
relations without supervision. Our implementation is available at
https://github.com/jw9730/setvae."
[],Hierarchical Graph Capsule Network,"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the
topological information of structured data. However, existing GNNs suffer from
limited capability in capturing the hierarchical graph representation which
plays an important role in graph classification. In this paper, we innovatively
propose hierarchical graph capsule network (HGCN) that can jointly learn node
embeddings and extract graph hierarchies. Specifically, disentangled graph
capsules are established by identifying heterogeneous factors underlying each
node, such that their instantiation parameters represent different properties
of the same entity. To learn the hierarchical representation, HGCN
characterizes the part-whole relationship between lower-level capsules (part)
and higher-level capsules (whole) by explicitly considering the structure
information among the parts. Experimental studies demonstrate the effectiveness
of HGCN and the contribution of each component."
[],Spatio-Temporal Sparsification for General Robust Graph Convolution Networks,"Graph Neural Networks (GNNs) have attracted increasing attention due to its
successful applications on various graph-structure data. However, recent
studies have shown that adversarial attacks are threatening the functionality
of GNNs. Although numerous works have been proposed to defend adversarial
attacks from various perspectives, most of them can be robust against the
attacks only on specific scenarios. To address this shortage of robust
generalization, we propose to defend the adversarial attacks on GNN through
applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN
hidden node representation. ST-Sparse is similar to the Dropout regularization
in spirit. Through intensive experiment evaluation with GCN as the target GNN
model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows
the defense performance improvement in most cases, as it can effectively
increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse
illustrates its robust generalization capability by integrating with the
existing defense methods, similar to the integration of Dropout into various
deep learning models as a standard regularization technique; (3) ST-Sparse also
shows its ordinary generalization capability on clean datasets, in that
ST-SparseGCN (the integration of ST-Sparse and the original GCN) even
outperform the original GCN, while the other three representative defense
methods are inferior to the original GCN."
[],Understanding Heart-Failure Patients EHR Clinical Features via SHAP Interpretation of Tree-Based Machine Learning Model Predictions,"Heart failure (HF) is a major cause of mortality. Accurately monitoring HF
progress and adjust therapies are critical for improving patient outcomes. An
experienced cardiologist can make accurate HF stage diagnoses based on
combination of symptoms, signs, and lab results from the electronic health
records (EHR) of a patient, without directly measuring heart function. We
examined whether machine learning models, more specifically the XGBoost model,
can accurately predict patient stage based on EHR, and we further applied the
SHapley Additive exPlanations (SHAP) framework to identify informative features
and their interpretations. Our results indicate that based on structured data
from EHR, our models could predict patients' ejection fraction (EF) scores with
moderate accuracy. SHAP analyses identified informative features and revealed
potential clinical subtypes of HF. Our findings provide insights on how to
design computing systems to accurately monitor disease progression of HF
patients through continuously mining patients' EHR data."
[],Recognizing Predictive Substructures with Subgraph Information Bottleneck,"The emergence of Graph Convolutional Network (GCN) has greatly boosted the
progress of graph learning. However, two disturbing factors, noise and
redundancy in graph data, and lack of interpretation for prediction results,
impede further development of GCN. One solution is to recognize a predictive
yet compressed subgraph to get rid of the noise and redundancy and obtain the
interpretable part of the graph. This setting of subgraph is similar to the
information bottleneck (IB) principle, which is less studied on
graph-structured data and GCN. Inspired by the IB principle, we propose a novel
subgraph information bottleneck (SIB) framework to recognize such subgraphs,
named IB-subgraph. However, the intractability of mutual information and the
discrete nature of graph data makes the objective of SIB notoriously hard to
optimize. To this end, we introduce a bilevel optimization scheme coupled with
a mutual information estimator for irregular graphs. Moreover, we propose a
continuous relaxation for subgraph selection with a connectivity loss for
stabilization. We further theoretically prove the error bound of our estimation
scheme for mutual information and the noise-invariant nature of IB-subgraph.
Extensive experiments on graph learning and large-scale point cloud tasks
demonstrate the superior property of IB-subgraph."
[],Set Representation Learning with Generalized Sliced-Wasserstein Embeddings,"An increasing number of machine learning tasks deal with learning
representations from set-structured data. Solutions to these problems involve
the composition of permutation-equivariant modules (e.g., self-attention, or
individual processing via feed-forward neural networks) and
permutation-invariant modules (e.g., global average pooling, or pooling by
multi-head attention). In this paper, we propose a geometrically-interpretable
framework for learning representations from set-structured data, which is
rooted in the optimal mass transportation problem. In particular, we treat
elements of a set as samples from a probability measure and propose an exact
Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn
from set-structured data effectively. We evaluate our proposed framework on
multiple supervised and unsupervised set learning tasks and demonstrate its
superiority over state-of-the-art set representation learning approaches."
[],Unified Robust Training for Graph NeuralNetworks against Label Noise,"Graph neural networks (GNNs) have achieved state-of-the-art performance for
node classification on graphs. The vast majority of existing works assume that
genuine node labels are always provided for training. However, there has been
very little research effort on how to improve the robustness of GNNs in the
presence of label noise. Learning with label noise has been primarily studied
in the context of image classification, but these techniques cannot be directly
applied to graph-structured data, due to two major challenges -- label sparsity
and label dependency -- faced by learning on graphs. In this paper, we propose
a new framework, UnionNET, for learning with noisy labels on graphs under a
semi-supervised setting. Our approach provides a unified solution for robustly
training GNNs and performing label correction simultaneously. The key idea is
to perform label aggregation to estimate node-level class probability
distributions, which are used to guide sample reweighting and label correction.
Compared with existing works, UnionNET has two appealing advantages. First, it
requires no extra clean supervision, or explicit estimation of the noise
transition matrix. Second, a unified learning framework is proposed to robustly
train GNNs in an end-to-end manner. Experimental results show that our proposed
approach: (1) is effective in improving model robustness against different
types and levels of label noise; (2) yields significant improvements over
state-of-the-art baselines."
[],Learning Structural Edits via Incremental Tree Transformations,"While most neural generative models generate outputs in a single pass, the
human creative process is usually one of iterative building and refinement.
Recent work has proposed models of editing processes, but these mostly focus on
editing sequential data and/or only model a single editing pass. In this paper,
we present a generic model for incremental editing of structured data (i.e.,
""structural edits""). Particularly, we focus on tree-structured data, taking
abstract syntax trees of computer programs as our canonical example. Our editor
learns to iteratively generate tree edits (e.g., deleting or adding a subtree)
and applies them to the partially edited data, thereby the entire editing
process can be formulated as consecutive, incremental tree transformations. To
show the unique benefits of modeling tree edits directly, we further propose a
novel edit encoder for learning to represent edits, as well as an imitation
learning method that allows the editor to be more robust. We evaluate our
proposed editor on two source code edit datasets, where results show that, with
the proposed edit encoder, our editor significantly improves accuracy over
previous approaches that generate the edited program directly in one pass.
Finally, we demonstrate that training our editor to imitate experts and correct
its mistakes dynamically can further improve its performance."
[],Deep Graph Structure Learning for Robust Representations: A Survey,"Graph Neural Networks (GNNs) are widely used for analyzing graph-structured
data. Most GNN methods are highly sensitive to the quality of graph structures
and usually require a perfect graph structure for learning informative
embeddings. However, the pervasiveness of noise in graphs necessitates learning
robust representations for real-world problems. To improve the robustness of
GNN models, many studies have been proposed around the central concept of Graph
Structure Learning (GSL), which aims to jointly learn an optimized graph
structure and corresponding representations. Towards this end, in the presented
survey, we broadly review recent progress of GSL methods for learning robust
representations. Specifically, we first formulate a general paradigm of GSL,
and then review state-of-the-art methods classified by how they model graph
structures, followed by applications that incorporate the idea of GSL in other
graph tasks. Finally, we point out some issues in current studies and discuss
future directions."
[],Wide Graph Neural Networks: Aggregation Provably Leads to Exponentially Trainability Loss,"Graph convolutional networks (GCNs) and their variants have achieved great
success in dealing with graph-structured data. However, it is well known that
deep GCNs will suffer from over-smoothing problem, where node representations
tend to be indistinguishable as we stack up more layers. Although extensive
research has confirmed this prevailing understanding, few theoretical analyses
have been conducted to study the expressivity and trainability of deep GCNs. In
this work, we demonstrate these characterizations by studying the Gaussian
Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an
infinitely-wide GCN, corresponding to the analysis on expressivity and
trainability, respectively. We first prove the expressivity of infinitely-wide
GCNs decaying at an exponential rate by applying the mean-field theory on GPK.
Besides, we formulate the asymptotic behaviors of GNTK in the large depth,
which enables us to reveal the dropping trainability of wide and deep GCNs at
an exponential rate. Additionally, we extend our theoretical framework to
analyze residual connection-resemble techniques. We found that these techniques
can mildly mitigate exponential decay, but they failed to overcome it
fundamentally. Finally, all theoretical results in this work are corroborated
experimentally on a variety of graph-structured datasets."
[],Mini-Batch Consistent Slot Set Encoder for Scalable Set Encoding,"Most existing set encoding algorithms operate under the assumption that all
the elements of the set are accessible during training and inference.
Additionally, it is assumed that there are enough computational resources
available for concurrently processing sets of large cardinality. However, both
assumptions fail when the cardinality of the set is prohibitively large such
that we cannot even load the set into memory. In more extreme cases, the set
size could be potentially unlimited, and the elements of the set could be given
in a streaming manner, where the model receives subsets of the full set data at
irregular intervals. To tackle such practical challenges in large-scale set
encoding, we go beyond the usual constraints of invariance and equivariance and
introduce a new property termed Mini-Batch Consistency that is required for
large scale mini-batch set encoding. We present a scalable and efficient set
encoding mechanism that is amenable to mini-batch processing with respect to
set elements and capable of updating set representations as more data arrives.
The proposed method respects the required symmetries of invariance and
equivariance as well as being Mini-Batch Consistent for random partitions of
the input set. We perform extensive experiments and show that our method is
computationally efficient and results in rich set encoding representations for
set-structured data."
[],Incorporating Symbolic Domain Knowledge into Graph Neural Networks,"Our interest is in scientific problems with the following characteristics:
(1) Data are naturally represented as graphs; (2) The amount of data available
is typically small; and (3) There is significant domain-knowledge, usually
expressed in some symbolic form. These kinds of problems have been addressed
effectively in the past by Inductive Logic Programming (ILP), by virtue of 2
important characteristics: (a) The use of a representation language that easily
captures the relation encoded in graph-structured data, and (b) The inclusion
of prior information encoded as domain-specific relations, that can alleviate
problems of data scarcity, and construct new relations. Recent advances have
seen the emergence of deep neural networks specifically developed for
graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have
been shown to be able to handle graph-structured data, less has been done to
investigate the inclusion of domain-knowledge. Here we investigate this aspect
of GNNs empirically by employing an operation we term ""vertex-enrichment"" and
denote the corresponding GNNs as ""VEGNNs"". Using over 70 real-world datasets
and substantial amounts of symbolic domain-knowledge, we examine the result of
vertex-enrichment across 5 different variants of GNNs. Our results provide
support for the following: (a) Inclusion of domain-knowledge by
vertex-enrichment can significantly improve the performance of a GNN. That is,
the performance VEGNNs is significantly better than GNNs across all GNN
variants; (b) The inclusion of domain-specific relations constructed using ILP
improves the performance of VEGNNs, across all GNN variants. Taken together,
the results provide evidence that it is possible to incorporate symbolic domain
knowledge into a GNN, and that ILP can play an important role in providing
high-level relationships that are not easily discovered by a GNN."
[],Interpretable Stability Bounds for Spectral Graph Filters,"Graph-structured data arise in a variety of real-world context ranging from
sensor and transportation to biological and social networks. As a ubiquitous
tool to process graph-structured data, spectral graph filters have been used to
solve common tasks such as denoising and anomaly detection, as well as design
deep learning architectures such as graph neural networks. Despite being an
important tool, there is a lack of theoretical understanding of the stability
properties of spectral graph filters, which are important for designing robust
machine learning models. In this paper, we study filter stability and provide a
novel and interpretable upper bound on the change of filter output, where the
bound is expressed in terms of the endpoint degrees of the deleted and newly
added edges, as well as the spatial proximity of those edges. This upper bound
allows us to reason, in terms of structural properties of the graph, when a
spectral graph filter will be stable. We further perform extensive experiments
to verify intuition that can be gained from the bound."
[],Ego-based Entropy Measures for Structural Representations on Graphs,"Machine learning on graph-structured data has attracted high research
interest due to the emergence of Graph Neural Networks (GNNs). Most of the
proposed GNNs are based on the node homophily, i.e neighboring nodes share
similar characteristics. However, in many complex networks, nodes that lie to
distant parts of the graph share structurally equivalent characteristics and
exhibit similar roles (e.g chemical properties of distant atoms in a molecule,
type of social network users). A growing literature proposed representations
that identify structurally equivalent nodes. However, most of the existing
methods require high time and space complexity. In this paper, we propose
VNEstruct, a simple approach, based on entropy measures of the neighborhood's
topology, for generating low-dimensional structural representations, that is
time-efficient and robust to graph perturbations. Empirically, we observe that
VNEstruct exhibits robustness on structural role identification tasks.
Moreover, VNEstruct can achieve state-of-the-art performance on graph
classification, without incorporating the graph structure information in the
optimization, in contrast to GNN competitors."
[],Integrating Floor Plans into Hedonic Models for Rent Price Appraisal,"Online real estate platforms have become significant marketplaces
facilitating users' search for an apartment or a house. Yet it remains
challenging to accurately appraise a property's value. Prior works have
primarily studied real estate valuation based on hedonic price models that take
structured data into account while accompanying unstructured data is typically
ignored. In this study, we investigate to what extent an automated visual
analysis of apartment floor plans on online real estate platforms can enhance
hedonic rent price appraisal. We propose a tailored two-staged deep learning
approach to learn price-relevant designs of floor plans from historical price
data. Subsequently, we integrate the floor plan predictions into hedonic rent
price models that account for both structural and locational characteristics of
an apartment. Our empirical analysis based on a unique dataset of 9174 real
estate listings suggests that current hedonic models underutilize the available
data. We find that (1) the visual design of floor plans has significant
explanatory power regarding rent prices - even after controlling for structural
and locational apartment characteristics, and (2) harnessing floor plans
results in an up to 10.56% lower out-of-sample prediction error. We further
find that floor plans yield a particularly high gain in prediction performance
for older and smaller apartments. Altogether, our empirical findings contribute
to the existing research body by establishing the link between the visual
design of floor plans and real estate prices. Moreover, our approach has
important implications for online real estate platforms, which can use our
findings to enhance user experience in their real estate listings."
[],On PyTorch Implementation of Density Estimators for von Mises-Fisher and Its Mixture,"The von Mises-Fisher (vMF) is a well-known density model for directional
random variables. The recent surge of the deep embedding methodologies for
high-dimensional structured data such as images or texts, aimed at extracting
salient directional information, can make the vMF model even more popular. In
this article, we will review the vMF model and its mixture, provide detailed
recipes of how to train the models, focusing on the maximum likelihood
estimators, in Python/PyTorch. In particular, implementation of vMF typically
suffers from the notorious numerical issue of the Bessel function evaluation in
the density normalizer, especially when the dimensionality is high, and we
address the issue using the MPMath library that supports arbitrary precision.
For the mixture learning, we provide both minibatch-based large-scale SGD
learning, as well as the EM algorithm which is a full batch estimator. For each
estimator/methodology, we test our implementation on some synthetic data, while
we also demonstrate the use case in a more realistic scenario of image
clustering. Our code is publicly available in
https://github.com/minyoungkim21/vmf-lib."
